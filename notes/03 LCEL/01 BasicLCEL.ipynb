{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81c991a9-e888-48ea-a5f3-c37818532ab0",
   "metadata": {},
   "source": [
    "# LCEL的好处"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c5f62d-96c4-44ec-9082-7e3a768c489c",
   "metadata": {},
   "source": [
    "以下是 **langchain 文档的原文**摘取："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b9326b-6b32-4306-a606-5c3322184050",
   "metadata": {},
   "source": [
    "**LangChain 表达式语言（LCEL）是一种轻松地将链组合在一起的声明性方式。**\n",
    "\n",
    "LCEL 从第一天起就被设计为支持将原型投入生产，无需更改代码，从最简单的 **「Prompt + Model」** 链到最复杂的链（我们已经看到人们在生产中成功运行了 100 个步骤的 LCEL 链）。 \n",
    "\n",
    "强调一下您可能想要使用 LCEL 的一些原因："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91468962-c8c4-44d0-a2a3-18c5346288da",
   "metadata": {},
   "source": [
    "\n",
    "**流支持**<br>\n",
    "当您使用 LCEL 构建链时，您可以获得最佳的首次Token时间（直到第一个输出块出现之前经过的时间）。 对于某些连锁店来说，这意味着例如。 我们将令牌直接从 LLM 流式传输到流式输出解析器，然后您会以与 LLM 提供者输出原始令牌相同的速率返回已解析的增量输出块。\n",
    "\n",
    "**异步支持**<br>\n",
    "使用 LCEL 构建的任何链都可以使用同步 API（例如，在原型设计时在 Jupyter 笔记本中）和异步 API（例如，在 LangServe 服务器中）进行调用。 这使得能够在原型和生产中使用相同的代码，具有出色的性能，并且能够在同一服务器中处理许多并发请求。\n",
    "\n",
    "**优化的并行执行**<br>\n",
    "只要您的 LCEL 链具有可以并行执行的步骤（例如，如果您从多个检索器获取文档），我们就会在同步和异步接口中自动执行此操作，以尽可能减少延迟。\n",
    "\n",
    "**重试和回退**<br>\n",
    "为 LCEL 链的任何部分配置重试和回退。 这是让您的链条在规模上更加可靠的好方法。 我们目前正在努力添加对重试/回退的流支持，以便您可以获得更高的可靠性，而无需任何延迟成本。\n",
    "\n",
    "**访问中间结果**<br>\n",
    "对于更复杂的链，即使在生成最终输出之前访问中间步骤的结果通常也非常有用。 这可以用来让最终用户知道正在发生的事情，甚至只是为了调试您的链。 您可以流式传输中间结果，并且它在每个 LangServe 服务器上都可用。\n",
    "\n",
    "**输入和输出模式**<br>\n",
    "输入和输出模式为每个 LCEL 链提供从链结构推断出的 Pydantic 和 JSONSchema 模式。 这可用于验证输入和输出，并且是 LangServe 的组成部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa1c53b-9fc9-4506-af58-8f5a6a22653a",
   "metadata": {},
   "source": [
    "# 基本模式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c094a6c-d42f-4322-be30-d5458c797c9b",
   "metadata": {},
   "source": [
    "## Prompt + Model + OutputParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d566041-d951-485e-b3d8-393ee721340f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a798698e-73a1-49b7-b4f2-17acfd3767c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream go to therapy? Because it was feeling a little sundae-pressed!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0233dc07-a3f1-46c8-aad8-e7e95d747693",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1fe034-5c34-4082-ac6d-d86848d9511b",
   "metadata": {},
   "source": [
    "Prompt 是一个 BasePromptTemplate，这意味着它接受模板变量的字典并生成 PromptValue。 \n",
    "PromptValue 是完整提示的包装器，可以传递给 LLM（将字符串作为输入）或 ChatModel（将一系列消息作为输入）。 \n",
    "它可以使用任一语言模型类型，因为它定义了用于生成 BaseMessage 和生成字符串的逻辑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2f0d596-c2d8-4efa-a391-6b0f60573c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value = prompt.invoke({\"topic\": \"ice cream\"})\n",
    "prompt_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043fb0ea-ebe3-4c73-973f-48da2d21d009",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0037e6d-1650-4dcb-b7d6-bdafabca2bb5",
   "metadata": {},
   "source": [
    "然后将 PromptValue 传递给模型。 在本例中，我们的模型是 ChatModel，这意味着它将输出 BaseMessage。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "482f1044-9610-434e-aa0f-e47a022560f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the ice cream go to therapy? Because it had too many sprinkles of anxiety!')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = model.invoke(prompt_value)\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8fa721a-0d5b-4b84-b5c9-366801b0b59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\nAI: Why did the ice cream go to therapy? Because it had a rocky road!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果不是对话模型，而是LLM模型，就会输出字符串\n",
    "\n",
    "from langchain_openai.llms import OpenAI\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "llm.invoke(prompt_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed0ed5e-6af2-44f6-a58d-1dfc26514432",
   "metadata": {},
   "source": [
    "## OutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c3befb-96e0-4357-a0f9-ae57091f0829",
   "metadata": {},
   "source": [
    "最后，我们将模型输出传递给 output_parser，它是一个 BaseOutputParser，这意味着它接受字符串或 BaseMessage 作为输入。\n",
    "StrOutputParser 特别简单地将任何输入转换为字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a33fc1ec-c4f8-462a-8588-0b913ddc7307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream go to therapy? Because it had too many sprinkles of anxiety!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchani-book-py3.10-ipykernel",
   "language": "python",
   "name": "langchani-book-py3.10-ipykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
