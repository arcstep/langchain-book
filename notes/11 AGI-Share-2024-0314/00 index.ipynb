{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e5b5362-cf90-4d8a-9c70-3db56e3f8dcc",
   "metadata": {},
   "source": [
    "# 🦜🔗 LangChain核心源代码解读"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09652ad-45cf-4152-8794-2639a92a4ac2",
   "metadata": {},
   "source": [
    "# 课程开始"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bac26c1-1878-43d9-984d-bebacabccefb",
   "metadata": {},
   "source": [
    "## ❤️ 这节课会带给你"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eb7b7a-d4a8-4e8d-b757-a55062c3fad8",
   "metadata": {},
   "source": [
    "1. 🌹一起阅读 Langchain 核心组件的源码：Runnable、LLM、Agent、Langgraph\n",
    "2. 🌹从解读源码的角度了解 Langchain 整体设计思路，以便更好地阅读官方文档\n",
    "3. 🌹掌握 Langchain 文档中未曾提及、翻看源码才知晓的实用技巧\n",
    "4. ✍️ 动手实现一个自定义大模型框架：会说话的邻居老大爷\n",
    "5. ✍️ 动手集成自己的大模型到 LangChain：智谱AI\n",
    "6. ✍️ 动手实现基于 LCEL 的智能体：再现手撕AutoGPT\n",
    "7. ✍️ 动手实现基于 Langgraph 的智能体：再现手撕AutoGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2d3fb7-9b0a-4592-ac64-ea74630e75be",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>注意：</b><br>\n",
    "<ul>\n",
    "    <li>大模型认知：请参考AGI课堂正课相关章节</li>\n",
    "    <li>Langchain 基础：请参考AGI课堂正课相关章节</li>\n",
    "    <li>看懂源码是为了写好代码</li>\n",
    "</ul> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff511b-10e1-43d4-92e0-829a1fe4d5d2",
   "metadata": {},
   "source": [
    "## 如何解读 Langchain 的源代码结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ac50aa-ed71-46ea-ac3c-e1af5abe5fc9",
   "metadata": {},
   "source": [
    "### 1、你有哪些资源可以利用？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f539f3d-148f-4c18-a959-881dfc8ce4b7",
   "metadata": {},
   "source": [
    "![](./langchain_ai.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbce31b-2fd4-43cd-8204-98fa50ae8bf5",
   "metadata": {},
   "source": [
    "（1）学习资源：\n",
    "- [langchain源代码](https://github.com/langchain-ai/)：All you need r here !!!\n",
    "- [langchain官方文档](https://python.langchain.com/docs)：与源代码相互印证\n",
    "- [langgraph example](https://github.com/langchain-ai/langgraph/tree/main/examples)：Jupyter Notes\n",
    "\n",
    "（2）良师益友：\n",
    "- langchain [聊天](https://chat.langchain.com/?llm=anthropic_claude_2_1)：免费的大模型+RAG（也可以学习其源码）\n",
    "- Github Copilot：程序员无法离开的工具，就像现在的人开车无法离开地图导航"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab7f60d-7043-42e0-98c2-459355c52f19",
   "metadata": {},
   "source": [
    "### 2、🌹 阅读源码：LangChain 源代码概览"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4481a6-d92d-4df6-8cfd-65e57c2463f5",
   "metadata": {},
   "source": [
    "[langchain源代码结构](https://github.com/langchain-ai/langchain/tree/master/libs)\n",
    "\n",
    "| 源码位置 | 功能描述 |\n",
    "| :--- | :--- |\n",
    "| langchain/libs/langchain | 模块入口，会导入core、community等其他模块 |\n",
    "| langchain/libs/core | 核心组件和关键的基类实现 |\n",
    "| langchain/libs/partners | 合作伙伴（官方合作）组件 |\n",
    "| langchain/libs/community | 社区（非官方）组件 |\n",
    "| langchain/libs/experimental | 试验性功能（前沿探索组件，不对版本稳定做承诺） |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01548080-b79b-4a88-9b9f-7af2ef9ae567",
   "metadata": {},
   "source": [
    "### 3、LangChain核心框架的三轮迭代"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc361bf-be03-4dbe-bf58-03f11e20fef8",
   "metadata": {},
   "source": [
    "- Runnable + Chain 时代\n",
    "- Runnable + LCEL 时代\n",
    "- Runnable + LCEL + Langgraph 时代"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e936103d-a47c-4455-81ca-10a8cbb8449a",
   "metadata": {},
   "source": [
    "# （一）从零开始集成大模型到 Langchain 实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4086972-8af4-41e8-9f1d-7d3a5e984bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载 .env 到环境变量\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a33f05-0ea9-427c-b851-6511e0b9fc29",
   "metadata": {},
   "source": [
    "## 大模型的一般用法回顾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56f9e2e4-782e-466b-be9d-c23b0c30242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c4db118-7b74-4d8e-8d9c-e8976ff4294e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 智能字铅\n",
      "2. AI文笔\n",
      "3. 自动书写\n",
      "4. 涂字智能\n",
      "5. 字迹神笔\n",
      "6. AI字画\n",
      "7. 自动书法\n",
      "8. 智能写字\n",
      "9. 自动笔迹\n",
      "10. AI字迹通\n"
     ]
    }
   ],
   "source": [
    "# invoke\n",
    "text = \"请帮我想一想，生产一款「基于AI自动写字的铅笔」的公司有什么好名字?\"\n",
    "response = llm.invoke(text)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ad948cb-8584-43f4-9dce-b008fa6cbc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|1|.| 智|能|笔|迹|\n",
      "|2|.| 创|意|笔|锋|\n",
      "|3|.| AI|写|字|宝|\n",
      "|4|.| 涂|鸦|助|手|\n",
      "|5|.| 文|字|小|帮|手|\n",
      "|6|.| 心|灵|笔|耕|\n",
      "|7|.| 智|能|书|写|\n",
      "|8|.| 创|造|笔|迹|\n",
      "|9|.| 智|能|铅|笔|坊|\n",
      "|10|.| 文|字|魔|法|使||"
     ]
    }
   ],
   "source": [
    "# stream\n",
    "for chunk in llm.stream(text):\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32dea0d5-8c5c-436d-b006-67b07eceae8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|1|.| 智|能|笔|记|\n",
      "|2|.| AI|笔|迹|\n",
      "|3|.| 心|灵|笔|\n",
      "|4|.| 智|慧|铅|笔|\n",
      "|5|.| 创|意|笔|迹|\n",
      "|6|.| 自|动|笔|画|\n",
      "|7|.| 智|能|写|字|\n",
      "|8|.| AI|手|写|笔|\n",
      "|9|.| 智|能|文字|\n",
      "|10|.| 创|意|笔|锋||"
     ]
    }
   ],
   "source": [
    "# LCEL LLM+Prompt+Outputparser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"请帮我想一想，生产一款{product}的公司有什么好名字?\")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "for chunk in chain.stream({\"product\": \"基于AI自动写字的铅笔\"}):\n",
    "    print(chunk, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d63eec-9f22-4f5b-94bf-40f721313ad1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>⚠️ 思考</b><br>\n",
    "    langchain 支持的8个方法都在什么场景下使用？\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73527d37-fdb8-4d76-b70c-4ce1c209a3ed",
   "metadata": {},
   "source": [
    "## ❤️ 实例1 实现「楼下邻居老大爷」AI大模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a3028e-da74-4247-af83-d76f03544797",
   "metadata": {},
   "source": [
    "### 🦜 需求分析：参考电影片段，把「非常有智慧的楼下邻居老大爷」变成 AI大模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e6fe6d-6fdd-49ea-88ce-496eddd23e5b",
   "metadata": {},
   "source": [
    "1. 使用大模型：模拟一个大模型\n",
    "2. 生成能力：提及马冬梅时生成打岔闲聊（马什么梅？马冬什么？什么冬梅？），其余生成“哦...“\n",
    "\n",
    "![马冬梅](./madongmei.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d58bd3-44a0-4d8e-a884-76a17f501b00",
   "metadata": {},
   "source": [
    "### 🌹 看源码："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77551be1-5a9f-4a81-91c1-b81750203f0e",
   "metadata": {},
   "source": [
    "#### （1）BaseLanguageModel\n",
    "\n",
    "来自：[https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/language_models/base.py#L74-L81)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd55cf-68b2-4586-8119-672d55e66026",
   "metadata": {},
   "source": [
    "```python\n",
    "class BaseLanguageModel(\n",
    "    RunnableSerializable[LanguageModelInput, LanguageModelOutputVar], ABC\n",
    "):\n",
    "    \"\"\"Abstract base class for interfacing with language models.\n",
    "\n",
    "    All language model wrappers inherit from BaseLanguageModel.\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f943d2a1-b8a2-4525-af16-0905b0f581a3",
   "metadata": {},
   "source": [
    "#### （2）BaseChatModel\n",
    "\n",
    "来自：[https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/language_models/chat_models.py#L100)\n",
    "\n",
    "**核心逻辑：**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8accb174-c5e8-493f-8f20-530be6f854b0",
   "metadata": {},
   "source": [
    "```python\n",
    "class BaseChatModel(BaseLanguageModel[BaseMessage], ABC):\n",
    "    \"\"\"Base class for Chat models.\"\"\"\n",
    "\n",
    "    ...\n",
    "\n",
    "    def invoke(...) -> BaseMessage:\n",
    "        # ...\n",
    "        self.generate_prompt(...)\n",
    "        # generate_prompt >> generate >> _generate\n",
    "\n",
    "    def ainvoke(...) -> BaseMessage:\n",
    "        # ...\n",
    "        self.agenerate_prompt(...)\n",
    "        # agenerate_prompt >> agenerate >> _agenerate >> _generate\n",
    "    \n",
    "    def stream(...) -> Iterator[BaseMessageChunk]:\n",
    "        # ...\n",
    "        if type(self)._stream == BaseChatModel._stream:\n",
    "            # model doesn't implement streaming, so use default implementation\n",
    "            yield cast(\n",
    "                BaseMessageChunk, self.invoke(input, config=config, stop=stop, **kwargs)\n",
    "            )\n",
    "        # ...\n",
    "\n",
    "    async def astream(...) -> AsyncIterator[BaseMessageChunk]:\n",
    "        # 在#19332合并中，_astream方法实现已经被简化\n",
    "        # https://github.com/langchain-ai/langchain/pull/19332/commits/afbe6ac659e41ab5f4a6f4dcaa33511e9e59e4d5\n",
    "        if (\n",
    "            type(self)._astream is BaseChatModel._astream\n",
    "            and type(self)._stream is BaseChatModel._stream\n",
    "        ):\n",
    "            # No async or sync stream is implemented, so fall back to ainvoke\n",
    "            yield cast(\n",
    "                BaseMessageChunk,\n",
    "                await self.ainvoke(input, config=config, stop=stop, **kwargs),\n",
    "            )\n",
    "        # ...\n",
    "\n",
    "    # bacth, abatch, astream_log, astream_events \n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af6f28c-87d2-48a3-b78f-72af08ed9413",
   "metadata": {},
   "source": [
    "**必须实现的部分：**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9da607-5cfe-48a4-adaa-05d057e4dbcb",
   "metadata": {},
   "source": [
    "```python\n",
    "    ## ******** invoke / ainvoke / batch / abatch **********\n",
    "    @abstractmethod\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        \"\"\"Top Level call\"\"\"\n",
    "\n",
    "    ## ******** stream / astream / astream_log / astream_events **********\n",
    "    def _stream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGenerationChunk]:\n",
    "        raise NotImplementedError()        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c0526e-50a9-481f-ae14-6433fc114d25",
   "metadata": {},
   "source": [
    "### ✍️ 基本实现：invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52b2c9ba-39af-488b-af00-d272b7bff551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "from typing import Any, Dict, Iterator, List, Optional, Union\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import HumanMessage, HumanMessageChunk, AIMessage, AIMessageChunk, BaseMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2ebdb44f-afd7-4ffc-9419-56e144a6d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatWithOlderAI(BaseChatModel):\n",
    "    \"\"\"模拟跟马冬梅楼下邻居老大爷的对话\"\"\"\n",
    "\n",
    "    responses: List[BaseMessage] = [HumanMessage(m) for m in [\n",
    "        \"马什么梅？\", \"什么冬梅？？\", \"马东什么？？？\",\n",
    "    ]]\n",
    "    \n",
    "    sleep: Optional[float] = 0.1\n",
    "    i: int = 0\n",
    "\n",
    "    # 必须实现\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"chat-with-neighber-older\"\n",
    "\n",
    "    # 必须实现\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        generations = [ChatGeneration(message=res) for res in self._ask_remote(messages)]\n",
    "        return ChatResult(generations=generations)\n",
    "\n",
    "    # 访问远程大模型\n",
    "    def _ask_remote(self, messages: List[BaseMessage]) -> List[BaseMessage]:\n",
    "        if(re.search(\"马冬梅\", messages[0].content)):\n",
    "            response = self.responses[self.i]\n",
    "            if self.i < len(self.responses) - 1:\n",
    "                self.i += 1\n",
    "            else:\n",
    "                self.i = 0\n",
    "        else:\n",
    "            response = AIMessage(\"哦...\")\n",
    "        return [response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9fb12783-b794-4ce2-8ace-4bd781be835e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "夏洛：大爷，楼上322住的是马冬梅家吗？\n",
      "大爷：马什么梅？|\n",
      "\n",
      "夏洛：马冬梅啊\n",
      "大爷：什么冬梅？？|\n",
      "\n",
      "夏洛：马冬梅！\n",
      "大爷：马东什么？？？|\n",
      "\n",
      "夏洛：我是说马冬梅！\n",
      "大爷：马什么梅？|\n",
      "\n",
      "夏洛：您歇着吧...\n",
      "大爷：哦...|"
     ]
    }
   ],
   "source": [
    "questions = [[HumanMessage(m)] for m in [\n",
    "    \"大爷，楼上322住的是马冬梅家吗？\",\n",
    "    \"马冬梅啊\",\n",
    "    \"马冬梅！\",\n",
    "    \"我是说马冬梅！\",\n",
    "    \"您歇着吧...\"\n",
    "]]\n",
    "\n",
    "llm = ChatWithOlderAI()\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n\\n夏洛：{question[0].content}\")\n",
    "    print(\"大爷：\", end=\"\")\n",
    "    # print(llm.invoke(question).content, end=\"\")\n",
    "    for chunk in llm.stream(question):\n",
    "        print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8231e5a-c8b9-4d8e-8e56-280ccc889fd7",
   "metadata": {},
   "source": [
    "### ✍️ 支持流式输出：stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe2633f0-9002-4684-818a-44daed91c706",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamChatWithOlderAI(ChatWithOlderAI):\n",
    "    \"\"\"模拟跟大爷的对话，支持流\"\"\"\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        *args,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGenerationChunk]:\n",
    "        response = self._ask_remote(messages)\n",
    "        for chunk in response[0].content:\n",
    "            if self.sleep is not None:\n",
    "                time.sleep(self.sleep)\n",
    "            yield ChatGenerationChunk(message=AIMessageChunk(content=chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "236c1cf8-0886-423f-9710-cd10ef7c6e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "夏洛：大爷，楼上322住的是马冬梅家吗？\n",
      "大爷：马|什|么|梅|？|\n",
      "\n",
      "夏洛：马冬梅啊\n",
      "大爷：什|么|冬|梅|？|？|\n",
      "\n",
      "夏洛：马冬梅！\n",
      "大爷：马|东|什|么|？|？|？|\n",
      "\n",
      "夏洛：我是说马冬梅！\n",
      "大爷：马|什|么|梅|？|\n",
      "\n",
      "夏洛：您歇着吧...\n",
      "大爷：哦|.|.|.|"
     ]
    }
   ],
   "source": [
    "llm_stream = StreamChatWithOlderAI()\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n\\n夏洛：{question[0].content}\")\n",
    "    print(\"大爷：\", end=\"\")\n",
    "    for chunk in llm_stream.stream(question):\n",
    "        print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a7d4000-66db-4be9-9dae-e9ab7e14cbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "夏洛：楼上322住的是马冬梅家吗？\n",
      "大爷：马|什|么|梅|？|\n",
      "\n",
      "夏洛：马冬梅啊\n",
      "大爷：什|么|冬|梅|？|？|\n",
      "\n",
      "夏洛：马冬梅！\n",
      "大爷：马|东|什|么|？|？|？|\n",
      "\n",
      "夏洛：我是说马冬梅！\n",
      "大爷：马|什|么|梅|？|\n",
      "\n",
      "夏洛：大爷您歇着吧...\n",
      "大爷：哦|.|.|.|"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "questions_text = [\n",
    "    \"楼上322住的是马冬梅家吗？\",\n",
    "    \"马冬梅啊\",\n",
    "    \"马冬梅！\",\n",
    "    \"我是说马冬梅！\",\n",
    "    \"大爷您歇着吧...\"\n",
    "]\n",
    "\n",
    "llm_stream = StreamChatWithOlderAI()\n",
    "prompt = PromptTemplate.from_template(\"大爷，{question}\")\n",
    "chain = prompt | llm_stream | StrOutputParser()\n",
    "\n",
    "for question in questions_text:\n",
    "    print(f\"\\n\\n夏洛：{question}\")\n",
    "    print(\"大爷：\", end=\"\")\n",
    "    for chunk in chain.stream({\"question\": question}):\n",
    "        print(chunk, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae412a-cfd5-4160-80df-a6265560cd34",
   "metadata": {},
   "source": [
    "### 🌹 审视这个模拟大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4aeecb41-c94c-4046-817f-8642d0ecb937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StreamChatWithOlderAI |  \n",
      "+-----------------------+  \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()\n",
    "# llm_stream.input_schema.schema()\n",
    "# chain.input_schema.schema()\n",
    "# prompt.output_schema.schema()\n",
    "# prompt.invoke({\"question\":\"你好\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f664542-8ea0-4cba-ba61-735c0fea6b04",
   "metadata": {},
   "source": [
    "## ❤️ 实例2 集成智谱大模型到 Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9e4176-391c-41a4-bc21-988f7b295843",
   "metadata": {},
   "source": [
    "### 🦜 需求分析：结合智谱官方文档，编写langchain的ChatZhipuAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ff16f8-4f04-4639-9cbb-5aead5c88076",
   "metadata": {},
   "source": [
    "[访问智谱AI官方的Python接口文档](https://maas.aminer.cn/dev/api#sdk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b7ebf3-ec67-4cbc-8c13-3cba71d7461f",
   "metadata": {},
   "source": [
    "#### （1）可用接口\n",
    "- 直接调用：invoke\n",
    "- 异步调用（先调用，再查询结果）：适合实现 ainvoke / batch / abatch\n",
    "- 流式调用（SSE，Server-Send Events)：适合实现 stream / astream / stream_log / stream_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859d217d-27e3-4802-9ea6-e5d585a2bd01",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>⚠️ 思考：</b><br>\n",
    "    在哪些场景下可以充分利用官方API提供的异步能力？\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf4066-d16d-4307-93d5-d54f56cdd3a2",
   "metadata": {},
   "source": [
    "#### （2）支持能力\n",
    "- 支持 工具回调：形式上支持多工具返回\n",
    "- 支持 识别图片\n",
    "- 支持 生成图片"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f4f39d-46f4-401f-93c3-b6a59a687754",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>⚠️ 思考：</b><br>\n",
    "    能否实现一个自定义大模型，支持多模态能力？\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dbbbff-1483-487a-8648-141b5f64fc99",
   "metadata": {},
   "source": [
    "#### （3）速率限制\n",
    "\n",
    "[智谱AI官方文档中对速率限制的说明](https://maas.aminer.cn/dev/howuse/rate-limits/why?tab=5)：\n",
    "\n",
    "> 当前我们基于用户的月度 API 调用消耗金额情况将速率控制分为6种等级。\n",
    ">\n",
    "> 消耗金额选取逻辑：我们会选取用户当前月份1号～t-1日的调用 API 推理消耗总金额和用户上个月的 API 调用消耗总金额做比较，取更高金额作为用户当前的 API 消耗金额。\n",
    ">\n",
    "> 特别的，若您从未曾付费充值/购买过资源包，则会归为免费级别。\n",
    "\n",
    "**整理GLM4模型使用限制如下：**\n",
    "|用户等级|使用量|GLM4并发限制|\n",
    "|:---|:---|:---|\n",
    "|免费|api调用消耗0元-50元/每月（不含）|5|\n",
    "|使用量1|api调用消耗50元-500元/每月（不含）|10|\n",
    "|使用量2|api调用消耗500元-5000元/每月（不含）|20|\n",
    "|使用量3|api调用消耗5000元-10000元/每月（不含）|30|\n",
    "|使用量4|api调用消耗10000元-30000元/每月（不含）|100|\n",
    "|使用量5|api调用消耗30000元以上/每月|200|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3941a68e-e6de-40f9-8ecb-610556b92296",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>⚠️ 思考：</b><br>\n",
    "    能否实现一个自定义大模型，通过多个低用量账户管理机制来扩大调用速率？\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e18cd-e44e-4f28-a8a2-819f719c4daf",
   "metadata": {},
   "source": [
    "### ✍️ 测试官方例子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c317160f-22fe-4210-bc69-ed83abacfeca",
   "metadata": {},
   "source": [
    "看官方例子：[https://github.com/MetaGLM/zhipuai-sdk-python-v4](https://github.com/MetaGLM/zhipuai-sdk-python-v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9dad0716-95a2-4966-bf90-07d33be54cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_zhipu import ZhipuAI\n",
    "# from zhipuai import ZhipuAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9681792b-ab7d-4269-b9d3-8f55a25ce0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='你好，我叫智谱清言，是基于智谱AI公司于2023年训练的ChatGLM开发的。很高兴见到你，欢迎问我任何问题。' role='assistant' tool_calls=None\n"
     ]
    }
   ],
   "source": [
    "## 看看官方的例子是否能正确运行\n",
    "client = ZhipuAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"你叫什么名字\"},\n",
    "    ],\n",
    ")\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9835dc40-1269-4415-8127-70f62f8ea0a0",
   "metadata": {},
   "source": [
    "### ✍️ 包装为一个 函数调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9c782a4d-663b-423c-8cd7-3d63a7893e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is your name?'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask_zhipu(question) -> str:\n",
    "    client = ZhipuAI()\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一个翻译机器人，我说中文你就直接翻译成英文，我说英文你就直接翻译为中文。不要输出其他，不要啰嗦。\"},\n",
    "        {\"role\": \"user\", \"content\": \"你好\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"hello\"},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    return(response.choices[0].message.content)\n",
    "\n",
    "ask_zhipu(\"你叫什么名字？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8f6f0e-3891-4a30-91a8-afcfd2cf5d08",
   "metadata": {},
   "source": [
    "### ✍️ 支持与 Prompt 协作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2572d41a-b2a6-4b81-9426-f083081f86ac",
   "metadata": {},
   "source": [
    "```python\n",
    "# 能否实现如下场景？\n",
    "chain = prompt | llm\n",
    "chain.invoke({\"question\": \"你叫什么名字？\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd50c3d-5a21-4d6b-96e2-76a2e5ae5566",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>⚠️ 思考</b><br>\n",
    "    下面代码中， 为什么 from_messages 支持 system、human、ai 这些名字？还有其他名字吗？文档在哪里？\n",
    "</div>\n",
    "\n",
    "**🌞 答案：**\n",
    "- [查看 _create_message_from_message_type() 源码](https://github.com/langchain-ai/langchain/blob/c93d4ea91cfcf55dfe871931d42aa22562f8dae2/libs/core/langchain_core/messages/utils.py#L130-L168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "024270e5-a030-421f-8af7-af64c1251dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='你是一个翻译机器人，我说中文你就直接翻译成英文，我说英文你就直接翻译为中文。不要输出其他，不要啰嗦。'), HumanMessage(content='你好'), AIMessage(content='hello'), HumanMessage(content='你叫什名字？')])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个翻译机器人，我说中文你就直接翻译成英文，我说英文你就直接翻译为中文。不要输出其他，不要啰嗦。\"),\n",
    "    (\"human\", \"你好\"),\n",
    "    (\"ai\", \"hello\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "prompt.invoke({\"question\":\"你叫什名字？\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "5d90cc73-c09f-44c0-92bf-4bee04489894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='你是一个翻译机器人，我说中文你就直接翻译成英文，我说英文你就直接翻译为中文。不要输出其他，不要啰嗦。'),\n",
       " HumanMessage(content='你好'),\n",
       " AIMessage(content='hello'),\n",
       " HumanMessage(content='你叫什名字？')]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"question\":\"你叫什名字？\"}).to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "a48adc15-0507-4950-bb27-f034e5a5ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "\n",
    "def convert_message_to_dict(message: BaseMessage) -> dict:\n",
    "    \"\"\"从langchain消息格式转换为大模型字典格式\"\"\"\n",
    "    message_dict: Dict[str, Any]\n",
    "    if isinstance(message, HumanMessage):\n",
    "        message_dict = {\"role\": \"user\", \"content\": message.content}\n",
    "    elif isinstance(message, AIMessage):\n",
    "        message_dict = {\"role\": \"assistant\", \"content\": message.content}\n",
    "        if \"tool_calls\" in message.additional_kwargs:\n",
    "            message_dict[\"tool_calls\"] = message.additional_kwargs[\"tool_calls\"]\n",
    "    elif isinstance(message, SystemMessage):\n",
    "        message_dict = {\"role\": \"system\", \"content\": message.content}\n",
    "    elif isinstance(message, ToolMessage):\n",
    "        message_dict = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": message.content,\n",
    "            \"tool_call_id\": message.tool_call_id,\n",
    "        }\n",
    "    else:\n",
    "        raise TypeError(f\"Unknown type from langchain to LLM: {type(message).__name__} {message}\")\n",
    "    return message_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "a24c1316-3741-49a7-85da-806b5fcd2e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '你是一个翻译机器人，我说中文你就直接翻译成英文，我说英文你就直接翻译为中文。不要输出其他，不要啰嗦。'},\n",
       " {'role': 'user', 'content': '你好'},\n",
       " {'role': 'assistant', 'content': 'hello'},\n",
       " {'role': 'user', 'content': '你叫什名字？'}]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[convert_message_to_dict(m) for m in prompt.invoke({\"question\":\"你叫什名字？\"}).to_messages()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7648a6-6ab7-4a62-81b3-d0b02c553ea2",
   "metadata": {},
   "source": [
    "### ✍️ 包装为一个 RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "63e5a0a3-433d-48e7-950b-fc3d8a445e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain\n",
    "from typing import List\n",
    "from langchain_core.prompt_values import ChatPromptValue \n",
    "\n",
    "@chain\n",
    "def ask_zhipu(promptValue: ChatPromptValue) -> str:\n",
    "    client = ZhipuAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4\",\n",
    "        messages=[convert_message_to_dict(m) for m in promptValue.to_messages()],\n",
    "    )\n",
    "    return(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "367b9894-8a00-4913-b542-ed0f6e73ec91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is your name?'"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | ask_zhipu\n",
    "chain.invoke({\"question\": \"你叫什名字？\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "41dae6d9-9301-4c56-bc78-96f1ad2f758c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    +-------------+    \n",
      "    | PromptInput |    \n",
      "    +-------------+    \n",
      "           *           \n",
      "           *           \n",
      "           *           \n",
      "+--------------------+ \n",
      "| ChatPromptTemplate | \n",
      "+--------------------+ \n",
      "           *           \n",
      "           *           \n",
      "           *           \n",
      "+-------------------+  \n",
      "| Lambda(ask_zhipu) |  \n",
      "+-------------------+  \n",
      "           *           \n",
      "           *           \n",
      "           *           \n",
      " +------------------+  \n",
      " | ask_zhipu_output |  \n",
      " +------------------+  \n"
     ]
    }
   ],
   "source": [
    "# 看看当前链的结构\n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa150d60-9630-4597-bd2f-16524238f899",
   "metadata": {},
   "source": [
    "### ✍️ 基于BaseChatModel实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "a0374ab1-7121-4513-869e-bbc5ab56302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from typing import Any, Dict, Iterator, List, Optional, cast, Mapping\n",
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    AIMessage,\n",
    "    BaseMessage, \n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "from langchain_core.outputs import (\n",
    "    ChatGeneration,\n",
    "    ChatResult\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "d46fb809-bfbd-4035-9de4-0a1d52a6da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_to_message(_dict: Mapping[str, Any]) -> BaseMessage:\n",
    "    \"\"\"从大模型字典格式转换为langchain消息格式\"\"\"\n",
    "    role = _dict.get(\"role\")\n",
    "    if role == \"user\":\n",
    "        return HumanMessage(content=_dict.get(\"content\", \"\"))\n",
    "    elif role == \"assistant\":\n",
    "        content = _dict.get(\"content\", \"\") or \"\"\n",
    "        additional_kwargs: Dict = {}\n",
    "        if tool_calls := _dict.get(\"tool_calls\"):\n",
    "            additional_kwargs[\"tool_calls\"] = tool_calls\n",
    "        return AIMessage(content=content, additional_kwargs=additional_kwargs)\n",
    "    elif role == \"system\":\n",
    "        return SystemMessage(content=_dict.get(\"content\", \"\"))\n",
    "    elif role == \"tool\":\n",
    "        additional_kwargs = {}\n",
    "        if \"name\" in _dict:\n",
    "            additional_kwargs[\"name\"] = _dict[\"name\"]\n",
    "        return ToolMessage(\n",
    "            content=_dict.get(\"content\", \"\"),\n",
    "            tool_call_id=_dict.get(\"tool_call_id\"),\n",
    "            additional_kwargs=additional_kwargs,\n",
    "            id=id_,\n",
    "        )\n",
    "    else:\n",
    "        raise TypeError(f\"Unknow type from LLM to langchain: {_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "be2ac9a7-62e2-45c8-a0b5-5b0d97824ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniZhipuAI(BaseChatModel):\n",
    "    \"\"\"支持最新的智谱API\"\"\"\n",
    "\n",
    "    client: Optional[ZhipuAI] = None\n",
    "\n",
    "    def __init__(self, *args: Any, **kwargs: Any) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.client = ZhipuAI()\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return the type of chat model.\"\"\"\n",
    "        return \"zhipuai\"\n",
    "\n",
    "    def _ask_remote(self, messages, streaming=False, **kwargs):\n",
    "        # 从langchain消息格式，转换到智谱AI输入的格式\n",
    "        dict_zhipu = [convert_message_to_dict(m) for m in messages]\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"glm-4\",\n",
    "            messages=dict_zhipu,\n",
    "            stream=streaming,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # 从智谱AI输出的格式，转换到langchain的消息格式\n",
    "        if not isinstance(response, dict):\n",
    "            response = response.dict()\n",
    "        return [convert_dict_to_message(c[\"message\"]) for c in response[\"choices\"]]\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        stream: Optional[bool] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        \"\"\"实现 ZhiputAI 的同步调用\"\"\"\n",
    "\n",
    "        # 问智谱AI，并得到回复\n",
    "        responses = self._ask_remote(messages, streaming=False, **kwargs)\n",
    "\n",
    "        return ChatResult(\n",
    "            generations=[ChatGeneration(message=m) for m in responses]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "fee54840-3dfb-4d6d-ae4b-6a07ba3b2d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='中美在人工智能领域的竞争非常激烈。中国能赶得上吗？')"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个中英互译机器人，只负责翻译，不要试图对问题做解答。我说中文你就直接翻译成英文，我说英文你就直接翻译为中文。不要输出其他，不要啰嗦。\"),\n",
    "    (\"human\", \"你好\"),\n",
    "    (\"ai\", \"hello\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "llm_zhipu = MiniZhipuAI()\n",
    "chain = prompt | llm_zhipu\n",
    "\n",
    "chain.invoke({\"question\": \"The competition between China and the United States in the AI field is very intense. Can China catch up?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7467f98a-f6c6-4443-b0ef-ef906cf26111",
   "metadata": {},
   "source": [
    "### ✍️ 尝试调用工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "2adeaac3-3dd6-4a5e-9591-7da1b31660c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个简单的工具\n",
    "from langchain.tools import tool\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool, convert_to_openai_function\n",
    "import re\n",
    "\n",
    "@tool\n",
    "def ask_neighber(query: str) -> str:\n",
    "    \"\"\"我在玩找人的游戏，我知道你找的人住在哪个房间\"\"\"\n",
    "    if(re.search(\"马冬梅\", query)):\n",
    "        return \"楼上322\"\n",
    "    else:\n",
    "        return \"我不清楚\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "98d510aa-b493-4920-968b-909cfdfd2933",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_zhipu = MiniZhipuAI().bind(tools=[convert_to_openai_tool(ask_neighber)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "52acb9b5-9948-4de6-b364-c1da895d1a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_8513183423441263269', 'function': {'arguments': '{\"query\":\"马冬梅\"}', 'name': 'ask_neighber'}, 'type': 'function'}]})"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_zhipu.invoke(\"告诉我马冬梅在哪个房间？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "9eb8a61d-573c-468d-b9df-62516c521e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'楼上322'"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_neighber.invoke({\"query\":\"马冬梅\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b829b6-ec99-4077-9608-59ed42add56c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>⚠️ 思考：</b><br>\n",
    "    openai_function 和 convert_to_openai_tool 的区别是什么？\n",
    "</div>\n",
    "\n",
    "**🌞 参考：**\n",
    "- [查看 convert_to_openai_tool 的实现源码](https://github.com/langchain-ai/langchain/blob/c93d4ea91cfcf55dfe871931d42aa22562f8dae2/libs/core/langchain_core/utils/function_calling.py#L323-L341)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "53631752-d072-4087-ae58-9a8eafee22ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'ask_neighber',\n",
       " 'description': 'ask_neighber(query: str) -> str - 我在玩找人的游戏，我知道你找的人住在哪个房间',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'query': {'type': 'string'}},\n",
       "  'required': ['query']}}"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_openai_function(ask_neighber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "a1f6fefc-f96f-446f-a72b-de3cad4a9afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'ask_neighber',\n",
       "  'description': 'ask_neighber(query: str) -> str - 我在玩找人的游戏，我知道你找的人住在哪个房间',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'type': 'string'}},\n",
       "   'required': ['query']}}}"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_openai_tool(ask_neighber)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7486f46b-8c36-49ba-aec8-5340206443fe",
   "metadata": {},
   "source": [
    "### ✍️ 尝试调用智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "5c09ac0d-f981-4c50-a2c7-fc41f082618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, Tool, create_openai_tools_agent\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "tools = [ask_neighber]\n",
    "\n",
    "agent = create_openai_tools_agent(llm_zhipu, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "fc382cdb-9ffc-40fa-a16c-41b5eb371c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `ask_neighber` with `{'query': '马冬梅住哪里'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m楼上322\u001b[0m\u001b[32;1m\u001b[1;3m根据我的查询结果，马冬梅住在楼上322房间。希望这个信息对您有所帮助！\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '马冬梅住哪里', 'output': '根据我的查询结果，马冬梅住在楼上322房间。希望这个信息对您有所帮助！'}"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"马冬梅住哪里\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77770d1-c8fa-4967-8142-e91d012652e9",
   "metadata": {},
   "source": [
    "### 🌹 ChatZhipuAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ff552d-78cd-41c0-a302-b9a695c2887d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>⚠️ 思考：</b><br>\n",
    "    上面代码已经相对完整实现了一个 langchain 大模型；但缺少很多细节控制，可以尝试自己动手添加！\n",
    "</div>\n",
    "\n",
    "**🌞 参考：**\n",
    "- [查看 langchain_zhpu 中的实现源码](https://github.com/arcstep/langchain_zhipuai/blob/e55af13eed673bc409ffdb143030e6cc0b2af27c/langchain_zhipu/chat.py#L304-L354)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "91fddecd-fd0c-41f7-b262-114a36d54145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我|使用的|模型|是|清华大学| K|EG| 实|验|室|和|智|谱|AI|共同|训练|的| GL|M| 模|型|，|一种|基于| Transformer| 的|通用|预|训练|语言|模型|。|Transformer| 模|型|是一种|基于|自|注意力|机制的|深度|神经网络|模型|，|经常|用于|处理|序列|数据|。\n",
      "\n",
      "我|可能|用到|最大的|模型|是| GL|M|-|130|B|，|具有| |130|0| 亿|参数|，|支持|中|英|双语|。|我|具体|使用的|模型|规模|视|应用|场景|可能会有|所|变化|。||"
     ]
    }
   ],
   "source": [
    "from langchain_zhipu import ChatZhipuAI\n",
    "\n",
    "llm_zhipuai = ChatZhipuAI()\n",
    "for chunk in llm_zhipuai.stream(\"你是什么模型\"):\n",
    "    print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "bc557ddb-fa06-4fbf-90da-d9418123dab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|我|是|一个|基|于|人|工|智|能|技|术|的|对|话|模|型|，|可以|进行|自|然|语|言|交|互|并|提|供|相关|信息|和|帮|助|。|我|不|是|特|定|的|机|器|学|习|模|型|，|而|是|由|多|种|技|术|和|算|法|组|合|而|成|的|。||"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "langchain_openai = ChatOpenAI()\n",
    "for chunk in langchain_openai.stream(\"你是什么模型\"):\n",
    "    print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00700e0-ac72-424a-a3f3-1bd1b41adc4e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>⚠️ 思考：</b><br>\n",
    "    事实上，看起来没有 stream 的 RunnableLambda 可以返回一个具有 stream 能力的 Runnable！你知道这是为什么？\n",
    "</div>\n",
    "\n",
    "**答案：**\n",
    "- [查看 RunnableLambda 源码](https://github.com/langchain-ai/langchain/blob/c93d4ea91cfcf55dfe871931d42aa22562f8dae2/libs/core/langchain_core/runnables/base.py#L4056-L4071)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb2e199-6d19-47db-888d-ea07303af597",
   "metadata": {},
   "source": [
    "## 版本1\n",
    "\n",
    "### 看看智谱官方的简单例子\n",
    "### Langchain 相关库源码\n",
    "### 写一个简单的 Langchain 版本\n",
    "### 在LCEL中使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630b0bf2-030c-40e3-8702-153cddd37031",
   "metadata": {},
   "source": [
    "## 版本2\n",
    "\n",
    "### 试试流：暂时不支持\n",
    "### 看看智谱官方对流的支持\n",
    "### 看看流的默认实现源码\n",
    "### 让我们也支持流"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7e820d-46fc-4bcb-8264-ce0c34e416b4",
   "metadata": {},
   "source": [
    "## 版本3\n",
    "\n",
    "### 试试工具：暂时不支持\n",
    "### 看看智谱官方对工具回调的支持\n",
    "### 回顾工具回调：只是一种消息格式\n",
    "### 看看 openai 实现源码\n",
    "### 让我们也支持工具回调\n",
    "### 在智能体中使用\n",
    "### 完整代码请参考 langchain_zhipu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318c36b2-6d05-4e49-bbd5-34d231c08ae1",
   "metadata": {},
   "source": [
    "## 1、✍️ 代码准备：langchain中的LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d17151-50f4-4d7c-97a6-6df12706b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM invoke\n",
    "# OpenAI\n",
    "\n",
    "# LLM stream\n",
    "# OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee8b752-1a25-4bb9-844e-e92308966b51",
   "metadata": {},
   "source": [
    "### 2、✍️ 代码实践：如何实现一个FakeLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d7f2b17-1585-4fe5-bfce-65ff0d9d28d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 马冬梅楼下老大爷"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68825344-97a0-4b23-b27b-d7aea3019cad",
   "metadata": {},
   "source": [
    "[已经实现的FakeLLM](https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/language_models/fake.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0b74ef-7182-4c5a-a185-371b9266b90f",
   "metadata": {},
   "source": [
    "### 3、🌹 阅读源码：集成自己的大模型需要如何下手？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8e1905-cbbe-4339-bf5b-9ff81836a756",
   "metadata": {},
   "source": [
    "### 4、✍️ 代码实践：一步步集成智谱AI大模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fdcc70-1271-4e35-8e06-9d8ae37f48ea",
   "metadata": {},
   "source": [
    "- 支持 invoke\n",
    "- 支持 stream\n",
    "- 支持 tools-calling\n",
    "\n",
    "- 完整实现：[langchain_zhipu](https://github.com/arcstep/langchain_zhipu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358c0a94-ec25-4bec-bdff-93acc9e025bf",
   "metadata": {},
   "source": [
    "# ❤️（二）LLM输入输出：全面拆解 Runnable 结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b4800b-4b93-4a89-8040-397b53c402dc",
   "metadata": {},
   "source": [
    "### 1、大模型调用过程中发生的数据流转换"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2077bf62-1adb-4e92-b2fe-80cbb5265342",
   "metadata": {},
   "source": [
    "- 从训过的模型说起：训练预料和预测文本风格（|<assistent>| xxxxxx |</assistent>|)\n",
    "- 作为API提供：openAI风格\n",
    "- 从提示语到langchain消息：BaseMessages\n",
    "- 从langchain消息到大模型客户端：BaseChatModel\n",
    "- 从大模型结果到langchain消息：BaseMessages\n",
    "- 从langchain消息到输出解析器：OutpurParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a6a02-27aa-4996-81ff-0c27da433aef",
   "metadata": {},
   "source": [
    "### 2、如果在这个过程中统一各模块标准，随时更换？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59d13c5-663b-440c-92fd-9d1c51eb630d",
   "metadata": {},
   "source": [
    "### 3、如果在这个过程中使用流、异步、批量、事件流 ... ？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09417b7-12b2-4998-8d89-d5695140a5fa",
   "metadata": {},
   "source": [
    "### 4、这个过程中的缓存、重试、配置 ... ？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee06284-0b63-4231-9aa7-a9dced804e0e",
   "metadata": {},
   "source": [
    "### 1、为什么说可以用 Runnable 组件在生产系统中搭积木？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e918dc61-89d0-47c0-877b-2489dcb44b47",
   "metadata": {},
   "source": [
    "- Runnable接口标准化的价值\n",
    "    - 替换大模型：从GPT到国产云模型、到开源自训模型的多层次尝试和落地是必要的\n",
    "    - 替换调用方式：满足业务验证、生产上线、业务扩容等多场景\n",
    "    - 替换提示语模板：适应技术研究、业务定制、运营优化等多阶段\n",
    "    - 替换解析器：适应模块调用、API调用等多协议对接\n",
    "    - 替换回调集成：langsmith、langfuse云、langfuse本地化、自建运维系统\n",
    "    - 替换向量数据库：适应不同场景、阶段的技术选型调整\n",
    "    - 替换持久化：...\n",
    "    - 替换智能体：..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233dae74-6f40-4195-a825-cde34826bd07",
   "metadata": {},
   "source": [
    "### 2、🌹 阅读源码：Runnable 组件8个方法的实现逻辑"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5002f62-1c47-4157-9485-eaa10314a517",
   "metadata": {},
   "source": [
    "### 3、🌹 阅读源码：Runnable 组件配置自举能力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95d9e06-606f-4f72-b9fe-8ec9530eed05",
   "metadata": {},
   "source": [
    "- 了解Runnable 组件的 8 个方法的默认实现\n",
    "- 自定义Runnable（必要实现一般是invoke / stream / astream，或对应的内部函数）\n",
    "- invoke：标准化调度\n",
    "- batch: 标准化批量调度\n",
    "- stream：标准化流式输出\n",
    "- ainvoke / astream / abatch: 标准化异步调度\n",
    "- astream_log / astream_events: 在链、智能体、langgraph等输出中按照names、tags、events提取流式日志\n",
    "- config：统一管理配置\n",
    "- schema：统一探查参数和配置\n",
    "\n",
    "- 序列化\n",
    "\n",
    "- 配置自举：在开放式应用中支持客户端自动识别自定义服务\n",
    "- 容错：标准化重试策略\n",
    "- 与langserve等api框架标准化对接\n",
    "- 与langfuse等callbck框架标准化对接\n",
    "- 与langchainjs等异构实现标准化对接\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a62f75-52bd-462b-a396-8fb9751fc84a",
   "metadata": {},
   "source": [
    "### 4、🌹 阅读源码：遗留的 Chain 是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f68e5-b811-4854-af6f-7a8ded1ba8bd",
   "metadata": {},
   "source": [
    "- langchain中提前写好的Chain资源"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e3fa84-035e-4eb5-be59-4c3a902fea45",
   "metadata": {},
   "source": [
    "- 这些 Chain 局限性在究竟哪里？\n",
    "    - 流程不灵活\n",
    "    - 支持流式输出不彻底"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a0ddb0-134d-4641-ad0f-994df1e945b5",
   "metadata": {},
   "source": [
    "### 5、✍️ 代码实践：若干种情况下的流输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49b85a1-eda7-4c3c-80af-0a125e5b4fee",
   "metadata": {},
   "source": [
    "# ❤️ 第 2 部份：Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64598d5-cdac-4a60-96d9-52b44730d6a2",
   "metadata": {},
   "source": [
    "# ❤️（三）由LCEL实现智能体：全面拆解 LCEL 能力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af86c4c-549a-4eb6-9ddc-f07314e393aa",
   "metadata": {},
   "source": [
    "### 1、LCEL 比 遗留 Chain 多哪些优势？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76adf428-cdd4-4aa8-ac8b-3a3648ae7ead",
   "metadata": {},
   "source": [
    "- LCEL 构建的替代 Chain\n",
    "- 了解支撑LCEL的Runnable组件\n",
    "    - Lambda\n",
    "    - 迭代器\n",
    "    - 字典和并行\n",
    "    - 路由\n",
    "    - 条件\n",
    "    - 迭代执行\n",
    "    - 绑定\n",
    "    - 绘图"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9e9ba-3388-40f2-a8f0-38ea7b7b2efd",
   "metadata": {},
   "source": [
    "### 2、如何用 LCEL 定义智能体？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5bd7d1-572a-447b-89b6-528b45258be9",
   "metadata": {},
   "source": [
    "- 工具：定义一个简单工具\n",
    "- 智能体：\n",
    "    - Tools-Calling 智能体\n",
    "    - ReAct 智能体\n",
    "- 执行器：AgentExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d886c2-abec-4423-8742-068d144e66d3",
   "metadata": {},
   "source": [
    "### 3、🌹 阅读源码：了解 create_react_agent 的设计"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad31612-b006-4803-96a9-6bb824b1c25a",
   "metadata": {},
   "source": [
    "- 一个简单的智能体需求：与AI玩一个捉迷藏游戏\n",
    "- react智能体的prompt如何工作\n",
    "- 中间步骤的一步步产生过程\n",
    "- action如何解析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0378cc-4936-4ce2-abc6-9a7deebfa4c1",
   "metadata": {},
   "source": [
    "### 4、🌹 阅读源码：了解 AgentExecutor 的执行逻辑"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f243379b-0e24-42ce-9087-239608bf5882",
   "metadata": {},
   "source": [
    "### 4、✍️ 代码实践：如何用 AgentExcutor 再现《手撕AutoGPT》？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba7531-66a6-4fda-81dc-03a87a2a637f",
   "metadata": {},
   "source": [
    "难点：\n",
    "- 官方例子和内置智能体无法支持pydantic参数解析（智谱AI等推理能力较弱的模型可以使用）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702bd5b7-8aeb-4d5a-bd14-dfe0cc39e238",
   "metadata": {},
   "source": [
    "# ❤️（四）由LangGraph实现智能体：全面拆解 LangGraph 能力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b530de-c32e-4de7-a53d-2a7403ae881e",
   "metadata": {},
   "source": [
    "### 1、LangGraph 比 LCEL 多了什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28481dec-c573-4c30-ba7d-a557702475ee",
   "metadata": {},
   "source": [
    "### 2、如何使用 LangGraph 定义智能体？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b186660d-0f16-449c-82b1-ee02cb95d07f",
   "metadata": {},
   "source": [
    "### 3、🌹 阅读源码：了解 LangGraph 的执行逻辑"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eda0a46-ce13-4ce2-9a98-fb743118baaf",
   "metadata": {},
   "source": [
    "### 4、✍️ 代码实践：如何用 LangGraph 再现《手撕AutoGPT》？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed73748-3266-4a30-9468-f441ab10157c",
   "metadata": {},
   "source": [
    "难点：\n",
    "- 官方例子和内置智能体无法支持流"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bf21ca-cf12-4cd3-a53c-f451ccafa852",
   "metadata": {},
   "source": [
    "# ❤️ 课程结束"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284f9b10-8af0-4a42-80be-935f1fc6c853",
   "metadata": {},
   "source": [
    "## 1、课程总结\n",
    "\n",
    "- 我们一起阅读了langchain的源代码结构和部份细节\n",
    "    - BaseLanguageModel / BaseChatModel / \n",
    "    - Runnable\n",
    "    - LambdaRunnable\n",
    "    - Chain\n",
    "    - AgentExcutor\n",
    "    - langgraph.prebuild\n",
    "- 我们学习了如何自己动手集成大模型到 langchain 中\n",
    "- 我们拆解了langchain的基石组件：Runnable\n",
    "- 我们拆解了langchain的核心逻辑能力：LCEL\n",
    "- 我们拆解了langchain的最新逻辑能力：langgraph\n",
    "- 我们动手做了一些代码实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2454ac52-a105-45d0-b99e-aa8183cb5f29",
   "metadata": {},
   "source": [
    "## 2、最后建议\n",
    "\n",
    "- 技术选型时要对 Langchain 有绝对信心（几乎都不会是langchain的错）\n",
    "- 内置链尽量使用LCEL链\n",
    "- 内置智能体尽量使用 Langgraph\n",
    "- 自定义智能体时使用 Langgraph\n",
    "- 模块优先做成Runnable或LCEL链，其次再考虑Lambda\n",
    "- 工具中包含大模型调用时优先做成Runnable或LCEL链，其次再考虑invoke\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f473875b-dd97-48be-b285-dce1528a5b58",
   "metadata": {},
   "source": [
    "## 3、彩蛋"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a44e1a-5197-41d2-b6f8-669e59a6196e",
   "metadata": {},
   "source": [
    "### 1、✍️ 代码实践：如何同时使用langchain的记忆和持久化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4021d211-8b68-400e-976b-3250035a2e65",
   "metadata": {},
   "source": [
    "这是 langchain 文档中一个自相矛盾的地方，留给大家课后讨论。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa187b9-3d0c-4981-873b-56704520af13",
   "metadata": {},
   "source": [
    "### 2、✍️ 代码实践：如何将自己训的大模型集成到 langchain 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49e01f4-4317-46d4-889c-fbc923a1ff2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchani-book-py3.10-ipykernel",
   "language": "python",
   "name": "langchani-book-py3.10-ipykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
