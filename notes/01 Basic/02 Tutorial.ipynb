{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55faa189-72e3-49aa-a439-1c64b3feaaeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载 .env 到环境变量\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ed00a5-b48d-43d6-8c09-ce4203a0fa39",
   "metadata": {},
   "source": [
    "# langchain主要生态"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c6820e-d39c-41da-a2ec-cd9518200a6c",
   "metadata": {},
   "source": [
    "- langchain 模块：以LCEL方式开发DAG结构的chain，也可以作为访问远程chain的python客户端\n",
    "- langgraph 模块：让LCEL支持循环结构的chain\n",
    "- langserve 模块：将chain转化为API\n",
    "- langchain-js 模块：提供langchain的`js`版本，也可以作为访问远程chain的JS客户端\n",
    "- langsmith 模块：跟踪和调试的平台\n",
    "- langfuse 模块：langsmith的开源平替"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c52f1-580e-4efe-8151-fa6889e90896",
   "metadata": {},
   "source": [
    "# langchain模块的代码结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff186ec5-fa1e-42f4-9e40-df5fd980002b",
   "metadata": {},
   "source": [
    "- langchain：入口主模块\n",
    "- core：核心模块\n",
    "- community：社区贡献\n",
    "- partners：合作伙伴模块\n",
    "- experimetal：实验性模块\n",
    "- cli：命令行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228d94f8-5e5f-4579-a772-d40698223dba",
   "metadata": {},
   "source": [
    "# 快速开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1bf070-e5ca-4630-87e9-c20088c500c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 请使用最新的open包\n",
    "!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4f7f0b-5dfe-4768-85d3-bfe4231d2f33",
   "metadata": {},
   "source": [
    "## 定义大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "300f3790-56f9-4cbf-9840-2255a78a253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI().bind(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08679cf1-e498-4e03-ade2-2ce8cef6bc58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. 彩虹铅笔有限公司\\n2. 色彩创意铅笔厂\\n3. 彩绘铅笔制造厂\\n4. 彩色梦想铅笔公司\\n5. 色彩世界铅笔厂\\n6. 绚丽铅笔制造有限公司\\n7. 色彩艺术铅笔厂\\n8. 彩虹色铅笔创意工场\\n9. 色彩创意笔具有限公司\\n10. 绚烂铅笔艺术厂')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"请帮我想一想，生产彩色铅笔的公司有什么好名字?\"\n",
    "llm.invoke(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9860db1-b98a-4b6f-93c9-b432d5321902",
   "metadata": {},
   "source": [
    "## 定义提示语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6a5e725-15b2-411e-bc42-b2563f698087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'请帮我想一想，生产彩色铅笔的公司有什么好名字?'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"请帮我想一想，生产{product}的公司有什么好名字?\")\n",
    "prompt.format(product=\"彩色铅笔\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c0869b5-e737-4103-9a04-c27b94841ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='你是一个有用的翻译助手，从英文语言翻译到中文语言.'),\n",
       " HumanMessage(content='I love programming.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "template = \"你是一个有用的翻译助手，从{input_language}语言翻译到{output_language}语言.\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "chat_prompt.format_messages(input_language=\"英文\", output_language=\"中文\", text=\"I love programming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff33fb98-a31a-410e-9828-fd93f240c992",
   "metadata": {},
   "source": [
    "## 使用LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee60b12-1e94-47c4-92a1-7d524d2351ca",
   "metadata": {},
   "source": [
    "查看不加解析器的生成结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cb0653dc-33ca-4e2e-9f04-48a2d0ebb97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='红色, 蓝色, 黄色, 绿色, 紫色')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = (prompt | llm).invoke({\"product\": \"颜色\"})\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0f6e6334-1a45-45c1-b8e5-6dbb104c4a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'红色, 蓝色, 黄色, 绿色, 紫色'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 提取文本\n",
    "resp.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04ee16f-bd06-4403-b0c5-b20f400d371d",
   "metadata": {},
   "source": [
    "你也可以使用解析器直接提取文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "87732626-3c81-4430-994f-6c11203d6506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'红色, 蓝色, 黄色, 绿色, 紫色'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 注意 StrOutputParser 属于基础解析器，所以与 CommaSeparatedListOutputParser 位置不同\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "(prompt | llm | StrOutputParser()).invoke({\"product\": \"颜色\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08538769-1c49-432c-9fd0-6695a00b2d2b",
   "metadata": {},
   "source": [
    "## 按格式提取文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bb9bf1a8-f648-451b-af96-7971d03291e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "49fba3ca-c270-47b8-9e90-888ddcd22249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'bye']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这个解析器的基本能力就是按逗号分解字符串，生成一个列表\n",
    "output_parser.parse(\"hi, bye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c44e54-5203-4e7f-a0d5-9fafbfe6a019",
   "metadata": {},
   "source": [
    "使用这个输出解析器时，通常你要告诉大模型按照期待的逗号间隔的方式生成文本。<br>\n",
    "因此，你可以使用`get_format_instructions`生成的格式提示信息插入到提示语中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3b2255d0-5035-4c26-a5ab-54bf775f1fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your response should be a list of comma separated values, eg: `foo, bar, baz`'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "76c82f98-dbb1-4ff4-8bfe-97130ff2d497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['红色', '蓝色', '黄色', '绿色', '紫色']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 组装提示模板\n",
    "template = ChatPromptTemplate.from_template(\"生成5个关于{product}的列表.\\n\\n{format_instructions}\")\n",
    "prompt = template.partial(format_instructions=output_parser.get_format_instructions())\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "chain.invoke({\"product\": \"颜色\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfb3d7d-bfb4-4414-8c1e-acf29ce31be2",
   "metadata": {},
   "source": [
    "# 主要概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d4149-1f64-4fbc-b855-463168df5cdb",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cd6cee-0301-4b02-88c1-adce1224d6af",
   "metadata": {},
   "source": [
    "请记住：OpenAI模型对提示语中包含JSON的情况非常友好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23847910-53b3-4a07-9a56-055727270021",
   "metadata": {},
   "source": [
    "## 消息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aef250a-ad08-43da-9131-c6494f51608c",
   "metadata": {},
   "source": [
    "- HumanMessage： 一般是纯文字内容\n",
    "- AIMessage： 可能包含additional_kwargs，例如 funciton calling 提示\n",
    "- SystemMessage：部份模型支持的内容提示\n",
    "- FunctionMessage：函数调用的名称和参数\n",
    "- ToolMessage：工具调用结果（与FunctionMessage不同）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a6bd18-586d-4677-bc90-da4b964cc69d",
   "metadata": {},
   "source": [
    "## 提示语"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd7cc5-042a-4ef1-b856-b5fcf57b6fbb",
   "metadata": {},
   "source": [
    "- PromptValue\n",
    "- PromptTemplate\n",
    "- MessagePromptTemplate\n",
    "- MessagesPlaceholder\n",
    "- ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3b1cfa-af5b-4f73-8893-e0d1f77edd97",
   "metadata": {},
   "source": [
    "## Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb550b86-b53a-4bf4-83c6-9fb71b1a5279",
   "metadata": {},
   "source": [
    "- StrOutputParser：仅输出字符串；如果输出是 ChatModel，它会仅输出Message的content属性\n",
    "- OpenAI Functions Parsers：处理OpenAI函数调用所需的函数名和参数\n",
    "- Agent Output Parsers：帮助智能体解析执行计划"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "368a8d4f-ea4c-47bf-9b1c-ed6d6641eac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='你是一个有用的翻译助手，从English语言翻译到French语言.'), HumanMessage(content='I love programming.')]\n",
      "[SystemMessage(content='你是一个有用的翻译助手，从English语言翻译到French语言.'), HumanMessage(content='I love programming.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "parser = StrOutputParser()\n",
    "response = chat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n",
    "print(response)\n",
    "print(parser.parse(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff5372c-e711-4250-915e-87f667789a70",
   "metadata": {},
   "source": [
    "## Prompt封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f290ab2-7794-43f7-9202-16167fd1f1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['subject'] template='给我讲个关于{subject}的笑话'\n",
      "给我讲个关于小明的笑话\n"
     ]
    }
   ],
   "source": [
    "# 简单的例子\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"给我讲个关于{subject}的笑话\")\n",
    "print(template)\n",
    "print(template.format(subject='小明'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367ce3f3-89a8-49d5-9abe-55f301f7576e",
   "metadata": {},
   "source": [
    "### 字符串模板提示语"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eff1ae2-ef96-4a9c-a83c-c454be9f4419",
   "metadata": {},
   "source": [
    "#### 基本语法：PromptTemplate.from_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5732d9f1-0f47-424f-a2bc-8a9d731e9dc3",
   "metadata": {},
   "source": [
    "使用字符串提示时，每个模板都会连接在一起。<br>\n",
    "您可以直接使用prompt模板或字符串（但列表中的第一个元素必须是prompt模板类型）。\n",
    "\n",
    "在 langchain 内部，Prompt模板使用 jinja 模板的语法，主要是使用 \"{}\" 元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "683eda3d-fa95-4906-99e8-6d72a3d93ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['language', 'topic'], template='告诉我一个关于{topic}的笑话, 一定要特别好笑\\n\\\\使用 {language} 表述')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 提示语模板与字符串可以直接相加，简化模板构造\n",
    "from langchain.prompts import PromptTemplate\n",
    "prompt = (\n",
    "    PromptTemplate.from_template(\"告诉我一个关于{topic}的笑话\")\n",
    "    + \", 一定要特别好笑\"\n",
    "    + \"\\n\\使用 {language} 表述\"\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ba2bf6c-f77b-4d4b-9dd7-732a5e55ab1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'告诉我一个关于足球的笑话, 一定要特别好笑\\n\\\\使用 中文 表述'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(topic=\"足球\", language=\"中文\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd126b4-560f-4e32-8a50-fb2b8fc59f4e",
   "metadata": {},
   "source": [
    "#### 完整语法：PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5903d64f-d71b-416b-a67e-9160621af4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['language', 'topic'], template='告诉我一个关于{topic}的笑话, 一定要特别好笑\\n\\\\使用 {language} 表述', validate_template=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这是一个完整的函数调用，达到同样的效果\n",
    "from langchain.prompts import PromptTemplate\n",
    "PromptTemplate(\n",
    "    input_variables=['language', 'topic'],\n",
    "    output_parser=None,\n",
    "    partial_variables={},\n",
    "    template='告诉我一个关于{topic}的笑话, 一定要特别好笑\\n\\使用 {language} 表述',\n",
    "    template_format='f-string',\n",
    "    validate_template=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6a9fb7-cd4f-48a0-8470-4396b2eee288",
   "metadata": {},
   "source": [
    "#### 结合 Chain 使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e964c6d-56f9-442d-ac57-2bc34d0e1ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': '足球',\n",
       " 'language': '中文',\n",
       " 'text': '为什么足球比赛中的球员总是穿着裤子？\\n因为他们不想踢到“球”露出来！哈哈哈哈哈哈哈！'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 也可以直接在chain中给参数\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "chain = LLMChain(llm=model, prompt=prompt,output_parser=parser)\n",
    "\n",
    "chain.invoke({\"topic\": \"足球\", \"language\": \"中文\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c421968-1d5b-4bd9-b922-895ff4499dd5",
   "metadata": {},
   "source": [
    "### 对话模板提示语"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c52d8-061a-4d98-9924-95c0531d3c7e",
   "metadata": {},
   "source": [
    "#### 基本语法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5307906a-701b-4b5d-a6e0-83e9fb55f0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a nice pirate'),\n",
       " HumanMessage(content='hi'),\n",
       " AIMessage(content='what?'),\n",
       " HumanMessage(content='i said hi')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "prompt = SystemMessage(content=\"You are a nice pirate\")\n",
    "new_prompt = (\n",
    "    prompt + HumanMessage(content=\"hi\") + AIMessage(content=\"what?\") + \"{input}\"\n",
    ")\n",
    "new_prompt.format_messages(input=\"i said hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbee1fc-26f3-4ffe-b02b-4b1255d39f54",
   "metadata": {},
   "source": [
    "#### 使用字典构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb1e3b4d-b9b9-4895-9c9d-e79ac25ea315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n",
       " HumanMessage(content='Hello, how are you doing?'),\n",
       " AIMessage(content=\"I'm doing well, thanks!\"),\n",
       " HumanMessage(content='What is your name?')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "template.format_messages(\n",
    "    name=\"Bob\",\n",
    "    user_input=\"What is your name?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c922fb6-d4c3-43d2-8bf1-865b3b5bf3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a nice pirate'),\n",
       " HumanMessage(content='hi'),\n",
       " AIMessage(content='what?'),\n",
       " HumanMessage(content='i said hi')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.\n",
    "resp.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f4522e-74e6-4406-993f-153471093d4e",
   "metadata": {},
   "source": [
    "### 组装能力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24bc29a-4e70-487c-b11b-42b7a55b52c3",
   "metadata": {},
   "source": [
    "#### 在提示语中填充例子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602f9354-6e70-4a8d-9f2c-61d1011876ea",
   "metadata": {},
   "source": [
    "#### 在提示语中填充小样本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108e720-9d2b-4d6e-8448-73a121fb1e46",
   "metadata": {},
   "source": [
    "#### 局部修改提示语模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e60c5b4b-bdda-44b4-b63c-7641174b2c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foobaz\n"
     ]
    }
   ],
   "source": [
    "# 通过局部修改实现提示语管理\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(template=\"{foo}{bar}\", input_variables=[\"foo\", \"bar\"])\n",
    "partial_prompt = prompt.partial(foo=\"foo\")\n",
    "print(partial_prompt.format(bar=\"baz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5b27f319-bb29-4d8b-a2e0-bf7f7724b225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foobaz\n"
     ]
    }
   ],
   "source": [
    "# 或者这样做\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{foo}{bar}\", input_variables=[\"bar\"], partial_variables={\"foo\": \"foo\"}\n",
    ")\n",
    "print(prompt.format(bar=\"baz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e39a7fd8-b005-4048-9962-ee7d8a8d836c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about the day 01/23/2024, 16:27:57\n"
     ]
    }
   ],
   "source": [
    "# 使用函数\n",
    "from datetime import datetime\n",
    "\n",
    "def _get_datetime():\n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about the day {date}\",\n",
    "    input_variables=[\"adjective\", \"date\"],\n",
    ")\n",
    "partial_prompt = prompt.partial(date=_get_datetime)\n",
    "print(partial_prompt.format(adjective=\"funny\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "99bb0e90-a44d-43e4-9070-0013c758eca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about the day 01/23/2024, 16:28:51\n"
     ]
    }
   ],
   "source": [
    "# 换个方式使用函数\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about the day {date}\",\n",
    "    input_variables=[\"adjective\"],\n",
    "    partial_variables={\"date\": _get_datetime},\n",
    ")\n",
    "print(prompt.format(adjective=\"funny\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffe0cfc-e7ef-4d6a-8592-da87a5b42dc2",
   "metadata": {},
   "source": [
    "#### 提示语pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "15260c2d-780a-4528-9784-82ca6917ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "64f96217-df5b-466f-85b2-74416ef18aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_template = \"\"\"{introduction}\n",
    "\n",
    "{example}\n",
    "\n",
    "{start}\"\"\"\n",
    "full_prompt = PromptTemplate.from_template(full_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c467332b-5e3e-4380-adcd-f1e95566663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "introduction_template = \"\"\"You are impersonating {person}.\"\"\"\n",
    "introduction_prompt = PromptTemplate.from_template(introduction_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b6b87843-8eec-4f56-8c68-2f9a10dd1904",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_template = \"\"\"Here's an example of an interaction:\n",
    "\n",
    "Q: {example_q}\n",
    "A: {example_a}\"\"\"\n",
    "example_prompt = PromptTemplate.from_template(example_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0e5b0e85-5e0f-4dcf-8c11-7338bde9dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_template = \"\"\"Now, do this for real!\n",
    "\n",
    "Q: {input}\n",
    "A:\"\"\"\n",
    "start_prompt = PromptTemplate.from_template(start_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f86d36e8-584e-4b6e-8e5f-c4c5f75ad67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompts = [\n",
    "    (\"introduction\", introduction_prompt),\n",
    "    (\"example\", example_prompt),\n",
    "    (\"start\", start_prompt),\n",
    "]\n",
    "pipeline_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=full_prompt, pipeline_prompts=input_prompts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8ce75bcd-cb23-4807-8348-090b0c7d9436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person', 'example_a', 'example_q', 'input']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "94a7257f-71b8-496e-867c-fd2e91671994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are impersonating Elon Musk.\n",
      "\n",
      "Here's an example of an interaction:\n",
      "\n",
      "Q: What's your favorite car?\n",
      "A: Tesla\n",
      "\n",
      "Now, do this for real!\n",
      "\n",
      "Q: What's your favorite social media site?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    pipeline_prompt.format(\n",
    "        person=\"Elon Musk\",\n",
    "        example_q=\"What's your favorite car?\",\n",
    "        example_a=\"Tesla\",\n",
    "        input=\"What's your favorite social media site?\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d27a2ab-72b4-489a-ac09-8e779968fc78",
   "metadata": {},
   "source": [
    "## 对话模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b24936d-e423-42ac-b78a-9342b19d428f",
   "metadata": {},
   "source": [
    "### LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe3559e-bf45-4481-a1cf-03f01bc9205e",
   "metadata": {},
   "source": [
    "对话模型实现了Runnable接口，并自动实现以下接口：\n",
    "\n",
    "- invoke\n",
    "- ainvoke\n",
    "- stream\n",
    "- astream\n",
    "- batch\n",
    "- abatch\n",
    "- astream_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "eeaf9c3e-b786-405d-839a-98a1e52b2e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='模型在训练数据上表现良好，但在新的未见过的数据上可能会出现过拟合（overfitting）的情况。正则化是一种用来解决过拟合问题的技术。它通过在损失函数中引入一个正则化项，限制模型的复杂度，防止模型过度拟合训练数据。\\n\\n正则化的目的是平衡模型的拟合能力和泛化能力。如果模型过于复杂，它可能会过度拟合训练数据，导致在新数据上的表现较差。正则化通过对模型参数的惩罚，鼓励模型选择更简单的参数组合，从而降低模型的复杂度。\\n\\n常见的正则化方法包括L1正则化和L2正则化。L1正则化通过在损失函数中添加模型参数的绝对值之和，促使模型参数稀疏化，即让一些参数变为0，从而实现特征选择的效果。L2正则化通过在损失函数中添加模型参数的平方和，降低参数的绝对值，使模型更加平滑。\\n\\n正则化可以帮助减少模型的方差，提高模型的泛化能力，从而在新的未见过的数据上表现更好。它是训练模型时常用的一种技术，可以提高模型的稳定性和可靠性。')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(content=\"模型为什么要做正则化?\"),\n",
    "]\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0e480cbc-857b-48a8-801f-2ab3db89e053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型正则化是为了减少过拟合（Overfitting）的发生。在训练模型时，如果模型过于复杂，容易出现过拟合的情况，即在训练集上表现很好，但在未知数据上表现较差。过拟合的原因是因为模型过度拟合了训练数据的噪声和细节，并且没有很好地学习到数据的普遍规律。\n",
      "\n",
      "正则化通过在模型的损失函数中引入正则项，对模型的复杂度进行惩罚，从而降低模型的复杂度。正则化的目的是通过控制模型参数的大小，使模型更加简单，能够更好地泛化到未知数据上。常见的正则化方法有L1正则化和L2正则化。\n",
      "\n",
      "L1正则化通过在损失函数中添加模型参数的L1范数（绝对值之和）作为正则项，可以使得模型的部分参数变为0，从而实现特征选择的效果，减少模型的复杂度。\n",
      "\n",
      "L2正则化通过在损失函数中添加模型参数的L2范数（平方和的平方根）作为正则项，可以使得模型参数的值较小，从而降低模型的复杂度。\n",
      "\n",
      "正则化可以在一定程度上防止过拟合，提高模型的泛化能力，使模型在未知数据上表现更好。"
     ]
    }
   ],
   "source": [
    "for chunk in chat.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b0c500c3-791d-415a-b23d-7dabbe6590ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='模型正则化是为了解决过拟合问题。过拟合是指模型在训练数据上表现良好，但在新的未见过的数据上表现较差的现象。正则化通过在模型的损失函数中添加一个正则化项，惩罚模型的复杂度，从而限制模型的学习能力，减少模型对训练数据的过度拟合。\\n\\n正则化有助于提高模型的泛化能力，使其在新数据上的表现更好。常见的正则化方法包括L1正则化（Lasso）和L2正则化（Ridge），它们分别通过对模型的权重进行惩罚，降低模型的复杂度。正则化还可以用于特征选择，通过对特征的权重进行惩罚，减少对不相关特征的依赖。\\n\\n总之，模型正则化是为了防止过拟合，提高模型的泛化能力，从而使模型在未见过的数据上表现更好。')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.batch([messages])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56846f44-a867-4e8b-805b-f7b331ca798e",
   "metadata": {},
   "source": [
    "### 使用内存缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b18d5c06-64c4-41eb-82ff-89b018a7e7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "18730f67-1871-4e90-92a6-46accda6fb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 ms, sys: 2.09 ms, total: 13.3 ms\n",
      "Wall time: 1.98 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure, here's a classic one for you:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ad411a-f696-49a7-9bf5-a743d1f473a0",
   "metadata": {},
   "source": [
    "下面的相同调用不会重复访问大模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5829b899-c311-4e6d-b0cf-08ce66197e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.24 ms, sys: 101 µs, total: 1.34 ms\n",
      "Wall time: 2.09 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure, here's a classic one for you:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The second time it is, so it goes faster\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa6191e-2a40-4e90-8d17-8ccdd7fc0b6e",
   "metadata": {},
   "source": [
    "### 使用SQLite缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3383d107-bac8-4304-bfa7-4fcf44a359f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: .langchain.db: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm .langchain.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fe5edcbc-0dee-4a1a-85a6-f08c329d4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do the same thing with a SQLite cache\n",
    "from langchain.cache import SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "67f251c9-4e68-4d62-abc8-ac595a289b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.23 ms, sys: 1.27 ms, total: 4.49 ms\n",
      "Wall time: 3.59 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sure, here's a classic one for you:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\""
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm.predict(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5943571d-f4c3-49f6-b7c4-c37e9f9f265b",
   "metadata": {},
   "source": [
    "### Token跟踪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d2b4508d-7010-446e-8744-32328bcb0823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 35\n",
      "\tPrompt Tokens: 12\n",
      "\tCompletion Tokens: 23\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $6.4e-05\n",
      "Tokens Used: 32\n",
      "\tPrompt Tokens: 13\n",
      "\tCompletion Tokens: 19\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00153\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm.invoke(\"Tell me a new joke\")\n",
    "    print(cb)\n",
    "\n",
    "llm4 = ChatOpenAI(model_name=\"gpt-4\")\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm4.invoke(\"Tell me a new apple joke\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f738e594-1077-47e8-aa96-3fd05bb13ca1",
   "metadata": {},
   "source": [
    "## LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96cd1c1-bcd6-483a-93cb-17400524130e",
   "metadata": {},
   "source": [
    "### OpenAI封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "348c977d-3a63-4665-b7a0-824a95acb6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是一个AI助手，被称为OpenAI Assistant。我被设计用来回答各种问题和提供帮助。有什么我可以帮助你的吗？\n"
     ]
    }
   ],
   "source": [
    "# 最简单的代码\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI() # 默认是gpt-3.5-turbo\n",
    "response = llm.invoke(\"你是谁\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7f06c9-7446-41ca-b0a1-550947528351",
   "metadata": {},
   "source": [
    "### 通义千问封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad698bbe-c212-44ef-8474-2a3c435a0c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using version \u001b[39;1m^1.14.0\u001b[39;22m for \u001b[36mdashscope\u001b[39m\n",
      "\n",
      "\u001b[34mUpdating dependencies\u001b[39m\n",
      "\u001b[2K\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(0.8s)\u001b[39;22m\n",
      "\n",
      "\u001b[39;1mPackage operations\u001b[39;22m: \u001b[34m1\u001b[39m install, \u001b[34m0\u001b[39m updates, \u001b[34m0\u001b[39m removals\n",
      "\n",
      "  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdashscope\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m1.14.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mPending...\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdashscope\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m1.14.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m0%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdashscope\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m1.14.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m80%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdashscope\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m1.14.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m100%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdashscope\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m1.14.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mInstalling...\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdashscope\u001b[39m\u001b[39m (\u001b[39m\u001b[32m1.14.0\u001b[39m\u001b[39m)\u001b[39m\n",
      "\n",
      "\u001b[34mWriting lock file\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "!poetry add dashscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6abfb62c-edcc-4956-b574-d337d0bffcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我是阿里云开发的一款超大规模语言模型，我叫通义千问。'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 其它模型分装在 langchain_community 底包中\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Tongyi\n",
    "\n",
    "ty_llm = Tongyi()\n",
    "messages = [\n",
    "    HumanMessage(content=\"你是谁\") \n",
    "]\n",
    "ty_llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64d544b-73a6-4365-ae51-0b90d194bf30",
   "metadata": {},
   "source": [
    "### 自定义LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3feb9dff-a446-4206-9a07-8c1d1458fb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Optional\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e45d4573-2a6d-4721-a463-e5c83ff98003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现一个定制的LLM接入\n",
    "class CustomLLM(LLM):\n",
    "    n: int\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        return prompt[: self.n]\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"n\": self.n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "400693d7-e405-42a0-9ffb-6ec070532f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = CustomLLM(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "31702b80-091f-4f61-84b8-bc86ca812cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a '"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"This is a foobar thing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "26dc185d-8967-4fe9-97b1-a38da9409394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCustomLLM\u001b[0m\n",
      "Params: {'n': 10}\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3974273d-92e1-4dff-be9e-971cb17e1a67",
   "metadata": {},
   "source": [
    "## 从文件加载提示语模板"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5463439c-8110-48c5-be21-81f0dd21a5ad",
   "metadata": {},
   "source": [
    "### yaml格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c400265a-f80a-40a7-bbc6-6f7f63ee1cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    " _type: prompt\n",
    "input_variables:\n",
    "    [\"adjective\", \"content\"]\n",
    "template: \n",
    "    Tell me a {adjective} joke about {content}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39071cc3-0207-41d5-9b55-e054a6c6aa90",
   "metadata": {},
   "source": [
    "### json格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5d51063-eec2-4fc1-9ca8-2960dd4eb1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_type': 'prompt',\n",
       " 'input_variables': ['adjective', 'content'],\n",
       " 'template': 'Tell me a {adjective} joke about {content}.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"_type\": \"prompt\",\n",
    "    \"input_variables\": [\"adjective\", \"content\"],\n",
    "    \"template\": \"Tell me a {adjective} joke about {content}.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa1ae3c-ea74-40b3-b918-89b56add4a88",
   "metadata": {},
   "source": [
    "### json + txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2145f045-8cf3-479c-9c72-f16f691c8262",
   "metadata": {},
   "source": [
    "首先，将模板主要内容写入**final_step.txt**："
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ea02ebc-f8d7-476b-8bbf-69064dea8f93",
   "metadata": {},
   "source": [
    "你的名字是{ai_name}，你是{ai_role}\n",
    "\n",
    "你的任务是:\n",
    "{task_description}\n",
    "\n",
    "经过以下的思考过程，你已经完成任务:\n",
    "{short_term_memory}\n",
    "\n",
    "现在请详细给出你的最终答案:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ebb50c-00bb-4fbd-8160-01d0a696ce9a",
   "metadata": {},
   "source": [
    "然后，在**task.json**文件中指定**template_path**嵌入路径："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f4c129ad-9560-448e-9e7f-cc0ea2e2194d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_type': 'prompt',\n",
       " 'input_variables': ['ai_name',\n",
       "  'ai_role',\n",
       "  'task_description',\n",
       "  'short_term_memory'],\n",
       " 'template_path': 'final_step.txt'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"_type\": \"prompt\",\n",
    "    \"input_variables\": [\n",
    "      \"ai_name\",\n",
    "      \"ai_role\",\n",
    "      \"task_description\",\n",
    "      \"short_term_memory\"\n",
    "    ],\n",
    "    \"template_path\": \"final_step.txt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84d54ab-a3a9-44e4-8592-dab3857904b9",
   "metadata": {},
   "source": [
    "### 加载提示语模板文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88dd5344-db2d-48d2-9070-11100de48dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about Xiao Ming.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt(\"simple_prompt.json\")\n",
    "print(prompt.format(adjective=\"funny\", content=\"Xiao Ming\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa492fa-561c-4a90-8195-2a7b4885470e",
   "metadata": {},
   "source": [
    "## OutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49da2be0-86b6-47f5-b962-638759b7dba7",
   "metadata": {},
   "source": [
    "自动把 LLM 输出的字符串按指定格式加载。\n",
    "\n",
    "LangChain 内置的 OutputParser 包括:\n",
    "\n",
    "- StrOutputParser\n",
    "- OpenAIFunctions\n",
    "- ListParser\n",
    "- DatetimeParser\n",
    "- EnumParser\n",
    "- PydanticParser\n",
    "- XMLParser\n",
    "等等"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfc056e-a0c5-42ca-b1fc-b17268359cfc",
   "metadata": {},
   "source": [
    "### JSON parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0f373e25-642c-499f-be9e-7b7da6ccd3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "48e6d5bd-9880-42db-9256-f5d3160c245d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query'] partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n```'} template='Answer the user query.\\n{format_instructions}\\n{query}\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't scientists trust atoms?\",\n",
       " 'punchline': 'Because they make up everything!'}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "print(prompt)\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e7ef17-ee7a-4279-9ba4-7d8e49bfee3a",
   "metadata": {},
   "source": [
    "### OpenAI Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cd9e41-701f-44fc-bfe1-4fc66e871149",
   "metadata": {},
   "source": [
    "从 pydantic 转换 openai 函数名和参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3dd7b502-5475-40ad-a174-88982d77c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utils.openai_functions import (\n",
    "    convert_pydantic_to_openai_function,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "openai_functions = [convert_pydantic_to_openai_function(Joke)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8325a824-7bff-454d-8810-239568775030",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You are helpful assistant\"), (\"user\", \"{input}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd09d4ae-f4d1-472f-9e7c-66a672e87323",
   "metadata": {},
   "source": [
    "**JsonOutputFunctionsParser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c11cec0b-7ac4-4b09-ad0a-c1c5d647842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "parser = JsonOutputFunctionsParser()\n",
    "\n",
    "# 绑定openai函数，并使用json解析出参数\n",
    "chain = prompt | model.bind(functions=openai_functions) | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3b4f5b68-4514-491f-8569-789604a6d376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't scientists trust atoms?\",\n",
       " 'punchline': 'Because they make up everything!'}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"tell me a joke\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8da7210-d236-487e-ab08-1b934fbe67b9",
   "metadata": {},
   "source": [
    "**JsonKeyOutputFunctionsParser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9f8f9743-8a46-4b7c-be95-149c10d54c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "\n",
    "class Jokes(BaseModel):\n",
    "    \"\"\"Jokes to tell user.\"\"\"\n",
    "\n",
    "    joke: List[Joke]\n",
    "    funniness_level: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e522cb5d-0aa2-4ea7-aaff-b330ce9596ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = JsonKeyOutputFunctionsParser(key_name=\"joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e85b4820-6ae0-4454-894f-ff528981f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_functions = [convert_pydantic_to_openai_function(Jokes)]\n",
    "chain = prompt | model.bind(functions=openai_functions) | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e5322f1a-9740-4cbb-b063-39dcaca205d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'setup': \"Why don't scientists trust atoms?\",\n",
       "  'punchline': 'Because they make up everything!'},\n",
       " {'setup': 'Why did the scarecrow win an award?',\n",
       "  'punchline': 'Because he was outstanding in his field!'}]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"tell me two jokes\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8699284c-19d8-41c1-b134-4c5ce494e458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[{}]\n",
      "[{'setup': ''}]\n",
      "[{'setup': 'Why'}]\n",
      "[{'setup': 'Why don'}]\n",
      "[{'setup': \"Why don't\"}]\n",
      "[{'setup': \"Why don't scientists\"}]\n",
      "[{'setup': \"Why don't scientists trust\"}]\n",
      "[{'setup': \"Why don't scientists trust atoms\"}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\"}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': ''}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': ''}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scare'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win an'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win an award'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win an award?'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win an award?', 'punchline': ''}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win an award?', 'punchline': 'Because'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win an award?', 'punchline': 'Because he'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win an award?', 'punchline': 'Because he was'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win an award?', 'punchline': 'Because he was outstanding'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win an award?', 'punchline': 'Because he was outstanding in'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win an award?', 'punchline': 'Because he was outstanding in his'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win an award?', 'punchline': 'Because he was outstanding in his field'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win an award?', 'punchline': 'Because he was outstanding in his field!'}]\n"
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"input\": \"tell me two jokes\"}):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6103aa96-932d-48db-bf63-04175a2430ba",
   "metadata": {},
   "source": [
    "### Enum parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "39ca00a7-5b3d-4519-8e4c-c94975122016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.enum import EnumOutputParser\n",
    "from enum import Enum\n",
    "\n",
    "class Colors(Enum):\n",
    "    RED = \"red\"\n",
    "    GREEN = \"green\"\n",
    "    BLUE = \"blue\"\n",
    "\n",
    "parser = EnumOutputParser(enum=Colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "72e4d393-03f9-45f9-a285-067b0421a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"What color eyes does this person have?\n",
    "\n",
    "> Person: {person}\n",
    "\n",
    "Instructions: {instructions}\"\"\"\n",
    ").partial(instructions=parser.get_format_instructions())\n",
    "chain = prompt | ChatOpenAI() | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "428bec9e-f589-47bb-b9ac-642ad6fe1274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['person'] partial_variables={'instructions': 'Select one of the following options: red, green, blue'} template='What color eyes does this person have?\\n\\n> Person: {person}\\n\\nInstructions: {instructions}'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Colors.BLUE: 'blue'>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(prompt)\n",
    "chain.invoke({\"person\": \"Frank Sinatra\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44805b82-ad09-488d-91f6-91f242257a35",
   "metadata": {},
   "source": [
    "### Structured output parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3faea30-1fba-4365-8d75-e9bb0fa8062a",
   "metadata": {},
   "source": [
    "### YAML parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ee189f-d0d4-4450-8023-aa500c48dc92",
   "metadata": {},
   "source": [
    "### XML parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dfce27-d347-4491-8f5d-6802600e8781",
   "metadata": {},
   "source": [
    "### Datetime parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e2864bdd-c00a-4251-a05f-f4d939e01b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "220f45a1-29e2-4847-917c-06c363569c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = DatetimeOutputParser()\n",
    "template = \"\"\"Answer the users question:\n",
    "\n",
    "{question}\n",
    "\n",
    "{format_instructions}\"\"\"\n",
    "prompt = PromptTemplate.from_template(\n",
    "    template,\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9136cb36-5f56-448f-adaf-0485d37505de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], partial_variables={'format_instructions': \"Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\\n\\nExamples: 0126-01-12T17:02:00.512595Z, 0719-05-06T14:30:04.335045Z, 1391-07-06T00:25:48.331596Z\\n\\nReturn ONLY this string, no other words!\"}, template='Answer the users question:\\n\\n{question}\\n\\n{format_instructions}')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "722f08b7-30e5-4d20-a287-d4af664e50f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-01-03 18:15:05\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | OpenAI() | output_parser\n",
    "output = chain.invoke({\"question\": \"when was bitcoin founded?\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff658f0a-0603-4c7c-a985-c10ecffab6cd",
   "metadata": {},
   "source": [
    "### Pydantic parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53de56f-f0e8-4cf1-a821-a553f38b9c0c",
   "metadata": {},
   "source": [
    "### CSV parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "54e193f1-5572-4a48-a894-6c59370d9692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List five {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "072ad855-bfde-4556-b7d3-ff8f8fb08a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vanilla',\n",
       " 'Chocolate',\n",
       " 'Strawberry',\n",
       " 'Mint Chocolate Chip',\n",
       " 'Cookies and Cream']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"subject\": \"ice cream flavors\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e84d48ab-5c1a-4456-96ea-9707e6fd97fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Vanilla']\n",
      "['Chocolate']\n",
      "['Strawberry']\n",
      "['Mint Chocolate Chip']\n",
      "['Cookies and Cream']\n"
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"subject\": \"ice cream flavors\"}):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa0bada-d84a-4cf0-9a44-a1515b616ff1",
   "metadata": {},
   "source": [
    "### Pandas DataFrame Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6caa49-d42b-4e8e-8fe1-bab6337f122a",
   "metadata": {},
   "source": [
    "### Output-fixing parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327363d2-3574-48fa-a13d-c36a96fa7a09",
   "metadata": {},
   "source": [
    "### Retry parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c48842-48bb-48e0-a851-75d0a59e7427",
   "metadata": {},
   "source": [
    "## 链式调用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9726119-5287-4dce-bc4a-466c5e4fd98e",
   "metadata": {},
   "source": [
    "可以直接使用 **LCEL** 语法构建自己的链，也可以使用现成的。\n",
    "使用前最好直接查看 **langchain** 源代码。\n",
    "- 使用 OpenAI function calling\n",
    "- 创建数据库查询\n",
    "- 检索文档\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1e4070-db9f-4761-84c5-ab49e0cebd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain import hub\n",
    "\n",
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "llm = ChatOpenAI()\n",
    "retriever = ...\n",
    "combine_docs_chain = create_stuff_documents_chain(\n",
    "    llm, retrieval_qa_chat_prompt\n",
    ")\n",
    "retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "chain.invoke({\"input\": \"...\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9ed6ae-647a-48da-a41d-806a91ae434f",
   "metadata": {},
   "source": [
    "## 记忆封装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04296862-2555-4b67-81d3-d1d5f83ead08",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bbe88a-abe1-41bd-b050-4cef367d37e1",
   "metadata": {},
   "source": [
    "## RAG的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23b14de5-1ec3-4c0e-b69c-1f61487e078e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using version \u001b[39;1m^0.1.3\u001b[39;22m for \u001b[36mlangchain\u001b[39m\n",
      "\n",
      "\u001b[34mUpdating dependencies\u001b[39m\n",
      "\u001b[2K\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(14.7s)\u001b[39;22m[34mResolving dependencies...\u001b[39m \u001b[39;2m(9.5s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(9.6s)\u001b[39;22m\n",
      "\n",
      "No dependencies to install or update\n"
     ]
    }
   ],
   "source": [
    "!poetry add \"langchain[docarray]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da4f8f-544e-4ad3-9ed8-843f3d0ab1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires:\n",
    "# pip install langchain docarray tiktoken\n",
    "\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "\n",
    "chain.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659c6fdf-0a23-49d0-8250-6ea27868dcfe",
   "metadata": {},
   "source": [
    "# 综合实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40b72ad-7db0-453a-a836-5c96482bcb19",
   "metadata": {},
   "source": [
    "## 搜索引擎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9372e13a-b0ee-469a-89d9-9be1db7093c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using version \u001b[39;1m^4.3\u001b[39;22m for \u001b[36mduckduckgo-search\u001b[39m\n",
      "\n",
      "\u001b[34mUpdating dependencies\u001b[39m\n",
      "\u001b[2K\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(20.5s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(11.7s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(14.6s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(14.9s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(15.0s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(17.0s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(17.1s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(18.3s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(20.6s)\u001b[39;22m\n",
      "\n",
      "\u001b[39;1mPackage operations\u001b[39;22m: \u001b[34m3\u001b[39m installs, \u001b[34m0\u001b[39m updates, \u001b[34m0\u001b[39m removals\n",
      "\n",
      "  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mcurl-cffi\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m0.6.0b7\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mPending...\u001b[39m\n",
      "  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdocstring-inheritance\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m2.1.2\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mPending...\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdocstring-inheritance\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m2.1.2\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m0%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdocstring-inheritance\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m2.1.2\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m100%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdocstring-inheritance\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m2.1.2\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mInstalling...\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdocstring-inheritance\u001b[39m\u001b[39m (\u001b[39m\u001b[32m2.1.2\u001b[39m\u001b[39m)\u001b[39m\n",
      "\u001b[2A\u001b[0J  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdocstring-inheritance\u001b[39m\u001b[39m (\u001b[39m\u001b[32m2.1.2\u001b[39m\u001b[39m)\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mcurl-cffi\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m0.6.0b7\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m0%\u001b[39;22m\n",
      "  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdocstring-inheritance\u001b[39m\u001b[39m (\u001b[39m\u001b[32m2.1.2\u001b[39m\u001b[39m)\u001b[39m\n",
      "\u001b[2A\u001b[0J  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdocstring-inheritance\u001b[39m\u001b[39m (\u001b[39m\u001b[32m2.1.2\u001b[39m\u001b[39m)\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mcurl-cffi\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m0.6.0b7\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m10%\u001b[39;22m\n",
      "  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdocstring-inheritance\u001b[39m\u001b[39m (\u001b[39m\u001b[32m2.1.2\u001b[39m\u001b[39m)\u001b[39m\n",
      "\u001b[2A\u001b[0J  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdocstring-inheritance\u001b[39m\u001b[39m (\u001b[39m\u001b[32m2.1.2\u001b[39m\u001b[39m)\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mcurl-cffi\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m0.6.0b7\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m20%\u001b[39;22m\n",
      "  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdocstring-inheritance\u001b[39m\u001b[39m (\u001b[39m\u001b[32m2.1.2\u001b[39m\u001b[39m)\u001b[39m\n",
      "\u001b[2A\u001b[0J  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdocstring-inheritance\u001b[39m\u001b[39m (\u001b[39m\u001b[32m2.1.2\u001b[39m\u001b[39m)\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mcurl-cffi\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m0.6.0b7\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m40%\u001b[39;22m\n",
      "  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdocstring-inheritance\u001b[39m\u001b[39m (\u001b[39m\u001b[32m2.1.2\u001b[39m\u001b[39m)\u001b[39m\n",
      "\u001b[2A\u001b[0J  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdocstring-inheritance\u001b[39m\u001b[39m (\u001b[39m\u001b[32m2.1.2\u001b[39m\u001b[39m)\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mcurl-cffi\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m0.6.0b7\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m70%\u001b[39;22m\n",
      "  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdocstring-inheritance\u001b[39m\u001b[39m (\u001b[39m\u001b[32m2.1.2\u001b[39m\u001b[39m)\u001b[39m\n",
      "\u001b[2A\u001b[0J  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdocstring-inheritance\u001b[39m\u001b[39m (\u001b[39m\u001b[32m2.1.2\u001b[39m\u001b[39m)\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mcurl-cffi\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m0.6.0b7\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m100%\u001b[39;22m\n",
      "  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdocstring-inheritance\u001b[39m\u001b[39m (\u001b[39m\u001b[32m2.1.2\u001b[39m\u001b[39m)\u001b[39m\n",
      "\u001b[2A\u001b[0J  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdocstring-inheritance\u001b[39m\u001b[39m (\u001b[39m\u001b[32m2.1.2\u001b[39m\u001b[39m)\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mcurl-cffi\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m0.6.0b7\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mInstalling...\u001b[39m\n",
      "  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdocstring-inheritance\u001b[39m\u001b[39m (\u001b[39m\u001b[32m2.1.2\u001b[39m\u001b[39m)\u001b[39m\n",
      "\u001b[2A\u001b[0J  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdocstring-inheritance\u001b[39m\u001b[39m (\u001b[39m\u001b[32m2.1.2\u001b[39m\u001b[39m)\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mcurl-cffi\u001b[39m\u001b[39m (\u001b[39m\u001b[32m0.6.0b7\u001b[39m\u001b[39m)\u001b[39m\n",
      "  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdocstring-inheritance\u001b[39m\u001b[39m (\u001b[39m\u001b[32m2.1.2\u001b[39m\u001b[39m)\u001b[39m\n",
      "  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mduckduckgo-search\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m4.3\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mPending...\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mduckduckgo-search\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m4.3\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m0%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mduckduckgo-search\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m4.3\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m100%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mduckduckgo-search\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m4.3\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mInstalling...\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mduckduckgo-search\u001b[39m\u001b[39m (\u001b[39m\u001b[32m4.3\u001b[39m\u001b[39m)\u001b[39m\n",
      "\n",
      "\u001b[34mWriting lock file\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "!poetry add duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "228c53f2-f94b-433c-989f-7287f9cba74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a351ea5c-dcfb-427d-a280-56f0316c3351",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = DuckDuckGoSearchRun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d113c45a-b675-4a8e-9143-7676cd182fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.run(\"Obama's first name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be72852e-30f3-4987-a84f-baeb620334ec",
   "metadata": {},
   "source": [
    "## 通过文本向量路由Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4002ea3d-a7e9-4492-a5de-ebb4146b65c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "physics_template = \"\"\"你是一个物理学教授，负责给数学爱好者解答疑惑。\\\n",
    "你正在为小学生回答问题，注意使用小学生水平能听懂的词汇，避免过于专业晦涩的术语。 \\\n",
    "当你不知道答案时就回答不知道。\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"你是一个数学家，负责给小学生解答疑惑。\n",
    "注意使用小学生水平能听懂的词汇，避免过于专业晦涩的术语。 \\\n",
    "回答时，请举一些生活中的例子。\n",
    "当你不知道答案时就回答不知道。\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "\n",
    "def prompt_router(input):\n",
    "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "common_train = ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "43793a9c-bf60-42ba-9c86-e7cfeff80215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "黑洞是一种极度密集的天体，它具有非常强大的引力场，以至于连光都无法逃离它的吸引。黑洞的形成是由于一个恒星在死亡时，其质量过大，无法通过核聚变维持稳定，导致恒星坍缩成一个极为紧凑的物体。黑洞的中心部分称为奇点，奇点的密度和引力非常之大，超过了任何已知物质的极限。\n",
      "\n",
      "黑洞的存在可以通过它们产生的引力效应来间接观测，例如吸收附近的物质、扭曲周围空间和发射强烈的辐射。虽然我们无法直接观测到黑洞，但科学家们通过观测它们对周围物体的影响，以及通过天文观测和数学模型来研究黑洞的性质和行为。\n",
      "\n",
      "黑洞在宇宙中广泛存在，它们可能是恒星坍缩形成的中等质量黑洞，也可能是超大质量黑洞，如位于银河系中心的超大质量黑洞。黑洞对宇宙的演化和结构具有重要影响，它们是天体物理学和相对论研究的重要对象。\n"
     ]
    }
   ],
   "source": [
    "print(common_train.invoke(\"黑洞是什么？\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a463dc35-f5da-40b7-ae75-52203d1e2a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PHYSICS\n",
      "小朋友，黑洞是宇宙中一种非常特殊的东西。它是一种非常强大的引力场，就像一个很大的吸力。当一颗非常大的恒星（就是我们看到的星星）燃烧完燃料后，它会塌缩成一个非常小又非常密集的东西，就是黑洞。黑洞的引力非常强大，甚至连光也无法逃脱它的吸引力。所以我们看不到黑洞，它是非常神秘的。关于黑洞，科学家们还在研究中，有很多有趣的发现等待我们去探索。\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"黑洞是什么？\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "69d9155f-e7ab-4fc3-b83a-54cd5f79fe69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "路径积分是一个物理学概念，用来描述在一个力场中沿着一条曲线路径上的力的积累效果。简单来说，路径积分是将一个向量场沿着一条曲线进行积分，得到沿着该曲线的总体积效应。\n",
      "\n",
      "在物理学中，路径积分可以用来计算沿着一个曲线路径上的力的总效果，比如沿着一条曲线上的力的总功或者总位移。路径积分的计算方式是将力场在曲线上的每个点上的力与微小位移相乘，然后将所有微小的力与位移的乘积相加，得到曲线上的总效果。\n",
      "\n",
      "路径积分在许多领域中都有重要的应用，比如在力学中用于计算物体在曲线路径上的总功、在电磁学中用于计算电场或磁场沿着曲线的总位移等。路径积分的计算可以通过数学上的积分运算来实现，根据具体情况可以采用不同的积分方法，比如定积分或线积分等。\n"
     ]
    }
   ],
   "source": [
    "print(common_train.invoke(\"路径积分是什么？\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "508efa40-58d2-4a4d-9e07-87f5dcaabe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MATH\n",
      "路径积分是一种数学工具，它在物理学中常常被用来描述粒子或光在空间中的运动。你可以把路径积分想象成一个粒子或光在不同路径上行走的概率，就像我们在城市里选择不同的路线去目的地一样。\n",
      "\n",
      "想象一下你要从学校回家，有很多条路可以选择。每条路都有不同的长度、不同的交通状况和不同的风景。路径积分就是用来计算你选择每条路的概率，也就是说，你走每条路的可能性有多大。\n",
      "\n",
      "在物理学中，粒子或光在空间中运动的时候，也有很多可能的路径可以选择。路径积分可以帮助我们计算出每条路径的概率，从而更好地理解粒子或光的行为。\n",
      "\n",
      "但是，具体如何计算路径积分，需要更深入的数学知识和物理背景。这里只是简单介绍了路径积分的概念，如果你对它感兴趣，可以在以后的学习中深入了解。\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"路径积分是什么？\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51042965-b313-4b28-8b51-d4df928e9cf7",
   "metadata": {},
   "source": [
    "## 执行python代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e264dc04-4314-4736-a76c-fb0bc578e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ec5882ad-5528-41d9-a927-1c634c321e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Write some python code to solve the user's problem. \n",
    "\n",
    "Return only python code in Markdown format and Chinese, e.g.:\n",
    "\n",
    "```python\n",
    "....\n",
    "```\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", template), (\"human\", \"{input}\")])\n",
    "\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c8904de2-a9ad-428a-8bf6-0eaa9dd3ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sanitize_output(text: str):\n",
    "    print(text)\n",
    "    _, after = text.split(\"```python\")\n",
    "    return after.split(\"```\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "81461a08-c2d9-45f2-937c-c6634f570346",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | StrOutputParser() | _sanitize_output | PythonREPL().run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c4e3c00f-dcf9-4a80-95ea-0c0410267017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我们可以使用穷举法来解决这个问题。\n",
      "\n",
      "假设有 x 只兔子，y 只鸡。根据题意，可以得到以下两个方程：\n",
      "\n",
      "x + y = 5   # 头的数量\n",
      "4x + 2y = 12  # 脚的数量\n",
      "\n",
      "我们可以通过求解这个方程组来得到兔子和鸡的数量。让我们来编写代码实现这个算法。\n",
      "\n",
      "```python\n",
      "def solve():\n",
      "    for x in range(6):  # 兔子的数量最多为5只\n",
      "        y = 5 - x  # 根据第一个方程计算鸡的数量\n",
      "        if 4*x + 2*y == 12:  # 检查第二个方程是否满足\n",
      "            return x, y  # 返回兔子和鸡的数量\n",
      "\n",
      "rabbit, chicken = solve()\n",
      "print(f\"兔子的数量为：{rabbit} 只，鸡的数量为：{chicken} 只\")\n",
      "```\n",
      "\n",
      "运行这段代码，我们可以得到输出：\n",
      "\n",
      "```\n",
      "兔子的数量为：1 只，鸡的数量为：4 只\n",
      "```\n",
      "\n",
      "所以，笼子里有1只兔子和4只鸡。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'兔子的数量为：1 只，鸡的数量为：4 只\\n'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"一个笼子里有兔子和鸡若干，数一数有5个头，12只脚，请问有多少只兔子多少只鸡？\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957fe527-2db2-4f20-a5d1-778e11196025",
   "metadata": {},
   "source": [
    "## 查询数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7be6d4c3-3ebc-4c09-a206-35f536432380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Based on the table schema below, write a SQL query that would answer the user's question:\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "SQL Query:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4b05c675-6291-4759-9f5a-b61b531d2a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SQLDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4d95f54f-1591-447d-aedc-11b5e2363c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SQLDatabase.from_uri(\"sqlite:///./Chinook.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f2f5b9fa-c6d3-461d-b429-4fed0c95fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema(_):\n",
    "    return db.get_table_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fc4e86c3-6ac7-4b3d-9c0d-f2de665400f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query):\n",
    "    return db.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "52e9079c-8246-4077-848e-1e13ff0bafcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "sql_response = (\n",
    "    RunnablePassthrough.assign(schema=get_schema)\n",
    "    | prompt\n",
    "    | model.bind(stop=[\"\\nSQLResult:\"])\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7643bf79-908f-468d-823f-e9297b7ce940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT COUNT(*) FROM Employee'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 直接生成查询语句\n",
    "sql_response.invoke({\"question\": \"How many employees are there?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6a86b556-8fb6-487b-a98a-2adbbb697e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Based on the table schema below, question, sql query, and sql response, write a natural language response:\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Response: {response}\n",
    "\n",
    "请用中文回答。\n",
    "\"\"\"\n",
    "prompt_response = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ad471862-98d0-4b65-9a0e-9808abe854e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意要分两阶段执行assign：先生成SQL，才能执行SQL\n",
    "full_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        schema=get_schema,\n",
    "        query=sql_response\n",
    "    ).assign(\n",
    "        response=lambda x: db.run(x[\"query\"]),\n",
    "    )\n",
    "    | prompt_response\n",
    "    | model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "04a292fd-00f4-46c3-9ffb-c547abb5ebc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='员工人数是8人。')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"员工人数是多少?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a335e1-adfa-4ebb-aeb5-fe7f2f647cd2",
   "metadata": {},
   "source": [
    "# 集成langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c184133-600c-476a-b19f-022a14716781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langfuse.callback import CallbackHandler\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a568af10-0d5b-467d-bd2b-bd50fe253da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = CallbackHandler(trace_name=\"learning-langchain\", user_id=\"homeway\", session_id=str(uuid.uuid4()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "356fd868-cca0-4c79-82f7-a70ad50622d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model = \"gpt-3.5-turbo\", streaming = False, temperature = 0.5)\n",
    "parser = StrOutputParser()\n",
    "prompt = ChatPromptTemplate.from_template(\"hi\")\n",
    "train = (prompt | llm | parser)\n",
    "train.invoke({}, config = {\"callbacks\": [handler]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bad0cba7-8f2b-4bae-995f-85957fe9666a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='你是薛宏伟。')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 多轮对话\n",
    "from langchain.schema import (\n",
    "    AIMessage, #等价于OpenAI接口中的 assistant role\n",
    "    HumanMessage, #等价于OpenAI接口中的 user role\n",
    "    SystemMessage #等价于OpenAI接口中的 system role\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"你是AGIClass的课程助理。\"), \n",
    "    HumanMessage(content=\"我是学员，我叫薛宏伟。\"), \n",
    "    AIMessage(content=\"欢迎！\"),\n",
    "    HumanMessage(content=\"我是谁\") \n",
    "]\n",
    "llm.invoke(messages) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be80214d-8bf3-4cee-9170-4eb27b3048b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='我是广州鸿蒙的客服助手，名字叫蒙蒙。有什么可以帮到您的吗？')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对话提示语模板\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"你是{product}的客服助手。你的名字叫{name}\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = template.format_messages(\n",
    "        product=\"广州鸿蒙\",\n",
    "        name=\"蒙蒙\",\n",
    "        query=\"你是谁\"\n",
    "    )\n",
    "\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87250f5b-935e-4c81-bd93-424bf1f6d9b3",
   "metadata": {},
   "source": [
    "# 集成langserve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42b11bd-6f91-4aec-a3a1-f4b05b6da3d0",
   "metadata": {},
   "source": [
    "## 与fastapi一起使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c1e0ae-2ffa-4431-856d-edcda649f3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langserve import add_routes\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Server\",\n",
    "    version=\"1.0\",\n",
    "    description=\"A simple api server using Langchain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    ChatOpenAI(),\n",
    "    path=\"/openai\",\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "add_routes(\n",
    "    app,\n",
    "    prompt | model,\n",
    "    path=\"/joke\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a45e06a-8fa5-4895-82eb-c978de257286",
   "metadata": {},
   "source": [
    "## 与langfuse一起使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbe4dea-20fb-4ad2-801c-c1c1a691e5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = CallbackHandler(trace_name=\"chat_once\", user_id=\"wencheng\")\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"{question}\"\"\")\n",
    "llm = ChatOpenAI(model = \"gpt-3.5-turbo-16k\", streaming = True, temperature = 0)\n",
    "chain = (prompt | llm | parser).with_config({\"callbacks\": [handler]})\n",
    "\n",
    "add_routes(app, chain, path = \"/langserve/chat_once\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0224be3-bd65-403b-853a-341951f3b368",
   "metadata": {},
   "source": [
    "## python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a15c284d-cc7f-4846-a16f-7379e6c2dca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using version \u001b[39;1m^0.4.0\u001b[39;22m for \u001b[36mhttpx-sse\u001b[39m\n",
      "\n",
      "\u001b[34mUpdating dependencies\u001b[39m\n",
      "\u001b[2K\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(16.3s)\u001b[39;22m[34mResolving dependencies...\u001b[39m \u001b[39;2m(0.1s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(7.9s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(9.3s)\u001b[39;22m\n",
      "\n",
      "\u001b[39;1mPackage operations\u001b[39;22m: \u001b[34m1\u001b[39m install, \u001b[34m0\u001b[39m updates, \u001b[34m0\u001b[39m removals\n",
      "\n",
      "  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mhttpx-sse\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m0.4.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mPending...\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mhttpx-sse\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m0.4.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mInstalling...\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mhttpx-sse\u001b[39m\u001b[39m (\u001b[39m\u001b[32m0.4.0\u001b[39m\u001b[39m)\u001b[39m\n",
      "\n",
      "\u001b[34mWriting lock file\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "!poetry add httpx_sse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "29f99c08-0bf4-404a-8e1f-69cffe04d253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following packages are already present in the pyproject.toml and will be skipped:\n",
      "\n",
      "  • \u001b[36mlangserve\u001b[39m\n",
      "\n",
      "If you want to update it to the latest compatible version, you can use `poetry update package`.\n",
      "If you prefer to upgrade it to the latest available version, you can use `poetry add package@latest`.\n",
      "\n",
      "Nothing to add.\n"
     ]
    }
   ],
   "source": [
    "!poetry add langserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6c12ef37-c3a8-46e3-b360-ef729fb83a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "东莞是中国广东省下辖的一个地级市，位于珠江三角洲南部，东临深圳，西接广州，北邻惠州，南濒珠江口。作为中国改革开放的重要窗口和制造业基地，东莞是中国最重要的制造业城市之一。\n",
      "\n",
      "东莞是中国最早的经济特区之一，也是中国最大的制造业城市之一。它以制造业为主导，涵盖了电子、电器、纺织、玩具、家具、鞋业等多个行业。许多国内外知名品牌都在东莞设有生产基地。东莞的制造业发展水平和产业链完善程度在全国具有较高的竞争力。\n",
      "\n",
      "除了制造业，东莞也在不断发展其他产业，如现代服务业、高新技术产业和文化创意产业等。近年来，东莞还加大了对科技创新的投入，积极推动产业升级和转型发展。\n",
      "\n",
      "东莞也是一个宜居的城市，拥有良好的基础设施和公共服务。城市规划合理，交通便利，医疗、教育、文化等公共服务设施完善。同时，东莞还注重生态环境保护，积极推动绿色发展，建设了许多公园和绿地，提供了良好的生活环境。\n",
      "\n",
      "此外，东莞还有一些旅游景点值得一提。如虎门石龙山、广东现代国际展览中心、东莞松山湖科技产业园等。这些景点展示了东莞的自然风光和城市发展成果。\n",
      "\n",
      "总的来说，东莞是一个以制造业为主导的现代化城市，拥有发达的经济和良好的生活环境。无论是商务出差还是旅游观光，东莞都是一个值得一去的地方。"
     ]
    }
   ],
   "source": [
    "from langserve import RemoteRunnable\n",
    "chat_once = RemoteRunnable(\"http://localhost:8000/langserve/chat_once\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "32250360-72e1-4e68-834e-81af1df3b777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'东莞是中国广东省下辖的一个地级市，位于珠江三角洲南部，东临深圳，西接广州，北邻惠州，南濒珠江口。作为中国改革开放的重要窗口和制造业基地，东莞是中国最重要的制造业城市之一。\\n\\n东莞是中国最早的经济特区之一，也是中国最大的制造业城市之一。它以制造业为主导，涵盖了电子、电器、纺织、玩具、家具、鞋业等多个行业。许多国内外知名品牌都在东莞设有生产基地。东莞的制造业发展水平和产业链完善程度在全国具有较高的竞争力。\\n\\n除了制造业，东莞也在不断发展其他产业，如现代服务业、高新技术产业和文化创意产业等。近年来，东莞还加大了对科技创新的投入，积极推动产业升级和转型发展。\\n\\n东莞也是一个宜居的城市，拥有良好的基础设施和公共服务。城市规划合理，交通便利，医疗、教育、文化等公共服务设施完善。同时，东莞还注重生态环境保护，积极推动绿色发展，建设了许多公园和绿地，提供了良好的生活环境。\\n\\n此外，东莞还有一些旅游景点值得一提。如虎门石龙山、广东现代国际展览中心、东莞松山湖科技产业园等。这些景点展示了东莞的自然风光和城市发展成果。\\n\\n总的来说，东莞是一个以制造业为主导的现代化城市，拥有发达的经济和良好的生活环境。无论是商务出差还是旅游观光，东莞都是一个值得一去的地方。'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_once.invoke({\"question\": \"能帮我介绍一下东莞吗？\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96fd75d-7f1a-4da4-9cc8-8e9d2dd9f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chat_once.stream({\"question\": \"能帮我介绍一下东莞吗？\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829cceea-6f76-42e7-b58e-d68488612d50",
   "metadata": {},
   "source": [
    "# javascript client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f15d26e-4bf9-4fc5-829c-8a1e166e1751",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yarn add langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d3595c-89c0-4328-adb7-5205c47fdcf9",
   "metadata": {},
   "source": [
    "## 调用invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80373122-316f-456d-9fe6-beb214a7349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { RemoteRunnable } from \"langchain/runnables/remote\";\n",
    "\n",
    "const remoteChain = new RemoteRunnable({\n",
    "  url: \"https://your_hostname.com/path\",\n",
    "});\n",
    "\n",
    "const result = await remoteChain.invoke({\n",
    "  param1: \"param1\",\n",
    "  param2: \"param2\",\n",
    "});\n",
    "\n",
    "console.log(result);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaef472-51be-4643-a8ff-0ba0ad0ccab8",
   "metadata": {},
   "source": [
    "## 调用stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d905e70-dcbb-43cf-a423-471115110a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "const stream = await remoteChain.stream({\n",
    "  param1: \"param1\",\n",
    "  param2: \"param2\",\n",
    "});\n",
    "\n",
    "for await (const chunk of stream) {\n",
    "  console.log(chunk);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9405743c-85b7-4ba6-a78d-6b2f2f38a884",
   "metadata": {},
   "source": [
    "## 使用config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13999ec1-95e3-4593-93b3-d3625891ebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { RemoteRunnable } from \"langchain/runnables/remote\";\n",
    "\n",
    "const remoteChain = new RemoteRunnable({\n",
    "  url: \"https://your_hostname.com/path\",\n",
    "  options: {\n",
    "    timeout: 10000,\n",
    "    headers: {\n",
    "      Authorization: \"Bearer YOUR_TOKEN\",\n",
    "    },\n",
    "  },\n",
    "});\n",
    "\n",
    "const result = await remoteChain.invoke({\n",
    "  param1: \"param1\",\n",
    "  param2: \"param2\",\n",
    "});\n",
    "\n",
    "console.log(result);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
