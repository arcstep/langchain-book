{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55faa189-72e3-49aa-a439-1c64b3feaaeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载 .env 到环境变量\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c0869b5-e737-4103-9a04-c27b94841ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='你是一个有用的翻译助手，从英文语言翻译到中文语言.'),\n",
       " HumanMessage(content='I love programming.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "template = \"你是一个有用的翻译助手，从{input_language}语言翻译到{output_language}语言.\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "chat_prompt.format_messages(input_language=\"英文\", output_language=\"中文\", text=\"I love programming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ed00a5-b48d-43d6-8c09-ce4203a0fa39",
   "metadata": {},
   "source": [
    "# langchain主要生态"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c6820e-d39c-41da-a2ec-cd9518200a6c",
   "metadata": {},
   "source": [
    "- langchain 模块：以LCEL方式开发DAG结构的chain，也可以作为访问远程chain的python客户端\n",
    "- langgraph 模块：让LCEL支持循环结构的chain\n",
    "- langserve 模块：将chain转化为API\n",
    "- langchain-js 模块：提供langchain的`js`版本，也可以作为访问远程chain的JS客户端\n",
    "- langsmith 模块：跟踪和调试的平台\n",
    "- langfuse 模块：langsmith的开源平替"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c52f1-580e-4efe-8151-fa6889e90896",
   "metadata": {},
   "source": [
    "# langchain模块的代码结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff186ec5-fa1e-42f4-9e40-df5fd980002b",
   "metadata": {},
   "source": [
    "- langchain：入口主模块（很多模块从 langchain.core 和 langchain.community 中引入）\n",
    "- langchain.core：核心模块\n",
    "- langchain.community：社区贡献\n",
    "- langchain.partners：合作伙伴模块\n",
    "- langchain.experimetal：实验性模块\n",
    "- langchain.cli：命令行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228d94f8-5e5f-4579-a772-d40698223dba",
   "metadata": {},
   "source": [
    "# 快速开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce689d0-4b7a-44ca-bf21-5a81bc3b5f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 langchain\n",
    "!pip install -U langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1bf070-e5ca-4630-87e9-c20088c500c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 必须独立安装 openai\n",
    "!pip install -U langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c3f5ac-a790-47a6-ac3b-10c2658c94fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果要使用国内大模型：智谱AI、通义千问等\n",
    "!pip install -U langchain_chinese"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4f7f0b-5dfe-4768-85d3-bfe4231d2f33",
   "metadata": {},
   "source": [
    "## 定义大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "300f3790-56f9-4cbf-9840-2255a78a253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "08679cf1-e498-4e03-ade2-2ce8cef6bc58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. 彩虹铅笔有限公司\\n2. 色彩创意铅笔厂\\n3. 五彩斑斓铅笔制造厂\\n4. 彩色笔芯制造有限公司\\n5. 鲜明色彩铅笔有限公司\\n6. 彩绘创意铅笔公司\\n7. 彩色创意铅笔厂\\n8. 色彩之笔有限公司\\n9. 彩虹色铅笔制造有限公司\\n10. 彩绘艺术铅笔有限公司')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"请帮我想一想，生产彩色铅笔的公司有什么好名字?\"\n",
    "llm.invoke(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9860db1-b98a-4b6f-93c9-b432d5321902",
   "metadata": {},
   "source": [
    "## 定义提示语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b6a5e725-15b2-411e-bc42-b2563f698087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'请帮我想一想，生产彩色铅笔的公司有什么好名字?'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"请帮我想一想，生产{product}的公司有什么好名字?\")\n",
    "prompt.format(product=\"彩色铅笔\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff33fb98-a31a-410e-9828-fd93f240c992",
   "metadata": {},
   "source": [
    "## 使用 langchain 的 LCEL 语法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0015d493-eeaa-4f93-9cdf-efbbb2747502",
   "metadata": {},
   "source": [
    "实际上，你可以将组装好的提示语当作**具有某种特定功能的函数**，随时配合大语言模型调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cb0653dc-33ca-4e2e-9f04-48a2d0ebb97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. 快乐玩偶有限公司\\n2. 梦幻玩偶工房\\n3. 童趣玩偶制造厂\\n4. 爱心玩偶工坊\\n5. 欢乐玩偶创意集团\\n6. 甜蜜玩偶工厂\\n7. 小梦玩偶有限公司\\n8. 温馨玩偶坊\\n9. 萌萌玩偶制作中心\\n10. 幸福玩偶工艺厂')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "chain.invoke({\"product\": \"玩偶\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08538769-1c49-432c-9fd0-6695a00b2d2b",
   "metadata": {},
   "source": [
    "## 提取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9402c1a1-9f8a-41c6-b912-a53b28bbd27a",
   "metadata": {},
   "source": [
    "### 直接提取文本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04ee16f-bd06-4403-b0c5-b20f400d371d",
   "metadata": {},
   "source": [
    "你也可以使用解析器直接提取文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "87732626-3c81-4430-994f-6c11203d6506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. 快乐玩偶有限公司\\n2. 梦幻玩偶工房\\n3. 笑脸玩偶制造厂\\n4. 童趣玩偶集团\\n5. 甜蜜玩偶工坊\\n6. 幸福玩偶制作所\\n7. 萌萌玩偶工场\\n8. 心愿玩偶公司\\n9. 童心玩偶工坊\\n10. 惊喜玩偶制造有限公司'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 注意 StrOutputParser 属于基础解析器，所以与 CommaSeparatedListOutputParser 位置不同\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "chain.invoke({\"product\": \"玩偶\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6daf7f8-bfeb-47b5-adab-be5dda60710b",
   "metadata": {},
   "source": [
    "### 提取为一个列表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45400ed3-af49-40cb-872f-c216c90475c1",
   "metadata": {},
   "source": [
    "如果我们希望将上面的大模型调用在应用中当作函数使用，就会希望返回结果是格式化的，例如列表。\n",
    "\n",
    "要想将上述结果提取为一个列表，需要做两件事情：\n",
    "\n",
    "- 首先是大模型生成的文字要稳定，不能随着大模型的心思来\n",
    "- 其次是根据生成的文字结构解析，例如使用正则表达式\n",
    "\n",
    "langchain 中为此预设了很多输出解析器，即 **xxxOutputParser** 类。<br>\n",
    "接下来，我们使用 CommaSeparatedListOutputParser 来解决上述问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bb9bf1a8-f648-451b-af96-7971d03291e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "49fba3ca-c270-47b8-9e90-888ddcd22249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'bye']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这个解析器的基本能力就是按逗号分解字符串，生成一个列表\n",
    "output_parser.parse(\"hi, bye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "48b990cf-4b74-43ab-b770-5abd9ddd6c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    请帮我想一想，生产彩色铅笔的公司有什么好的中文名字?.\n",
      "    生成时请按逗号间隔\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "    请帮我想一想，生产{product}的公司有什么好的中文名字?.\n",
    "    {format_instructions}\n",
    "    \"\"\")\n",
    "print(prompt.format(product=\"彩色铅笔\", format_instructions=\"生成时请按逗号间隔\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c44e54-5203-4e7f-a0d5-9fafbfe6a019",
   "metadata": {},
   "source": [
    "使用这个输出解析器时，通常你要告诉大模型按照期待的逗号间隔的方式生成文本。<br>\n",
    "因此，你可以使用`get_format_instructions`生成的格式提示信息插入到提示语中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3b2255d0-5035-4c26-a5ab-54bf775f1fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your response should be a list of comma separated values, eg: `foo, bar, baz`'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "76c82f98-dbb1-4ff4-8bfe-97130ff2d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 组装提示模板\n",
    "prompt_with_parser = prompt.partial(format_instructions=output_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "04947246-5d59-4cc9-93f1-3e17781cb2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['product'], partial_variables={'format_instructions': 'Your response should be a list of comma separated values, eg: `foo, bar, baz`'}, template='\\n    请帮我想一想，生产{product}的公司有什么好的中文名字?.\\n    {format_instructions}\\n    ')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_with_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4fc87383-8a20-4295-9c17-b2da48b615f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['童趣制造',\n",
       " '玩偶乐园',\n",
       " '快乐玩偶',\n",
       " '玩偶之家',\n",
       " '玩具时光',\n",
       " '童心玩具',\n",
       " '乐趣工坊',\n",
       " '玩偶天地',\n",
       " '快乐玩具厂',\n",
       " '童趣创意']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt_with_parser | llm | output_parser\n",
    "chain.invoke({\"product\": \"玩偶\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfb3d7d-bfb4-4414-8c1e-acf29ce31be2",
   "metadata": {},
   "source": [
    "# 主要概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d4149-1f64-4fbc-b855-463168df5cdb",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cd6cee-0301-4b02-88c1-adce1224d6af",
   "metadata": {},
   "source": [
    "请记住：OpenAI模型对提示语中包含JSON的情况非常友好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23847910-53b3-4a07-9a56-055727270021",
   "metadata": {},
   "source": [
    "## 消息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aef250a-ad08-43da-9131-c6494f51608c",
   "metadata": {},
   "source": [
    "- HumanMessage： 一般是纯文字内容\n",
    "- AIMessage： 可能包含additional_kwargs，例如 funciton calling 提示\n",
    "- SystemMessage：部份模型支持的内容提示\n",
    "- FunctionMessage：函数调用的名称和参数\n",
    "- ToolMessage：工具调用结果（与FunctionMessage不同）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a6bd18-586d-4677-bc90-da4b964cc69d",
   "metadata": {},
   "source": [
    "## 提示语"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd7cc5-042a-4ef1-b856-b5fcf57b6fbb",
   "metadata": {},
   "source": [
    "- PromptValue\n",
    "- PromptTemplate\n",
    "- MessagePromptTemplate\n",
    "- MessagesPlaceholder\n",
    "- ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3b1cfa-af5b-4f73-8893-e0d1f77edd97",
   "metadata": {},
   "source": [
    "## Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb550b86-b53a-4bf4-83c6-9fb71b1a5279",
   "metadata": {},
   "source": [
    "- StrOutputParser：仅输出字符串；如果输出是 ChatModel，它会仅输出Message的content属性\n",
    "- OpenAI Functions Parsers：处理OpenAI函数调用所需的函数名和参数\n",
    "- Agent Output Parsers：帮助智能体解析执行计划"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff5372c-e711-4250-915e-87f667789a70",
   "metadata": {},
   "source": [
    "# Prompt封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f290ab2-7794-43f7-9202-16167fd1f1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['subject'] template='给我讲个关于{subject}的笑话'\n",
      "给我讲个关于小明的笑话\n"
     ]
    }
   ],
   "source": [
    "# 简单的例子\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"给我讲个关于{subject}的笑话\")\n",
    "print(template)\n",
    "print(template.format(subject='小明'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367ce3f3-89a8-49d5-9abe-55f301f7576e",
   "metadata": {},
   "source": [
    "## 字符串模板提示语"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eff1ae2-ef96-4a9c-a83c-c454be9f4419",
   "metadata": {},
   "source": [
    "### 基本语法：PromptTemplate.from_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5732d9f1-0f47-424f-a2bc-8a9d731e9dc3",
   "metadata": {},
   "source": [
    "使用字符串提示时，每个模板都会连接在一起。<br>\n",
    "您可以直接使用prompt模板或字符串（但列表中的第一个元素必须是prompt模板类型）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "683eda3d-fa95-4906-99e8-6d72a3d93ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['language', 'topic'], template='告诉我一个关于{topic}的笑话, 一定要特别好笑\\n使用{language}表述')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 提示语模板与字符串可以直接相加，简化模板构造\n",
    "from langchain.prompts import PromptTemplate\n",
    "prompt = (\n",
    "    PromptTemplate.from_template(\"告诉我一个关于{topic}的笑话\")\n",
    "    + \", 一定要特别好笑\"\n",
    "    + \"\\n使用{language}表述\"\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4ba2bf6c-f77b-4d4b-9dd7-732a5e55ab1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'告诉我一个关于足球的笑话, 一定要特别好笑\\n使用中文表述'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(topic=\"足球\", language=\"中文\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd126b4-560f-4e32-8a50-fb2b8fc59f4e",
   "metadata": {},
   "source": [
    "### 完整语法：PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa4333a-c137-4b26-a632-6dca25ecdaf0",
   "metadata": {},
   "source": [
    "**Prompt模板可以使用 f-string 和 jinja2 两种语法，通过 template_format 指定。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf121913-fb4e-46a7-afe0-c3c36e2798be",
   "metadata": {},
   "source": [
    "#### f-string 模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5903d64f-d71b-416b-a67e-9160621af4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'告诉我一个关于足球的笑话, 一定要特别好笑\\n使用中文表述'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这是一个完整的函数调用，达到同样的效果\n",
    "from langchain.prompts import PromptTemplate\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['language', 'topic'],\n",
    "    output_parser=None,\n",
    "    partial_variables={},\n",
    "    template='告诉我一个关于{topic}的笑话, 一定要特别好笑\\n使用{language}表述',\n",
    "    template_format='f-string',\n",
    "    validate_template=True\n",
    ")\n",
    "prompt.format(topic=\"足球\", language=\"中文\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf9d4ba-260f-4982-88fe-2f6c84faf4cb",
   "metadata": {},
   "source": [
    "#### jinja2 模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "db4e3fa7-4e4e-4571-ba4f-cead570bc4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_jinja2 = PromptTemplate(\n",
    "    input_variables=['topic'],\n",
    "    output_parser=None,\n",
    "    partial_variables={},\n",
    "    template=\"\"\"\n",
    "告诉我一个关于{{topic}}的笑话, 一定要特别好笑\n",
    "{% if topic == '足球' %}\n",
    "使用中文表述\n",
    "{% else %}\n",
    "使用英文表述\n",
    "{% endif %}\n",
    "    \"\"\",\n",
    "    template_format='jinja2',\n",
    "    validate_template=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "67e1fae2-3444-4d63-8842-a0c31f50628c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "告诉我一个关于足球的笑话, 一定要特别好笑\n",
      "\n",
      "使用中文表述\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(prompt_jinja2.format(topic=\"足球\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "760dfc14-a2de-4def-8bd7-864af22b21d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "告诉我一个关于程序员的笑话, 一定要特别好笑\n",
      "\n",
      "使用英文表述\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(prompt_jinja2.format(topic=\"程序员\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6a9fb7-cd4f-48a0-8470-4396b2eee288",
   "metadata": {},
   "source": [
    "### 在 LCEL 中使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0e964c6d-56f9-442d-ac57-2bc34d0e1ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'为什么足球场上的草总是那么绿？因为它们总是被球员踢得\"开心\"啊！哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 也可以直接在chain中给参数\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "chain = prompt|llm|StrOutputParser()\n",
    "\n",
    "chain.invoke({\"topic\": \"足球\", \"language\": \"中文\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c421968-1d5b-4bd9-b922-895ff4499dd5",
   "metadata": {},
   "source": [
    "## 对话模板提示语"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c52d8-061a-4d98-9924-95c0531d3c7e",
   "metadata": {},
   "source": [
    "### 构造方法1：直接将 xxxMessage 相加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5307906a-701b-4b5d-a6e0-83e9fb55f0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='你是一个翻译机器人，我说中文你就直接翻译成英文，不要啰嗦。'),\n",
       " HumanMessage(content='你好'),\n",
       " AIMessage(content='hello'),\n",
       " HumanMessage(content='你知道孙悟空吗？')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "prompt = (\n",
    "    SystemMessage(content=\"你是一个翻译机器人，我说中文你就直接翻译成英文，不要啰嗦。\") \n",
    "    + HumanMessage(content=\"你好\") \n",
    "    + AIMessage(content=\"hello\") + \n",
    "    \"{input}\"\n",
    ")\n",
    "prompt.format_messages(input=\"你知道孙悟空吗？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbee1fc-26f3-4ffe-b02b-4b1255d39f54",
   "metadata": {},
   "source": [
    "### 构造方法2：使用 xxxMessage 列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba0a8561-1f20-4e79-a6bf-be976f219fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='你是一个翻译机器人，我说中文你就直接翻译成英文，不要啰嗦。'),\n",
       " HumanMessage(content='你好'),\n",
       " AIMessage(content='hello'),\n",
       " HumanMessage(content='你知道孙悟空吗？')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"你是一个翻译机器人，我说中文你就直接翻译成英文，不要啰嗦。\"),\n",
    "    HumanMessage(content=\"你好\"),\n",
    "    AIMessage(content=\"hello\"),\n",
    "    \"{input}\"\n",
    "])\n",
    "prompt.format_messages(input=\"你知道孙悟空吗？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588800e7-7ae4-4554-ae85-53d8fc05c66b",
   "metadata": {},
   "source": [
    "### 构造方法3：使用元组列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cb1e3b4d-b9b9-4895-9c9d-e79ac25ea315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='你是一个翻译机器人，我说中文你就直接翻译成英文，不要啰嗦。'),\n",
       " HumanMessage(content='你好'),\n",
       " AIMessage(content='hello'),\n",
       " HumanMessage(content='你知道孙悟空吗？')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个翻译机器人，我说中文你就直接翻译成英文，不要啰嗦。\"),\n",
    "    (\"human\", \"你好\"),\n",
    "    (\"ai\", \"hello\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "prompt.format_messages(input=\"你知道孙悟空吗？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07473fc0-bfd0-4655-ac8c-2bfdb438b7ca",
   "metadata": {},
   "source": [
    "### 构造方法4：使用 xxxPromptTemplate 列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "782d90f7-a746-4792-8cfd-64117aafab7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='你是一个翻译机器人，我说中文你就直接翻译成英文，不要啰嗦。'),\n",
       " HumanMessage(content='你好'),\n",
       " AIMessage(content='hello'),\n",
       " HumanMessage(content='你知道孙悟空吗？')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\"你是一个翻译机器人，我说中文你就直接翻译成英文，不要啰嗦。\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"你好\"),\n",
    "        AIMessagePromptTemplate.from_template(\"hello\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "# print(prompt)\n",
    "prompt.format_messages(input=\"你知道孙悟空吗？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec999418-ac4d-4e34-85c7-ae248bcb4f7c",
   "metadata": {},
   "source": [
    "### 构造方法5：使用 MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "522a68dc-98db-49f2-89e0-e7c72b3e81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\"你是一个翻译机器人，我说中文你就直接翻译成英文，不要啰嗦。\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"你好\"),\n",
    "        AIMessagePromptTemplate.from_template(\"hello\"),\n",
    "        MessagesPlaceholder(variable_name=\"input\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6078de4b-5d76-4763-80ca-185888f8607f",
   "metadata": {},
   "source": [
    "<div class=\"alert-warning\" style=\"padding: 5px\">\n",
    "<b>注意：</b><br>\n",
    "    MessagesPlaceholder 是一个待填充的<b>消息列表</b>，而不是字符串。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0ffdbac0-0005-40f5-9da0-3af1b4168e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='你是一个翻译机器人，我说中文你就直接翻译成英文，不要啰嗦。'),\n",
       " HumanMessage(content='你好'),\n",
       " AIMessage(content='hello'),\n",
       " HumanMessage(content='你知道孙悟空吗？')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重复前面的示例，打印提示模板\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "my_input = [HumanMessage(content=\"你知道孙悟空吗？\")]\n",
    "prompt.format_messages(input=my_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbbe8f1-2f6a-4877-8b63-43dd7a62a191",
   "metadata": {},
   "source": [
    "### 在 LCEL 中使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c922fb6-d4c3-43d2-8bf1-865b3b5bf3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do you know about Sun Wukong?'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "chain = prompt|llm|StrOutputParser()\n",
    "\n",
    "chain.invoke({\"input\": \"你知道孙悟空吗\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108e720-9d2b-4d6e-8448-73a121fb1e46",
   "metadata": {},
   "source": [
    "## 局部修改提示语模板"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8854e486-170c-4d69-8c25-050322432360",
   "metadata": {},
   "source": [
    "### 构造方法1：使用 partial 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e60c5b4b-bdda-44b4-b63c-7641174b2c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foobaz\n"
     ]
    }
   ],
   "source": [
    "# 通过局部修改实现提示语管理\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(template=\"{foo}{bar}\", input_variables=[\"foo\", \"bar\"])\n",
    "partial_prompt = prompt.partial(foo=\"foo\")\n",
    "print(partial_prompt.format(bar=\"baz\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f847f6c9-897a-448e-9ea7-296003db9c36",
   "metadata": {},
   "source": [
    "### 构造方法2：在 PromptTemplate 中修改 partial_variables 变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5b27f319-bb29-4d8b-a2e0-bf7f7724b225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foobaz\n"
     ]
    }
   ],
   "source": [
    "# 或者这样做\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{foo}{bar}\", input_variables=[\"bar\"], partial_variables={\"foo\": \"foo\"}\n",
    ")\n",
    "print(prompt.format(bar=\"baz\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30edda9a-e2ae-4bd4-8d93-192aae5345e1",
   "metadata": {},
   "source": [
    "### 构造方法3：partial_variables 变量支持 lambda 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e39a7fd8-b005-4048-9962-ee7d8a8d836c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about the day 01/23/2024, 16:27:57\n"
     ]
    }
   ],
   "source": [
    "# 使用函数\n",
    "from datetime import datetime\n",
    "\n",
    "def _get_datetime():\n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about the day {date}\",\n",
    "    input_variables=[\"adjective\", \"date\"],\n",
    ")\n",
    "partial_prompt = prompt.partial(date=_get_datetime)\n",
    "print(partial_prompt.format(adjective=\"funny\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "99bb0e90-a44d-43e4-9070-0013c758eca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about the day 01/23/2024, 16:28:51\n"
     ]
    }
   ],
   "source": [
    "# 换个方式使用函数\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about the day {date}\",\n",
    "    input_variables=[\"adjective\"],\n",
    "    partial_variables={\"date\": _get_datetime},\n",
    ")\n",
    "print(prompt.format(adjective=\"funny\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffe0cfc-e7ef-4d6a-8592-da87a5b42dc2",
   "metadata": {},
   "source": [
    "## 使用 pipeline 组装提示语模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "15260c2d-780a-4528-9784-82ca6917ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d127372-2576-43ab-8d57-3761210facde",
   "metadata": {},
   "source": [
    "### 布局提示语：full_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "64f96217-df5b-466f-85b2-74416ef18aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_template = \"\"\"{introduction}\n",
    "\n",
    "{example}\n",
    "\n",
    "{start}\"\"\"\n",
    "full_prompt = PromptTemplate.from_template(full_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a34249-d149-48e9-ad65-05efcbeff155",
   "metadata": {},
   "source": [
    "### 局部提示语：introduction_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c467332b-5e3e-4380-adcd-f1e95566663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "introduction_template = \"\"\"You are impersonating {person}.\"\"\"\n",
    "introduction_prompt = PromptTemplate.from_template(introduction_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67430bf8-f30f-4a15-8bc3-fecd19cbaaaa",
   "metadata": {},
   "source": [
    "### 局部提示语：example_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b6b87843-8eec-4f56-8c68-2f9a10dd1904",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_template = \"\"\"Here's an example of an interaction:\n",
    "\n",
    "Q: {example_q}\n",
    "A: {example_a}\"\"\"\n",
    "example_prompt = PromptTemplate.from_template(example_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0b88a7-50ac-4390-baa7-9ca48dac7263",
   "metadata": {},
   "source": [
    "### 局部提示语：start_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0e5b0e85-5e0f-4dcf-8c11-7338bde9dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_template = \"\"\"Now, do this for real!\n",
    "\n",
    "Q: {input}\n",
    "A:\"\"\"\n",
    "start_prompt = PromptTemplate.from_template(start_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f59f81-e24d-48a3-849d-27e7f695dd76",
   "metadata": {},
   "source": [
    "### 集成布局模板和局部模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f86d36e8-584e-4b6e-8e5f-c4c5f75ad67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompts = [\n",
    "    (\"introduction\", introduction_prompt),\n",
    "    (\"example\", example_prompt),\n",
    "    (\"start\", start_prompt),\n",
    "]\n",
    "pipeline_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=full_prompt, pipeline_prompts=input_prompts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8ce75bcd-cb23-4807-8348-090b0c7d9436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person', 'input', 'example_q', 'example_a']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "94a7257f-71b8-496e-867c-fd2e91671994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are impersonating Elon Musk.\n",
      "\n",
      "Here's an example of an interaction:\n",
      "\n",
      "Q: What's your favorite car?\n",
      "A: Tesla\n",
      "\n",
      "Now, do this for real!\n",
      "\n",
      "Q: What's your favorite social media site?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    pipeline_prompt.format(\n",
    "        person=\"Elon Musk\",\n",
    "        example_q=\"What's your favorite car?\",\n",
    "        example_a=\"Tesla\",\n",
    "        input=\"What's your favorite social media site?\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d27a2ab-72b4-489a-ac09-8e779968fc78",
   "metadata": {},
   "source": [
    "# 大模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b24936d-e423-42ac-b78a-9342b19d428f",
   "metadata": {},
   "source": [
    "## LCEL 给大模型带来的能力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe3559e-bf45-4481-a1cf-03f01bc9205e",
   "metadata": {},
   "source": [
    "大模型实现了Runnable接口，统一实现以下接口，因此在任何符合 Runnable 的规范中都很方便使用：\n",
    "\n",
    "- invoke\n",
    "- ainvoke\n",
    "- stream\n",
    "- astream\n",
    "- batch\n",
    "- abatch\n",
    "- astream_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c112da65-27ac-4f1b-b42f-18bd9f029751",
   "metadata": {},
   "source": [
    "### invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "eeaf9c3e-b786-405d-839a-98a1e52b2e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='模型在训练数据上表现良好，但在新的未见过的数据上可能会出现过拟合（overfitting）的情况。正则化是一种用来解决过拟合问题的技术。它通过在损失函数中引入一个正则化项，限制模型的复杂度，防止模型过度拟合训练数据。\\n\\n正则化的目的是平衡模型的拟合能力和泛化能力。如果模型过于复杂，它可能会过度拟合训练数据，导致在新数据上的表现较差。正则化通过对模型参数的惩罚，鼓励模型选择更简单的参数组合，从而降低模型的复杂度。\\n\\n常见的正则化方法包括L1正则化和L2正则化。L1正则化通过在损失函数中添加模型参数的绝对值之和，促使模型参数稀疏化，即让一些参数变为0，从而实现特征选择的效果。L2正则化通过在损失函数中添加模型参数的平方和，降低参数的绝对值，使模型更加平滑。\\n\\n正则化可以帮助减少模型的方差，提高模型的泛化能力，从而在新的未见过的数据上表现更好。它是训练模型时常用的一种技术，可以提高模型的稳定性和可靠性。')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(content=\"模型为什么要做正则化?\"),\n",
    "]\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218400ad-9b18-498d-8e89-9868fecb7e8b",
   "metadata": {},
   "source": [
    "### stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0e480cbc-857b-48a8-801f-2ab3db89e053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型正则化是为了减少过拟合（Overfitting）的发生。在训练模型时，如果模型过于复杂，容易出现过拟合的情况，即在训练集上表现很好，但在未知数据上表现较差。过拟合的原因是因为模型过度拟合了训练数据的噪声和细节，并且没有很好地学习到数据的普遍规律。\n",
      "\n",
      "正则化通过在模型的损失函数中引入正则项，对模型的复杂度进行惩罚，从而降低模型的复杂度。正则化的目的是通过控制模型参数的大小，使模型更加简单，能够更好地泛化到未知数据上。常见的正则化方法有L1正则化和L2正则化。\n",
      "\n",
      "L1正则化通过在损失函数中添加模型参数的L1范数（绝对值之和）作为正则项，可以使得模型的部分参数变为0，从而实现特征选择的效果，减少模型的复杂度。\n",
      "\n",
      "L2正则化通过在损失函数中添加模型参数的L2范数（平方和的平方根）作为正则项，可以使得模型参数的值较小，从而降低模型的复杂度。\n",
      "\n",
      "正则化可以在一定程度上防止过拟合，提高模型的泛化能力，使模型在未知数据上表现更好。"
     ]
    }
   ],
   "source": [
    "for chunk in chat.stream(messages):\n",
    "    print(chunk.content, end=\"｜\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf1d23c-1dd7-456c-9094-ace2569c0304",
   "metadata": {},
   "source": [
    "### batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c500c3-791d-415a-b23d-7dabbe6590ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.batch([messages])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56846f44-a867-4e8b-805b-f7b331ca798e",
   "metadata": {},
   "source": [
    "## 使用内存缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b18d5c06-64c4-41eb-82ff-89b018a7e7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "18730f67-1871-4e90-92a6-46accda6fb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 ms, sys: 2.09 ms, total: 13.3 ms\n",
      "Wall time: 1.98 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure, here's a classic one for you:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ad411a-f696-49a7-9bf5-a743d1f473a0",
   "metadata": {},
   "source": [
    "下面的相同调用不会重复访问大模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5829b899-c311-4e6d-b0cf-08ce66197e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.24 ms, sys: 101 µs, total: 1.34 ms\n",
      "Wall time: 2.09 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure, here's a classic one for you:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The second time it is, so it goes faster\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa6191e-2a40-4e90-8d17-8ccdd7fc0b6e",
   "metadata": {},
   "source": [
    "## 使用SQLite缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3383d107-bac8-4304-bfa7-4fcf44a359f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: .langchain.db: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm .langchain.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fe5edcbc-0dee-4a1a-85a6-f08c329d4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do the same thing with a SQLite cache\n",
    "from langchain.cache import SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "67f251c9-4e68-4d62-abc8-ac595a289b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.23 ms, sys: 1.27 ms, total: 4.49 ms\n",
      "Wall time: 3.59 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sure, here's a classic one for you:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\""
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm.predict(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5943571d-f4c3-49f6-b7c4-c37e9f9f265b",
   "metadata": {},
   "source": [
    "## Token跟踪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d2b4508d-7010-446e-8744-32328bcb0823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 35\n",
      "\tPrompt Tokens: 12\n",
      "\tCompletion Tokens: 23\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $6.4e-05\n",
      "Tokens Used: 32\n",
      "\tPrompt Tokens: 13\n",
      "\tCompletion Tokens: 19\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00153\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm.invoke(\"Tell me a new joke\")\n",
    "    print(cb)\n",
    "\n",
    "llm4 = ChatOpenAI(model_name=\"gpt-4\")\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm4.invoke(\"Tell me a new apple joke\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3974273d-92e1-4dff-be9e-971cb17e1a67",
   "metadata": {},
   "source": [
    "# 从文件加载提示语模板"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5463439c-8110-48c5-be21-81f0dd21a5ad",
   "metadata": {},
   "source": [
    "## yaml格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c400265a-f80a-40a7-bbc6-6f7f63ee1cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    " _type: prompt\n",
    "input_variables:\n",
    "    [\"adjective\", \"content\"]\n",
    "template: \n",
    "    Tell me a {adjective} joke about {content}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39071cc3-0207-41d5-9b55-e054a6c6aa90",
   "metadata": {},
   "source": [
    "## json格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5d51063-eec2-4fc1-9ca8-2960dd4eb1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_type': 'prompt',\n",
       " 'input_variables': ['adjective', 'content'],\n",
       " 'template': 'Tell me a {adjective} joke about {content}.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"_type\": \"prompt\",\n",
    "    \"input_variables\": [\"adjective\", \"content\"],\n",
    "    \"template\": \"Tell me a {adjective} joke about {content}.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa1ae3c-ea74-40b3-b918-89b56add4a88",
   "metadata": {},
   "source": [
    "## json + txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2145f045-8cf3-479c-9c72-f16f691c8262",
   "metadata": {},
   "source": [
    "首先，将模板主要内容写入**final_step.txt**："
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ea02ebc-f8d7-476b-8bbf-69064dea8f93",
   "metadata": {},
   "source": [
    "你的名字是{ai_name}，你是{ai_role}\n",
    "\n",
    "你的任务是:\n",
    "{task_description}\n",
    "\n",
    "经过以下的思考过程，你已经完成任务:\n",
    "{short_term_memory}\n",
    "\n",
    "现在请详细给出你的最终答案:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ebb50c-00bb-4fbd-8160-01d0a696ce9a",
   "metadata": {},
   "source": [
    "然后，在**task.json**文件中指定**template_path**嵌入路径："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f4c129ad-9560-448e-9e7f-cc0ea2e2194d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_type': 'prompt',\n",
       " 'input_variables': ['ai_name',\n",
       "  'ai_role',\n",
       "  'task_description',\n",
       "  'short_term_memory'],\n",
       " 'template_path': 'final_step.txt'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"_type\": \"prompt\",\n",
    "    \"input_variables\": [\n",
    "      \"ai_name\",\n",
    "      \"ai_role\",\n",
    "      \"task_description\",\n",
    "      \"short_term_memory\"\n",
    "    ],\n",
    "    \"template_path\": \"final_step.txt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84d54ab-a3a9-44e4-8592-dab3857904b9",
   "metadata": {},
   "source": [
    "## 加载提示语模板文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88dd5344-db2d-48d2-9070-11100de48dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about Xiao Ming.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt(\"simple_prompt.json\")\n",
    "print(prompt.format(adjective=\"funny\", content=\"Xiao Ming\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa492fa-561c-4a90-8195-2a7b4885470e",
   "metadata": {},
   "source": [
    "# OutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49da2be0-86b6-47f5-b962-638759b7dba7",
   "metadata": {},
   "source": [
    "自动把 LLM 输出的字符串按指定格式加载。\n",
    "\n",
    "LangChain 内置的 OutputParser 包括:\n",
    "\n",
    "- StrOutputParser\n",
    "- OpenAIFunctions\n",
    "- ListParser\n",
    "- DatetimeParser\n",
    "- EnumParser\n",
    "- PydanticParser\n",
    "- XMLParser\n",
    "等等"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfc056e-a0c5-42ca-b1fc-b17268359cfc",
   "metadata": {},
   "source": [
    "## String parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682b603d-4f71-4425-9922-26ea75cd4c12",
   "metadata": {},
   "source": [
    "## JSON parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0f373e25-642c-499f-be9e-7b7da6ccd3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "48e6d5bd-9880-42db-9256-f5d3160c245d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query'] partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n```'} template='Answer the user query.\\n{format_instructions}\\n{query}\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't scientists trust atoms?\",\n",
       " 'punchline': 'Because they make up everything!'}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "print(prompt)\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd09d4ae-f4d1-472f-9e7c-66a672e87323",
   "metadata": {},
   "source": [
    "**JsonOutputFunctionsParser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c11cec0b-7ac4-4b09-ad0a-c1c5d647842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "parser = JsonOutputFunctionsParser()\n",
    "\n",
    "# 绑定openai函数，并使用json解析出参数\n",
    "chain = prompt | model.bind(functions=openai_functions) | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3b4f5b68-4514-491f-8569-789604a6d376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't scientists trust atoms?\",\n",
       " 'punchline': 'Because they make up everything!'}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"tell me a joke\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8da7210-d236-487e-ab08-1b934fbe67b9",
   "metadata": {},
   "source": [
    "**JsonKeyOutputFunctionsParser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9f8f9743-8a46-4b7c-be95-149c10d54c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "\n",
    "class Jokes(BaseModel):\n",
    "    \"\"\"Jokes to tell user.\"\"\"\n",
    "\n",
    "    joke: List[Joke]\n",
    "    funniness_level: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e522cb5d-0aa2-4ea7-aaff-b330ce9596ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = JsonKeyOutputFunctionsParser(key_name=\"joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e85b4820-6ae0-4454-894f-ff528981f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_functions = [convert_pydantic_to_openai_function(Jokes)]\n",
    "chain = prompt | model.bind(functions=openai_functions) | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e5322f1a-9740-4cbb-b063-39dcaca205d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'setup': \"Why don't scientists trust atoms?\",\n",
       "  'punchline': 'Because they make up everything!'},\n",
       " {'setup': 'Why did the scarecrow win an award?',\n",
       "  'punchline': 'Because he was outstanding in his field!'}]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"tell me two jokes\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8699284c-19d8-41c1-b134-4c5ce494e458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[{}]\n",
      "[{'setup': ''}]\n",
      "[{'setup': 'Why'}]\n",
      "[{'setup': 'Why don'}]\n",
      "[{'setup': \"Why don't\"}]\n",
      "[{'setup': \"Why don't scientists\"}]\n",
      "[{'setup': \"Why don't scientists trust\"}]\n",
      "[{'setup': \"Why don't scientists trust atoms\"}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\"}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': ''}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': ''}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scare'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win an'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win an award'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win an award?'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win an award?', 'punchline': ''}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win an award?', 'punchline': 'Because'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win an award?', 'punchline': 'Because he'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win an award?', 'punchline': 'Because he was'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win an award?', 'punchline': 'Because he was outstanding'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win an award?', 'punchline': 'Because he was outstanding in'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win an award?', 'punchline': 'Because he was outstanding in his'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win an award?', 'punchline': 'Because he was outstanding in his field'}]\n",
      "[{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}, {'setup': 'Why did the scarecrow win an award?', 'punchline': 'Because he was outstanding in his field!'}]\n"
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"input\": \"tell me two jokes\"}):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6103aa96-932d-48db-bf63-04175a2430ba",
   "metadata": {},
   "source": [
    "## Enum parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "39ca00a7-5b3d-4519-8e4c-c94975122016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.enum import EnumOutputParser\n",
    "from enum import Enum\n",
    "\n",
    "class Colors(Enum):\n",
    "    RED = \"red\"\n",
    "    GREEN = \"green\"\n",
    "    BLUE = \"blue\"\n",
    "\n",
    "parser = EnumOutputParser(enum=Colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "72e4d393-03f9-45f9-a285-067b0421a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"What color eyes does this person have?\n",
    "\n",
    "> Person: {person}\n",
    "\n",
    "Instructions: {instructions}\"\"\"\n",
    ").partial(instructions=parser.get_format_instructions())\n",
    "chain = prompt | ChatOpenAI() | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "428bec9e-f589-47bb-b9ac-642ad6fe1274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['person'] partial_variables={'instructions': 'Select one of the following options: red, green, blue'} template='What color eyes does this person have?\\n\\n> Person: {person}\\n\\nInstructions: {instructions}'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Colors.BLUE: 'blue'>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(prompt)\n",
    "chain.invoke({\"person\": \"Frank Sinatra\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dfce27-d347-4491-8f5d-6802600e8781",
   "metadata": {},
   "source": [
    "## Datetime parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e2864bdd-c00a-4251-a05f-f4d939e01b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "220f45a1-29e2-4847-917c-06c363569c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = DatetimeOutputParser()\n",
    "template = \"\"\"Answer the users question:\n",
    "\n",
    "{question}\n",
    "\n",
    "{format_instructions}\"\"\"\n",
    "prompt = PromptTemplate.from_template(\n",
    "    template,\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9136cb36-5f56-448f-adaf-0485d37505de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], partial_variables={'format_instructions': \"Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\\n\\nExamples: 0126-01-12T17:02:00.512595Z, 0719-05-06T14:30:04.335045Z, 1391-07-06T00:25:48.331596Z\\n\\nReturn ONLY this string, no other words!\"}, template='Answer the users question:\\n\\n{question}\\n\\n{format_instructions}')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "722f08b7-30e5-4d20-a287-d4af664e50f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-01-03 18:15:05\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | OpenAI() | output_parser\n",
    "output = chain.invoke({\"question\": \"when was bitcoin founded?\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53de56f-f0e8-4cf1-a821-a553f38b9c0c",
   "metadata": {},
   "source": [
    "## CSV parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "54e193f1-5572-4a48-a894-6c59370d9692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List five {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "072ad855-bfde-4556-b7d3-ff8f8fb08a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vanilla',\n",
       " 'Chocolate',\n",
       " 'Strawberry',\n",
       " 'Mint Chocolate Chip',\n",
       " 'Cookies and Cream']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"subject\": \"ice cream flavors\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e84d48ab-5c1a-4456-96ea-9707e6fd97fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Vanilla']\n",
      "['Chocolate']\n",
      "['Strawberry']\n",
      "['Mint Chocolate Chip']\n",
      "['Cookies and Cream']\n"
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"subject\": \"ice cream flavors\"}):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c48842-48bb-48e0-a851-75d0a59e7427",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b68b78-2617-417b-8024-88b8d6e59fbf",
   "metadata": {},
   "source": [
    "## 使用 create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf44eb73-1aa8-4d35-b8d4-f4fb85d2ad8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe currently activated Python version 3.9.18 is not supported by the project (>=3.10,<3.12).\n",
      "Trying to find and use a compatible version.\u001b[39m \n",
      "Using \u001b[36mpython3\u001b[39m (3.10.0)\n",
      "The following packages are already present in the pyproject.toml and will be skipped:\n",
      "\n",
      "  • \u001b[36mlangchainhub\u001b[39m\n",
      "\n",
      "If you want to update it to the latest compatible version, you can use `poetry update package`.\n",
      "If you prefer to upgrade it to the latest available version, you can use `poetry add package@latest`.\n",
      "\n",
      "Nothing to add.\n"
     ]
    }
   ],
   "source": [
    "!poetry add langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a1e4070-db9f-4761-84c5-ab49e0cebd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain import hub\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c013afd-e2e6-496f-8630-c9b5cff191fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt\n",
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "538956c4-9d49-4268-9214-4cd25b39166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# receiver\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"我家的狗狗喜欢吃蜂蜜\", \"我家的猫喜欢是白色的\"],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# llm\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# combine_docs_chain\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, retrieval_qa_chat_prompt)\n",
    "\n",
    "# retrieval_chain\n",
    "retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ac99c68-1f88-4b0e-aa10-354bd2e45bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '我家的狗狗喜欢吃什么？',\n",
       " 'context': [Document(page_content='我家的狗狗喜欢吃蜂蜜'),\n",
       "  Document(page_content='我家的猫喜欢是白色的')],\n",
       " 'answer': '我家的狗狗喜欢吃蜂蜜。'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invoke\n",
    "retrieval_chain.invoke({\"input\": \"我家的狗狗喜欢吃什么？\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bbe88a-abe1-41bd-b050-4cef367d37e1",
   "metadata": {},
   "source": [
    "## 使用 LCEL 构建 RAG 链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16da4f8f-544e-4ad3-9ed8-843f3d0ab1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'白色的。'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"我家的狗狗喜欢吃蜂蜜\", \"我家的猫喜欢是白色的\"],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"基于以下提供的信息回答问题:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "不要啰嗦。\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "\n",
    "chain.invoke(\"你猜猜我家有一只什么颜色的猫？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659c6fdf-0a23-49d0-8250-6ea27868dcfe",
   "metadata": {},
   "source": [
    "# 其他场景"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be72852e-30f3-4987-a84f-baeb620334ec",
   "metadata": {},
   "source": [
    "## 通过文本向量路由Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4002ea3d-a7e9-4492-a5de-ebb4146b65c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "physics_template = \"\"\"你是一个物理学教授，负责给数学爱好者解答疑惑。\\\n",
    "你正在为小学生回答问题，注意使用小学生水平能听懂的词汇，避免过于专业晦涩的术语。 \\\n",
    "当你不知道答案时就回答不知道。\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"你是一个数学家，负责给小学生解答疑惑。\n",
    "注意使用小学生水平能听懂的词汇，避免过于专业晦涩的术语。 \\\n",
    "回答时，请举一些生活中的例子。\n",
    "当你不知道答案时就回答不知道。\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "\n",
    "def prompt_router(input):\n",
    "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "common_train = ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "43793a9c-bf60-42ba-9c86-e7cfeff80215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "黑洞是一种极度密集的天体，它具有非常强大的引力场，以至于连光都无法逃离它的吸引。黑洞的形成是由于一个恒星在死亡时，其质量过大，无法通过核聚变维持稳定，导致恒星坍缩成一个极为紧凑的物体。黑洞的中心部分称为奇点，奇点的密度和引力非常之大，超过了任何已知物质的极限。\n",
      "\n",
      "黑洞的存在可以通过它们产生的引力效应来间接观测，例如吸收附近的物质、扭曲周围空间和发射强烈的辐射。虽然我们无法直接观测到黑洞，但科学家们通过观测它们对周围物体的影响，以及通过天文观测和数学模型来研究黑洞的性质和行为。\n",
      "\n",
      "黑洞在宇宙中广泛存在，它们可能是恒星坍缩形成的中等质量黑洞，也可能是超大质量黑洞，如位于银河系中心的超大质量黑洞。黑洞对宇宙的演化和结构具有重要影响，它们是天体物理学和相对论研究的重要对象。\n"
     ]
    }
   ],
   "source": [
    "print(common_train.invoke(\"黑洞是什么？\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a463dc35-f5da-40b7-ae75-52203d1e2a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PHYSICS\n",
      "小朋友，黑洞是宇宙中一种非常特殊的东西。它是一种非常强大的引力场，就像一个很大的吸力。当一颗非常大的恒星（就是我们看到的星星）燃烧完燃料后，它会塌缩成一个非常小又非常密集的东西，就是黑洞。黑洞的引力非常强大，甚至连光也无法逃脱它的吸引力。所以我们看不到黑洞，它是非常神秘的。关于黑洞，科学家们还在研究中，有很多有趣的发现等待我们去探索。\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"黑洞是什么？\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "69d9155f-e7ab-4fc3-b83a-54cd5f79fe69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "路径积分是一个物理学概念，用来描述在一个力场中沿着一条曲线路径上的力的积累效果。简单来说，路径积分是将一个向量场沿着一条曲线进行积分，得到沿着该曲线的总体积效应。\n",
      "\n",
      "在物理学中，路径积分可以用来计算沿着一个曲线路径上的力的总效果，比如沿着一条曲线上的力的总功或者总位移。路径积分的计算方式是将力场在曲线上的每个点上的力与微小位移相乘，然后将所有微小的力与位移的乘积相加，得到曲线上的总效果。\n",
      "\n",
      "路径积分在许多领域中都有重要的应用，比如在力学中用于计算物体在曲线路径上的总功、在电磁学中用于计算电场或磁场沿着曲线的总位移等。路径积分的计算可以通过数学上的积分运算来实现，根据具体情况可以采用不同的积分方法，比如定积分或线积分等。\n"
     ]
    }
   ],
   "source": [
    "print(common_train.invoke(\"路径积分是什么？\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "508efa40-58d2-4a4d-9e07-87f5dcaabe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MATH\n",
      "路径积分是一种数学工具，它在物理学中常常被用来描述粒子或光在空间中的运动。你可以把路径积分想象成一个粒子或光在不同路径上行走的概率，就像我们在城市里选择不同的路线去目的地一样。\n",
      "\n",
      "想象一下你要从学校回家，有很多条路可以选择。每条路都有不同的长度、不同的交通状况和不同的风景。路径积分就是用来计算你选择每条路的概率，也就是说，你走每条路的可能性有多大。\n",
      "\n",
      "在物理学中，粒子或光在空间中运动的时候，也有很多可能的路径可以选择。路径积分可以帮助我们计算出每条路径的概率，从而更好地理解粒子或光的行为。\n",
      "\n",
      "但是，具体如何计算路径积分，需要更深入的数学知识和物理背景。这里只是简单介绍了路径积分的概念，如果你对它感兴趣，可以在以后的学习中深入了解。\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"路径积分是什么？\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51042965-b313-4b28-8b51-d4df928e9cf7",
   "metadata": {},
   "source": [
    "## 执行python代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e264dc04-4314-4736-a76c-fb0bc578e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec5882ad-5528-41d9-a927-1c634c321e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Write some python code to solve the user's problem. \n",
    "\n",
    "Return only python code in Markdown format, e.g.:\n",
    "\n",
    "```python\n",
    "....\n",
    "```\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", template), (\"human\", \"{input}\")])\n",
    "\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c8904de2-a9ad-428a-8bf6-0eaa9dd3ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sanitize_output(text: str):\n",
    "    print(text)\n",
    "    _, after = text.split(\"```python\")\n",
    "    return after.split(\"```\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "81461a08-c2d9-45f2-937c-c6634f570346",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | StrOutputParser() | _sanitize_output | PythonREPL().run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c4e3c00f-dcf9-4a80-95ea-0c0410267017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我们可以使用穷举法来解决这个问题。\n",
      "\n",
      "假设有 x 只兔子，y 只鸡。根据题意，可以得到以下两个方程：\n",
      "\n",
      "x + y = 5   # 头的数量\n",
      "4x + 2y = 12  # 脚的数量\n",
      "\n",
      "我们可以通过求解这个方程组来得到兔子和鸡的数量。让我们来编写代码实现这个算法。\n",
      "\n",
      "```python\n",
      "def solve():\n",
      "    for x in range(6):  # 兔子的数量最多为5只\n",
      "        y = 5 - x  # 根据第一个方程计算鸡的数量\n",
      "        if 4*x + 2*y == 12:  # 检查第二个方程是否满足\n",
      "            return x, y  # 返回兔子和鸡的数量\n",
      "\n",
      "rabbit, chicken = solve()\n",
      "print(f\"兔子的数量为：{rabbit} 只，鸡的数量为：{chicken} 只\")\n",
      "```\n",
      "\n",
      "运行这段代码，我们可以得到输出：\n",
      "\n",
      "```\n",
      "兔子的数量为：1 只，鸡的数量为：4 只\n",
      "```\n",
      "\n",
      "所以，笼子里有1只兔子和4只鸡。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'兔子的数量为：1 只，鸡的数量为：4 只\\n'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"一个笼子里有兔子和鸡若干，数一数有5个头，12只脚，请问有多少只兔子多少只鸡？\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957fe527-2db2-4f20-a5d1-778e11196025",
   "metadata": {},
   "source": [
    "## 查询数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7be6d4c3-3ebc-4c09-a206-35f536432380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Based on the table schema below, write a SQL query that would answer the user's question:\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "SQL Query:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4b05c675-6291-4759-9f5a-b61b531d2a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SQLDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4d95f54f-1591-447d-aedc-11b5e2363c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SQLDatabase.from_uri(\"sqlite:///./Chinook.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f2f5b9fa-c6d3-461d-b429-4fed0c95fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema(_):\n",
    "    return db.get_table_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fc4e86c3-6ac7-4b3d-9c0d-f2de665400f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query):\n",
    "    return db.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "52e9079c-8246-4077-848e-1e13ff0bafcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "sql_response = (\n",
    "    RunnablePassthrough.assign(schema=get_schema)\n",
    "    | prompt\n",
    "    | model.bind(stop=[\"\\nSQLResult:\"])\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7643bf79-908f-468d-823f-e9297b7ce940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT COUNT(*) FROM Employee'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 直接生成查询语句\n",
    "sql_response.invoke({\"question\": \"How many employees are there?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6a86b556-8fb6-487b-a98a-2adbbb697e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Based on the table schema below, question, sql query, and sql response, write a natural language response:\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Response: {response}\n",
    "\n",
    "请用中文回答。\n",
    "\"\"\"\n",
    "prompt_response = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ad471862-98d0-4b65-9a0e-9808abe854e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意要分两阶段执行assign：先生成SQL，才能执行SQL\n",
    "full_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        schema=get_schema,\n",
    "        query=sql_response\n",
    "    ).assign(\n",
    "        response=lambda x: db.run(x[\"query\"]),\n",
    "    )\n",
    "    | prompt_response\n",
    "    | model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "04a292fd-00f4-46c3-9ffb-c547abb5ebc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='员工人数是8人。')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"员工人数是多少?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a335e1-adfa-4ebb-aeb5-fe7f2f647cd2",
   "metadata": {},
   "source": [
    "# 集成langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c184133-600c-476a-b19f-022a14716781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langfuse.callback import CallbackHandler\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a568af10-0d5b-467d-bd2b-bd50fe253da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = CallbackHandler(trace_name=\"learning-langchain\", user_id=\"homeway\", session_id=str(uuid.uuid4()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "356fd868-cca0-4c79-82f7-a70ad50622d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model = \"gpt-3.5-turbo\", streaming = False, temperature = 0.5)\n",
    "parser = StrOutputParser()\n",
    "prompt = ChatPromptTemplate.from_template(\"hi\")\n",
    "train = (prompt | llm | parser)\n",
    "train.invoke({}, config = {\"callbacks\": [handler]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bad0cba7-8f2b-4bae-995f-85957fe9666a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='你是薛宏伟。')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 多轮对话\n",
    "from langchain.schema import (\n",
    "    AIMessage, #等价于OpenAI接口中的 assistant role\n",
    "    HumanMessage, #等价于OpenAI接口中的 user role\n",
    "    SystemMessage #等价于OpenAI接口中的 system role\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"你是AGIClass的课程助理。\"), \n",
    "    HumanMessage(content=\"我是学员，我叫薛宏伟。\"), \n",
    "    AIMessage(content=\"欢迎！\"),\n",
    "    HumanMessage(content=\"我是谁\") \n",
    "]\n",
    "llm.invoke(messages) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be80214d-8bf3-4cee-9170-4eb27b3048b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='我是广州鸿蒙的客服助手，名字叫蒙蒙。有什么可以帮到您的吗？')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对话提示语模板\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"你是{product}的客服助手。你的名字叫{name}\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = template.format_messages(\n",
    "        product=\"广州鸿蒙\",\n",
    "        name=\"蒙蒙\",\n",
    "        query=\"你是谁\"\n",
    "    )\n",
    "\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87250f5b-935e-4c81-bd93-424bf1f6d9b3",
   "metadata": {},
   "source": [
    "# 集成langserve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42b11bd-6f91-4aec-a3a1-f4b05b6da3d0",
   "metadata": {},
   "source": [
    "## 与fastapi一起使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c1e0ae-2ffa-4431-856d-edcda649f3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langserve import add_routes\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Server\",\n",
    "    version=\"1.0\",\n",
    "    description=\"A simple api server using Langchain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    ChatOpenAI(),\n",
    "    path=\"/openai\",\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "add_routes(\n",
    "    app,\n",
    "    prompt | model,\n",
    "    path=\"/joke\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a45e06a-8fa5-4895-82eb-c978de257286",
   "metadata": {},
   "source": [
    "## 与langfuse一起使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbe4dea-20fb-4ad2-801c-c1c1a691e5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = CallbackHandler(trace_name=\"chat_once\", user_id=\"wencheng\")\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"{question}\"\"\")\n",
    "llm = ChatOpenAI(model = \"gpt-3.5-turbo-16k\", streaming = True, temperature = 0)\n",
    "chain = (prompt | llm | parser).with_config({\"callbacks\": [handler]})\n",
    "\n",
    "add_routes(app, chain, path = \"/langserve/chat_once\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0224be3-bd65-403b-853a-341951f3b368",
   "metadata": {},
   "source": [
    "## python Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125c80a3-06b9-4eeb-9560-ac2faa1a40b7",
   "metadata": {},
   "source": [
    "```python\n",
    "from langserve import RemoteRunnable\n",
    "chat_once = RemoteRunnable(\"http://localhost:8000/langserve/chat_once\")\n",
    "chat_once.invoke({\"question\": \"能帮我介绍一下东莞吗？\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec03ed0-2dac-49f8-9cad-2b535d50f5bb",
   "metadata": {},
   "source": [
    "```python\n",
    "for chunk in chat_once.stream({\"question\": \"能帮我介绍一下东莞吗？\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829cceea-6f76-42e7-b58e-d68488612d50",
   "metadata": {},
   "source": [
    "# javascript client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4326915-e794-413e-8518-da3d2dc1d5c0",
   "metadata": {},
   "source": [
    "```sh\n",
    "yarn add langchain\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d3595c-89c0-4328-adb7-5205c47fdcf9",
   "metadata": {},
   "source": [
    "## 调用invoke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e81171-aeaf-42fe-92f8-69d3fc2a649e",
   "metadata": {},
   "source": [
    "```javascript\n",
    "import { RemoteRunnable } from \"langchain/runnables/remote\";\n",
    "\n",
    "const remoteChain = new RemoteRunnable({\n",
    "  url: \"https://your_hostname.com/path\",\n",
    "});\n",
    "\n",
    "const result = await remoteChain.invoke({\n",
    "  param1: \"param1\",\n",
    "  param2: \"param2\",\n",
    "});\n",
    "\n",
    "console.log(result);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaef472-51be-4643-a8ff-0ba0ad0ccab8",
   "metadata": {},
   "source": [
    "## 调用stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622d784a-42f7-4762-b763-53a1cf488816",
   "metadata": {},
   "source": [
    "```javascript\n",
    "const stream = await remoteChain.stream({\n",
    "  param1: \"param1\",\n",
    "  param2: \"param2\",\n",
    "});\n",
    "\n",
    "for await (const chunk of stream) {\n",
    "  console.log(chunk);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9405743c-85b7-4ba6-a78d-6b2f2f38a884",
   "metadata": {},
   "source": [
    "## 使用config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b88cb5-3d7d-4502-9aaf-25438bb37fa5",
   "metadata": {},
   "source": [
    "```javascript\n",
    "import { RemoteRunnable } from \"langchain/runnables/remote\";\n",
    "\n",
    "const remoteChain = new RemoteRunnable({\n",
    "  url: \"https://your_hostname.com/path\",\n",
    "  options: {\n",
    "    timeout: 10000,\n",
    "    headers: {\n",
    "      Authorization: \"Bearer YOUR_TOKEN\",\n",
    "    },\n",
    "  },\n",
    "});\n",
    "\n",
    "const result = await remoteChain.invoke({\n",
    "  param1: \"param1\",\n",
    "  param2: \"param2\",\n",
    "});\n",
    "\n",
    "console.log(result);\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchani-book-py3.10-ipykernel",
   "language": "python",
   "name": "langchani-book-py3.10-ipykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
