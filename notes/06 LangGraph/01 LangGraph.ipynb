{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71090e79-690c-4028-81c8-0286b5ac88d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85aa431-6963-4259-8885-adaa72b29be7",
   "metadata": {},
   "source": [
    "# 基本用法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cce36f-a4b0-4553-8be1-42eb0c467c5d",
   "metadata": {},
   "source": [
    "## 例子1：简单的invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c496bce3-bff3-4a01-ba79-9eec42317d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': ['AAA', 'BBB']}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph import END\n",
    "import operator\n",
    "\n",
    "# 定义状态\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[str], operator.add]\n",
    "\n",
    "# 定义图\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# 定义节点\n",
    "def nodeA(state):\n",
    "    return {\"messages\": [\"AAA\"]}\n",
    "\n",
    "workflow.add_node(\"a\", nodeA)\n",
    "\n",
    "# 定义节点\n",
    "def nodeB(state):\n",
    "    return {\"messages\": [\"BBB\"]}\n",
    "\n",
    "workflow.add_node(\"b\", nodeB)\n",
    "\n",
    "# 定义边\n",
    "workflow.add_edge(\"a\", \"b\")\n",
    "workflow.add_edge(\"b\", END)\n",
    "workflow.set_entry_point(\"a\")\n",
    "\n",
    "# \n",
    "app = workflow.compile()\n",
    "\n",
    "# RUN\n",
    "app.invoke({\"messages\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75b26fb-cc85-4420-8449-bda737f56066",
   "metadata": {},
   "source": [
    "## 例子2：MessageGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "35d2f358-7cec-4a93-bbf9-c86fd2825553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.fake import FakeStreamingListLLM\n",
    "llm = FakeStreamingListLLM(responses=[\"你好，我是一个模拟的大语言模型\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "f108ea18-8d72-4e6f-b059-1544215a42f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import StateGraph, MessageGraph, END\n",
    "import operator\n",
    "\n",
    "# 定义图\n",
    "workflow = MessageGraph()\n",
    "\n",
    "# 定义节点\n",
    "def nodeA(state):\n",
    "    return [\"AAA\"]\n",
    "\n",
    "workflow.add_node(\"a\", nodeA)\n",
    "\n",
    "# 定义节点\n",
    "def nodeB(state):\n",
    "    return [\"BBB\"]\n",
    "\n",
    "workflow.add_node(\"b\", llm)\n",
    "\n",
    "# 定义边\n",
    "workflow.add_edge(\"a\", \"b\")\n",
    "workflow.add_edge(\"b\", END)\n",
    "workflow.set_entry_point(\"a\")\n",
    "\n",
    "# \n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f7682210-52b8-4474-8c74-1ec7a01f3e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': ['AAA']}\n",
      "{'b': '你好，我是一个模拟的大语言模型'}\n",
      "{'__end__': ['AAA', '你好，我是一个模拟的大语言模型']}\n"
     ]
    }
   ],
   "source": [
    "for s in app.stream([]):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "8b74a04f-ec1e-41a4-aa80-d8ec4d7c2bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>> 你\n",
      ">>>>>>>>>> 好\n",
      ">>>>>>>>>> ，\n",
      ">>>>>>>>>> 我\n",
      ">>>>>>>>>> 是\n",
      ">>>>>>>>>> 一\n",
      ">>>>>>>>>> 个\n",
      ">>>>>>>>>> 模\n",
      ">>>>>>>>>> 拟\n",
      ">>>>>>>>>> 的\n",
      ">>>>>>>>>> 大\n",
      ">>>>>>>>>> 语\n",
      ">>>>>>>>>> 言\n",
      ">>>>>>>>>> 模\n",
      ">>>>>>>>>> 型\n"
     ]
    }
   ],
   "source": [
    "# RUN\n",
    "async for chunk in app.astream_events([], version=\"v1\"):\n",
    "    # print(\" \"*10, chunk['event'], chunk['name'], chunk['tags'])\n",
    "    if(chunk['event']==\"on_chain_stream\" and chunk['name']==\"b\"):\n",
    "        print(\">\"*10, chunk['data']['chunk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75be8467-64a0-434a-98b2-de707fe09d5d",
   "metadata": {},
   "source": [
    "## 例子3：Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f052a730-5f57-40ae-a862-be872bf91ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.fake import FakeStreamingListLLM\n",
    "llm = FakeStreamingListLLM(responses=[\"你好，我是一个模拟的大语言模型\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "8ed78778-ed99-409b-ab33-30aa88850aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "736d08c8-2167-49a9-9f21-44b2100c007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    { \"messages\": lambda x: x['messages'] }\n",
    "    | chat_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0767ee6f-9917-41b1-9e37-47712c3003ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           LangGraph on_chain_start []\n",
      "           __start__ on_chain_start ['graph:step:0', 'langsmith:hidden']\n",
      "           __start__ on_chain_stream ['graph:step:0', 'langsmith:hidden']\n",
      "           __start__ on_chain_end ['graph:step:0', 'langsmith:hidden']\n",
      "           __start__ on_chain_start ['graph:step:0', 'langsmith:hidden']\n",
      "           __start__ on_chain_end ['graph:step:0', 'langsmith:hidden']\n",
      "           __start__:edges on_chain_start ['graph:step:1', 'langsmith:hidden']\n",
      "           ChannelRead<['messages']> on_chain_start ['seq:step:1', 'langsmith:hidden']\n",
      "           ChannelRead<['messages']> on_chain_stream ['seq:step:1', 'langsmith:hidden']\n",
      "           ChannelWrite<__Node_A__:inbox> on_chain_start ['seq:step:2', 'langsmith:hidden']\n",
      "           ChannelWrite<__Node_A__:inbox> on_chain_stream ['seq:step:2', 'langsmith:hidden']\n",
      "           __start__:edges on_chain_stream ['graph:step:1', 'langsmith:hidden']\n",
      "           ChannelRead<['messages']> on_chain_end ['seq:step:1', 'langsmith:hidden']\n",
      "           ChannelWrite<__Node_A__:inbox> on_chain_end ['seq:step:2', 'langsmith:hidden']\n",
      "           __start__:edges on_chain_end ['graph:step:1', 'langsmith:hidden']\n",
      "           __Node_A__ on_chain_start ['graph:step:2']\n",
      "           RunnableLambda on_chain_start ['seq:step:1']\n",
      "           RunnableLambda on_chain_stream ['seq:step:1']\n",
      "           nodeA on_chain_start ['seq:step:2']\n",
      "           RunnableLambda on_chain_end ['seq:step:1']\n",
      "           nodeA on_chain_stream ['seq:step:2']\n",
      "           ChannelWrite<__Node_A__,messages> on_chain_start ['seq:step:3']\n",
      "           ChannelWrite<__Node_A__,messages> on_chain_stream ['seq:step:3']\n",
      "           __Node_A__ on_chain_stream ['graph:step:2']\n",
      "           nodeA on_chain_end ['seq:step:2']\n",
      "           ChannelWrite<__Node_A__,messages> on_chain_end ['seq:step:3']\n",
      "           RunnableLambda on_chain_start ['seq:step:3']\n",
      "           RunnableLambda on_chain_end ['seq:step:3']\n",
      "           __Node_A__ on_chain_end ['graph:step:2']\n",
      "           LangGraph on_chain_stream []\n",
      "           __Node_A__:edges on_chain_start ['graph:step:3', 'langsmith:hidden']\n",
      "           ChannelRead<['messages']> on_chain_start ['seq:step:1', 'langsmith:hidden']\n",
      "           ChannelRead<['messages']> on_chain_stream ['seq:step:1', 'langsmith:hidden']\n",
      "           ChannelWrite<__Node_B__:inbox> on_chain_start ['seq:step:2', 'langsmith:hidden']\n",
      "           ChannelWrite<__Node_B__:inbox> on_chain_stream ['seq:step:2', 'langsmith:hidden']\n",
      "           __Node_A__:edges on_chain_stream ['graph:step:3', 'langsmith:hidden']\n",
      "           ChannelRead<['messages']> on_chain_end ['seq:step:1', 'langsmith:hidden']\n",
      "           ChannelWrite<__Node_B__:inbox> on_chain_end ['seq:step:2', 'langsmith:hidden']\n",
      "           __Node_A__:edges on_chain_end ['graph:step:3', 'langsmith:hidden']\n",
      "           __Node_B__ on_chain_start ['graph:step:4']\n",
      "           RunnableLambda on_chain_start ['seq:step:1']\n",
      "           RunnableLambda on_chain_stream ['seq:step:1']\n",
      "           nodeB on_chain_start ['seq:step:2']\n",
      "           RunnableLambda on_chain_end ['seq:step:1']\n",
      "           nodeB on_chain_stream ['seq:step:2']\n",
      "           ChannelWrite<__Node_B__,messages> on_chain_start ['seq:step:3']\n",
      "           ChannelWrite<__Node_B__,messages> on_chain_stream ['seq:step:3']\n",
      "           __Node_B__ on_chain_stream ['graph:step:4']\n",
      ">>>>>>>>>> {'messages': ['你好，我是一个模拟的大语言模型']}\n",
      "           nodeB on_chain_end ['seq:step:2']\n",
      "           ChannelWrite<__Node_B__,messages> on_chain_end ['seq:step:3']\n",
      "           RunnableLambda on_chain_start ['seq:step:3']\n",
      "           RunnableLambda on_chain_end ['seq:step:3']\n",
      "           __Node_B__ on_chain_end ['graph:step:4']\n",
      "           LangGraph on_chain_stream []\n",
      "           __Node_B__:edges on_chain_start ['graph:step:5', 'langsmith:hidden']\n",
      "           ChannelRead<['messages']> on_chain_start ['seq:step:1', 'langsmith:hidden']\n",
      "           ChannelRead<['messages']> on_chain_stream ['seq:step:1', 'langsmith:hidden']\n",
      "           ChannelWrite<__end__> on_chain_start ['seq:step:2', 'langsmith:hidden']\n",
      "           ChannelWrite<__end__> on_chain_stream ['seq:step:2', 'langsmith:hidden']\n",
      "           __Node_B__:edges on_chain_stream ['graph:step:5', 'langsmith:hidden']\n",
      "           ChannelRead<['messages']> on_chain_end ['seq:step:1', 'langsmith:hidden']\n",
      "           ChannelWrite<__end__> on_chain_end ['seq:step:2', 'langsmith:hidden']\n",
      "           __Node_B__:edges on_chain_end ['graph:step:5', 'langsmith:hidden']\n",
      "           LangGraph on_chain_stream []\n",
      "           LangGraph on_chain_end []\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import StateGraph, MessageGraph, END\n",
    "import operator\n",
    "\n",
    "# 定义状态\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[str], operator.add]\n",
    "\n",
    "# 定义图\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# 定义节点\n",
    "def nodeA(state):\n",
    "    return {\"messages\": [\"AAA\"]}\n",
    "\n",
    "workflow.add_node(\"__Node_A__\", nodeA)\n",
    "\n",
    "# 定义节点\n",
    "async def nodeB(state):\n",
    "    messages = state[\"messages\"]\n",
    "    response = await llm.ainvoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "workflow.add_node(\"__Node_B__\", nodeB)\n",
    "\n",
    "# 定义边\n",
    "workflow.add_edge(\"__Node_A__\", \"__Node_B__\")\n",
    "workflow.add_edge(\"__Node_B__\", END)\n",
    "workflow.set_entry_point(\"__Node_A__\")\n",
    "\n",
    "# \n",
    "app = workflow.compile()\n",
    "\n",
    "# RUN\n",
    "# app.invoke({\"messages\": []})\n",
    "async for chunk in app.astream_events({\"messages\": []}, version=\"v1\"):\n",
    "    print(\" \"*10, chunk['name'], chunk['event'], chunk['tags'])\n",
    "    if(chunk['event']==\"on_chain_stream\" and chunk['name']==\"__Node_B__\"):        \n",
    "        print(\">\"*10, chunk['data']['chunk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e78746-1fbd-48db-87c8-8aa1f5a5f46d",
   "metadata": {},
   "source": [
    "# Function-Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e092981-69ed-4be3-aa8f-4eebdf33ba39",
   "metadata": {},
   "source": [
    "## 准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05896c2-7c07-4939-8e37-cbbe261afd1a",
   "metadata": {},
   "source": [
    "## 简单的例子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c606924-072e-498b-b79b-9f4ebda693a3",
   "metadata": {},
   "source": [
    "### 定义图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df822818-837d-452f-9f1e-39fa96d06b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d14246b0-cfe9-463a-85a2-064321aa20c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义状态\n",
    "workflow = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30538341-09d3-4446-bbad-0df1d352fb3a",
   "metadata": {},
   "source": [
    "### 定义工具集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45421a16-0e49-4393-a5fb-f4d8b577e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tools = [TavilySearchResults(max_results=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed99125-a70f-4aea-9370-f0aa9f1b0c04",
   "metadata": {},
   "source": [
    "### 定义 agent 节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b973407-4b79-4ffd-a25d-a3609bf4c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a63bc70-60bb-4022-bc4f-5cef5707e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "functions = [convert_to_openai_function(t) for t in tools]\n",
    "model = model.bind(functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e00437ce-a18b-4288-9fe9-619dc9ecc471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6805e33-419e-41d1-9aff-ed70f0eab164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 agent 节点\n",
    "workflow.add_node(\"agent\", call_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa07fca-5dd1-47c6-8705-8d6f8c46f5cc",
   "metadata": {},
   "source": [
    "### 定义 action 节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45c9103a-42d9-4cfe-9ea5-b42588020e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import FunctionMessage\n",
    "from langgraph.prebuilt import ToolInvocation\n",
    "import json\n",
    "\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "def call_tool(state):\n",
    "    messages = state['messages']\n",
    "    # Based on the continue condition\n",
    "    # we know the last message involves a function call\n",
    "    last_message = messages[-1]\n",
    "    # We construct an ToolInvocation from the function_call\n",
    "    action = ToolInvocation(\n",
    "        tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
    "        tool_input=json.loads(last_message.additional_kwargs[\"function_call\"][\"arguments\"]),\n",
    "    )\n",
    "    # We call the tool_executor and get back a response\n",
    "    response = tool_executor.invoke(action)\n",
    "    # We use the response to create a FunctionMessage\n",
    "    function_message = FunctionMessage(content=str(response), name=action.tool)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [function_message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4e90b4c-8743-4e8c-b4ce-b7513749cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 action 节点\n",
    "workflow.add_node(\"action\", call_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3217f6af-c263-43cd-b22f-2983765bb928",
   "metadata": {},
   "source": [
    "### 定义条件边"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f9f84df-bb61-46c3-bd5d-8269cd467b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "\n",
    "def should_continue(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if \"function_call\" not in last_message.additional_kwargs:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "# 增加边\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\", # from节点\n",
    "    should_continue, # 条件\n",
    "    {\n",
    "        # to节点：回到 action\n",
    "        \"continue\": \"action\",\n",
    "        # to节点：结束\n",
    "        \"end\": END\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4887378a-6915-4b71-a521-1a96bbc86909",
   "metadata": {},
   "source": [
    "### 定义直连边"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "766e4a3d-6e25-4750-9b83-fdd3f8d5c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_edge('action', 'agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea32a23e-032d-414e-839d-e5953f5bfb21",
   "metadata": {},
   "source": [
    "### 设定执行入口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f80f55b1-b726-433f-8a31-b6de7e53bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.set_entry_point(\"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4376d7f7-f941-404a-93bf-5b85b917ae7c",
   "metadata": {},
   "source": [
    "### 编译 graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1952e9f-646b-4759-b8f5-617cb46350fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e0849b-abc2-43ac-83b5-fd0633f2b631",
   "metadata": {},
   "source": [
    "### 使用它！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4a2e81d-157b-48de-a7b9-1e27a8cf28cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the weather in sf?'),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"query\": \"weather in San Francisco\"\\n}', 'name': 'tavily_search_results_json'}}),\n",
       "  FunctionMessage(content=\"[{'url': 'https://www.whereandwhen.net/when/north-america/california/san-francisco-ca/february/', 'content': 'Best time to go to San Francisco? Weather in San Francisco in february 2024  How was the weather last february? Here is the day by day recorded weather in San Francisco in february 2023:  Seasonal average climate and temperature of San Francisco in february  The climate of San Francisco in february is goodSan Francisco at the Beginning of February. Temperatures ranging from 9° C to 14° C translate into very mild mornings and pleasant days. However, chilly people\\\\xa0...'}]\", name='tavily_search_results_json'),\n",
       "  AIMessage(content=\"I'm sorry, but I couldn't find the current weather in San Francisco. However, in February, the weather in San Francisco is generally mild with temperatures ranging from 9°C to 14°C.\")]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf?\")]}\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732fc73f-16b4-41c6-8ecd-96f27035c68c",
   "metadata": {},
   "source": [
    "# langgraph示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0438a124-02c3-40ce-9507-463b7fccc251",
   "metadata": {},
   "source": [
    "## 定义图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65a3f653-5175-47d2-82f1-f31024ec6d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "\n",
    "# 定义状态\n",
    "workflow = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0714ae8-ef53-4688-b1dc-d93684769a11",
   "metadata": {},
   "source": [
    "## 定义开始节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96a1b1d3-1966-4950-9ce2-2a48ff94a314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0, streaming=True)\n",
    "\n",
    "def speak_joke(state):\n",
    "    messages = state['messages']\n",
    "    response = model.stream('讲一个30字的笑话')\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    for chunk in response:\n",
    "        yield {\"messages\": [chunk]}\n",
    "\n",
    "workflow.add_node(\"speak_joke\", speak_joke)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85037d9c-85b4-4266-a0e6-e1a67cc26ad0",
   "metadata": {},
   "source": [
    "## 定义结束节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1de1d5f8-c42f-461a-bcd3-9e060640183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listen_joke(state):\n",
    "    messages = state['messages']\n",
    "    for m in messages:\n",
    "        print(m)\n",
    "    return {\"messages\": \"hahaha\"}\n",
    "\n",
    "workflow.add_node(\"listen_joke\", listen_joke)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffba849b-8281-4d90-81d9-2ffaf60cc4a5",
   "metadata": {},
   "source": [
    "## 定义边"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e623b90f-5cbf-4b31-b657-4d61c5a015d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "\n",
    "workflow.add_edge('speak_joke', 'listen_joke')\n",
    "workflow.add_edge('listen_joke', END)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f7e14-b61e-476d-a5ff-c23a81f13a3e",
   "metadata": {},
   "source": [
    "## 编译并使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46893b5f-8416-44a3-88ed-46b415afd6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.set_entry_point(\"speak_joke\")\n",
    "app = workflow.compile()\n",
    "for x in app.stream({}):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44034dc-6260-449d-a8fd-f87e18771e18",
   "metadata": {},
   "source": [
    "# chat_agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c424d83-a877-4181-aa3e-4a822e8aa605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.prebuilt import chat_agent_executor\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "tools = [TavilySearchResults(max_results=1)]\n",
    "model = ChatOpenAI(streaming=True)\n",
    "\n",
    "app = chat_agent_executor.create_tool_calling_executor(model, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94f595b6-c377-428f-b651-09a672e04765",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"霍金的生日是哪一天？\")]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c2e45f-6c34-48ef-b466-2fb12b1feae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in app.stream(inputs):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44deb6bd-5e98-4483-baa4-4a83d98d4e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for output in app.astream_log(inputs, include_types=[\"llm\"]):\n",
    "    # astream_log() yields the requested logs (here LLMs) in JSONPatch format\n",
    "    for op in output.ops:\n",
    "        if op[\"path\"] == \"/streamed_output/-\":\n",
    "            # this is the output from .stream()\n",
    "            ...\n",
    "            print(op[\"value\"])\n",
    "        elif op[\"path\"].startswith(\"/logs/\") and op[\"path\"].endswith(\n",
    "            \"/streamed_output/-\"\n",
    "        ):\n",
    "            # because we chose to only include LLMs, these are LLM tokens\n",
    "            print(op[\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28b3d83-1814-4bcc-91bb-8ee918cdb9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for chunk in app.astream_events(inputs, version=\"v1\"):\n",
    "    print(\"\\n\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"name: {chunk['name']}\")\n",
    "    print(f\"tags: {chunk['tags']}\")\n",
    "    print(f\"event: {chunk['event']}\")\n",
    "    if(chunk['event']==\"on_chain_stream\"):\n",
    "        m = chunk['data']['chunk']\n",
    "        if \"messages\" in m:\n",
    "            for x in m[\"messages\"]:\n",
    "                print(x.content, end=\"|\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221c4aa1-b4c2-4b13-8fd0-033fd6b030a2",
   "metadata": {},
   "source": [
    "# 从智能体中提取对话信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad7bfc8-71ca-4d2d-9c3f-af377ebc4eb6",
   "metadata": {},
   "source": [
    "您可以通过定义一个“记忆器”（通常可以是一个队列或者其他类型的数据结构，用于临时存储数据）来实现这个功能。这种方法允许您在服务器端持续将数据写入到这个记忆器中，同时客户端可以异步地读取这些数据，直到所有流式结果都被处理完毕。这种模式在处理实时数据流、日志监控或者其他需要实时数据更新的场景中非常有用。\n",
    "\n",
    "以下是实现这一功能的大致步骤和示例代码：\n",
    "\n",
    "步骤 1：定义记忆器\n",
    "记忆器可以是一个简单的队列（如 Python 的 queue.Queue），用于在生产者（数据生成函数）和消费者（客户端请求处理函数）之间传递消息。\n",
    "\n",
    "步骤 2：数据生成函数循环写入记忆器\n",
    "您的数据生成函数（或者其他逻辑）会在循环中生成数据，并将这些数据写入到记忆器中。\n",
    "\n",
    "步骤 3：客户端异步读取记忆器\n",
    "通过一个特定的 API 端点，客户端可以开始读取记忆器中的数据。服务器端可以使用异步处理来实时发送记忆器中的数据给客户端，直到所有数据都被发送。\n",
    "\n",
    "示例代码\n",
    "```python\n",
    "from fastapi import FastAPI, WebSocket\n",
    "import asyncio\n",
    "from typing import List\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# 定义一个简单的记忆器，这里使用列表模拟\n",
    "memory = []\n",
    "\n",
    "# 数据生成函数，模拟数据产生并写入记忆器\n",
    "async def generate_data():\n",
    "    for i in range(10):\n",
    "        await asyncio.sleep(1)  # 模拟异步操作\n",
    "        memory.append(f\"Data {i}\")  # 将数据写入记忆器\n",
    "\n",
    "# WebSocket路由，客户端通过WebSocket连接读取数据\n",
    "@app.websocket(\"/ws\")\n",
    "async def websocket_endpoint(websocket: WebSocket):\n",
    "    await websocket.accept()\n",
    "    # 启动数据生成任务，但不等待它完成\n",
    "    task = asyncio.create_task(generate_data())\n",
    "    while not task.done() or memory:  # 检查任务是否完成或记忆器中是否有数据\n",
    "        if memory:  # 如果记忆器中有数据，则发送给客户端\n",
    "            data_to_send = memory.pop(0)  # 获取记忆器中的第一个数据并发送\n",
    "            await websocket.send_text(data_to_send)\n",
    "        else:\n",
    "            await asyncio.sleep(0.1)  # 等待更多数据生成\n",
    "    await websocket.close()\n",
    "```\n",
    "在这个示例中，generate_data 函数模拟了数据生成过程，将数据逐个写入全局变量 memory 中。通过 WebSocket 连接，客户端可以实时地接收这些数据。服务器在有数据可发送时通过 WebSocket 向客户端发送数据，直到数据生成任务完成并且记忆器中的所有数据都被发送。\n",
    "\n",
    "请注意，这里使用全局变量作为记忆器仅用于示"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchani-book-py3.10-ipykernel",
   "language": "python",
   "name": "langchani-book-py3.10-ipykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
