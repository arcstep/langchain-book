{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0975d3fa-f0fd-4ba4-ac8a-155b06d56666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663f9e50-910d-4760-b6e7-142bd1e4b5fd",
   "metadata": {},
   "source": [
    "# Set up the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a4e4dd8-8115-409a-8a0a-f97a4b914941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tools = [TavilySearchResults(max_results=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "063f6972-b269-481c-867e-2344e567376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolExecutor\n",
    "\n",
    "tool_executor = ToolExecutor(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a05fc1c-dd8f-42a8-8231-92df373f751a",
   "metadata": {},
   "source": [
    "# Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ae6ffd8-6765-4544-9ff5-07313d38a4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# We will set streaming=True so that we can stream tokens\n",
    "# See the streaming section for more information on this.\n",
    "model = ChatOpenAI(temperature=0, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27bb63e1-8f42-4e33-8b03-23f0b58c5e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "tools = [convert_to_openai_tool(t) for t in tools]\n",
    "model = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c29315f-40b7-465f-a269-1dc681c3fccf",
   "metadata": {},
   "source": [
    "# Define the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f7dbbb0-072b-4209-85d4-eb55e8aa86fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolInvocation\n",
    "import json\n",
    "from langchain_core.messages import FunctionMessage\n",
    "\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(messages):\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if \"function_call\" not in last_message.additional_kwargs:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "async def call_model(messages):\n",
    "    response = await model.ainvoke(messages)\n",
    "    return response\n",
    "\n",
    "\n",
    "# Define the function to execute tools\n",
    "async def call_tool(messages):\n",
    "    # Based on the continue condition\n",
    "    # we know the last message involves a function call\n",
    "    last_message = messages[-1]\n",
    "    # We construct an ToolInvocation from the function_call\n",
    "    action = ToolInvocation(\n",
    "        tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
    "        tool_input=json.loads(\n",
    "            last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "        ),\n",
    "    )\n",
    "    # We call the tool_executor and get back a response\n",
    "    response = await tool_executor.ainvoke(action)\n",
    "    # We use the response to create a FunctionMessage\n",
    "    function_message = FunctionMessage(content=str(response), name=action.tool)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return function_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebb827a-4cd9-464b-bea6-d451f65953c0",
   "metadata": {},
   "source": [
    "# Define the edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe327610-28ef-466d-aecf-24dd20dbbfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessageGraph, END\n",
    "\n",
    "# Define a new graph\n",
    "workflow = MessageGraph()\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", call_tool)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    "    # Finally we pass in a mapping.\n",
    "    # The keys are strings, and the values are other nodes.\n",
    "    # END is a special node marking that the graph should finish.\n",
    "    # What will happen is we will call `should_continue`, and then the output of that\n",
    "    # will be matched against the keys in this mapping.\n",
    "    # Based on which one it matches, that node will then be called.\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"continue\": \"action\",\n",
    "        # Otherwise we finish.\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f10be91-b41a-4954-a68f-a1398b82cc1b",
   "metadata": {},
   "source": [
    "# Streaming LLM Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3fa7338-93f5-4be6-8dd7-9d854a0513b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_chain_start\n",
      "on_chain_start\n",
      "on_chain_stream\n",
      "{'chunk': [HumanMessage(content='马斯克投资了哪些公司？')]}\n",
      "on_chain_end\n",
      "on_chain_start\n",
      "on_chain_start\n",
      "on_chain_stream\n",
      "{'chunk': [HumanMessage(content='马斯克投资了哪些公司？')]}\n",
      "on_chain_start\n",
      "on_chain_stream\n",
      "{'chunk': [HumanMessage(content='马斯克投资了哪些公司？')]}\n",
      "on_chain_stream\n",
      "{'chunk': [HumanMessage(content='马斯克投资了哪些公司？')]}\n",
      "on_chain_end\n",
      "on_chain_end\n",
      "on_chain_end\n",
      "on_chain_start\n",
      "on_chain_start\n",
      "on_chain_stream\n",
      "{'chunk': [HumanMessage(content='马斯克投资了哪些公司？')]}\n",
      "on_chain_start\n",
      "on_chain_end\n",
      "on_chain_stream\n",
      "{'chunk': AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_VoVPfxqE8SSp4ib4vxRkds6D', 'function': {'arguments': '{\"query\":\"Elon Musk investments\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]})}\n",
      "on_chain_start\n",
      "on_chain_stream\n",
      "{'chunk': AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_VoVPfxqE8SSp4ib4vxRkds6D', 'function': {'arguments': '{\"query\":\"Elon Musk investments\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]})}\n",
      "on_chain_stream\n",
      "{'chunk': AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_VoVPfxqE8SSp4ib4vxRkds6D', 'function': {'arguments': '{\"query\":\"Elon Musk investments\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]})}\n",
      "on_chain_end\n",
      "on_chain_end\n",
      "on_chain_end\n",
      "on_chain_stream\n",
      "{'chunk': {'agent': AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_VoVPfxqE8SSp4ib4vxRkds6D', 'function': {'arguments': '{\"query\":\"Elon Musk investments\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]})}}\n",
      "on_chain_start\n",
      "on_chain_start\n",
      "on_chain_stream\n",
      "{'chunk': [HumanMessage(content='马斯克投资了哪些公司？'), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_VoVPfxqE8SSp4ib4vxRkds6D', 'function': {'arguments': '{\"query\":\"Elon Musk investments\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]})]}\n",
      "on_chain_start\n",
      "on_chain_end\n",
      "on_chain_start\n",
      "on_chain_stream\n",
      "{'chunk': [HumanMessage(content='马斯克投资了哪些公司？'), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_VoVPfxqE8SSp4ib4vxRkds6D', 'function': {'arguments': '{\"query\":\"Elon Musk investments\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]})]}\n",
      "on_chain_stream\n",
      "{'chunk': [HumanMessage(content='马斯克投资了哪些公司？'), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_VoVPfxqE8SSp4ib4vxRkds6D', 'function': {'arguments': '{\"query\":\"Elon Musk investments\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]})]}\n",
      "on_chain_stream\n",
      "{'chunk': [HumanMessage(content='马斯克投资了哪些公司？'), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_VoVPfxqE8SSp4ib4vxRkds6D', 'function': {'arguments': '{\"query\":\"Elon Musk investments\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]})]}\n",
      "on_chain_end\n",
      "on_chain_end\n",
      "on_chain_end\n",
      "on_chain_stream\n",
      "{'chunk': {'__end__': [HumanMessage(content='马斯克投资了哪些公司？'), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_VoVPfxqE8SSp4ib4vxRkds6D', 'function': {'arguments': '{\"query\":\"Elon Musk investments\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]})]}}\n",
      "on_chain_end\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = [HumanMessage(content=\"马斯克投资了哪些公司？\")]\n",
    "async for event in app.astream_events(inputs, version=\"v1\"):\n",
    "    kind = event[\"event\"]\n",
    "    print(kind)\n",
    "    if kind == \"on_chain_stream\":\n",
    "        print(event[\"data\"])\n",
    "        chunk = event[\"data\"][\"chunk\"]\n",
    "        for x in chunk:\n",
    "            if \"content\" in chunk:\n",
    "                print(chunk['content'], end=\"|\", flush=True)\n",
    "    elif kind == \"on_tool_start\":\n",
    "        print(\"--\")\n",
    "        print(\n",
    "            f\"Starting tool: {event['name']} with inputs: {event['data'].get('input')}\"\n",
    "        )\n",
    "    elif kind == \"on_tool_end\":\n",
    "        print(f\"Done tool: {event['name']}\")\n",
    "        print(f\"Tool output was: {event['data'].get('output')}\")\n",
    "        print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f12383-9861-4bf0-931e-8aff2c867578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict, Union\n",
    "\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage, ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\n",
    "\n",
    "\n",
    "def create_tool_calling_executor(\n",
    "    model: LanguageModelLike, tools: Union[ToolExecutor, Sequence[BaseTool]]\n",
    "):\n",
    "    if isinstance(tools, ToolExecutor):\n",
    "        tool_executor = tools\n",
    "        tool_classes = tools.tools\n",
    "    else:\n",
    "        tool_executor = ToolExecutor(tools)\n",
    "        tool_classes = tools\n",
    "    model = model.bind(tools=[convert_to_openai_tool(t) for t in tool_classes])\n",
    "\n",
    "    # We create the AgentState that we will pass around\n",
    "    # This simply involves a list of messages\n",
    "    # We want steps to return messages to append to the list\n",
    "    # So we annotate the messages attribute with operator.add\n",
    "    class AgentState(TypedDict):\n",
    "        messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "\n",
    "    # Define the function that determines whether to continue or not\n",
    "    def should_continue(state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If there is no function call, then we finish\n",
    "        if \"tool_calls\" not in last_message.additional_kwargs:\n",
    "            return \"end\"\n",
    "        # Otherwise if there is, we continue\n",
    "        else:\n",
    "            return \"continue\"\n",
    "\n",
    "    # Define the function that calls the model\n",
    "    def call_model(state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        response = model.invoke(messages)\n",
    "        # We return a list, because this will get added to the existing list\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    async def acall_model(state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        response = await model.ainvoke(messages)\n",
    "        # We return a list, because this will get added to the existing list\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    # Define the function to execute tools\n",
    "    def _get_actions(state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        # Based on the continue condition\n",
    "        # we know the last message involves a tool call\n",
    "        last_message = messages[-1]\n",
    "        # We construct an AgentAction from each of the tool_calls\n",
    "        return (\n",
    "            [\n",
    "                ToolInvocation(\n",
    "                    tool=tool_call[\"function\"][\"name\"],\n",
    "                    tool_input=json.loads(tool_call[\"function\"][\"arguments\"]),\n",
    "                )\n",
    "                for tool_call in last_message.additional_kwargs[\"tool_calls\"]\n",
    "            ],\n",
    "            [\n",
    "                tool_call[\"id\"]\n",
    "                for tool_call in last_message.additional_kwargs[\"tool_calls\"]\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def call_tool(state: AgentState):\n",
    "        actions, ids = _get_actions(state)\n",
    "        # We call the tool_executor and get back a response\n",
    "        responses = tool_executor.batch(actions)\n",
    "        # We use the response to create a FunctionMessage\n",
    "        tool_messages = [\n",
    "            ToolMessage(content=str(response), tool_call_id=id)\n",
    "            for response, id in zip(responses, ids)\n",
    "        ]\n",
    "        # We return a list, because this will get added to the existing list\n",
    "        return {\"messages\": tool_messages}\n",
    "\n",
    "    async def acall_tool(state: AgentState):\n",
    "        actions, ids = _get_actions(state)\n",
    "        # We call the tool_executor and get back a response\n",
    "        responses = await tool_executor.abatch(actions)\n",
    "        # We use the response to create a FunctionMessage\n",
    "        tool_messages = [\n",
    "            ToolMessage(content=str(response), tool_call_id=id)\n",
    "            for response, id in zip(responses, ids)\n",
    "        ]\n",
    "        # We return a list, because this will get added to the existing list\n",
    "        return {\"messages\": tool_messages}\n",
    "\n",
    "    # Define a new graph\n",
    "    workflow = StateGraph(AgentState)\n",
    "\n",
    "    # Define the two nodes we will cycle between\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model, acall_model))\n",
    "    workflow.add_node(\"action\", RunnableLambda(call_tool, acall_tool))\n",
    "\n",
    "    # Set the entrypoint as `agent`\n",
    "    # This means that this node is the first one called\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "\n",
    "    # We now add a conditional edge\n",
    "    workflow.add_conditional_edges(\n",
    "        # First, we define the start node. We use `agent`.\n",
    "        # This means these are the edges taken after the `agent` node is called.\n",
    "        \"agent\",\n",
    "        # Next, we pass in the function that will determine which node is called next.\n",
    "        should_continue,\n",
    "        # Finally we pass in a mapping.\n",
    "        # The keys are strings, and the values are other nodes.\n",
    "        # END is a special node marking that the graph should finish.\n",
    "        # What will happen is we will call `should_continue`, and then the output of that\n",
    "        # will be matched against the keys in this mapping.\n",
    "        # Based on which one it matches, that node will then be called.\n",
    "        {\n",
    "            # If `tools`, then we call the tool node.\n",
    "            \"continue\": \"action\",\n",
    "            # Otherwise we finish.\n",
    "            \"end\": END,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # We now add a normal edge from `tools` to `agent`.\n",
    "    # This means that after `tools` is called, `agent` node is called next.\n",
    "    workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "    # Finally, we compile it!\n",
    "    # This compiles it into a LangChain Runnable,\n",
    "    # meaning you can use it as you would any other runnable\n",
    "    return workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3af0c7-be19-4f69-9dc8-f746e9bd6b92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchani-book-py3.10-ipykernel",
   "language": "python",
   "name": "langchani-book-py3.10-ipykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
