{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e5b5362-cf90-4d8a-9c70-3db56e3f8dcc",
   "metadata": {},
   "source": [
    "# ğŸ¦œğŸ”— è§£è¯»LangChainæ ¸å¿ƒæºç ï¼šå…³äºLLMå’ŒAgentï¼ˆä¸Šï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09652ad-45cf-4152-8794-2639a92a4ac2",
   "metadata": {},
   "source": [
    "# è¯¾ç¨‹å¼€å§‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bac26c1-1878-43d9-984d-bebacabccefb",
   "metadata": {},
   "source": [
    "## è¿™èŠ‚è¯¾ä¼šå¸¦ç»™ä½ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eb7b7a-d4a8-4e8d-b757-a55062c3fad8",
   "metadata": {},
   "source": [
    "- ğŸŒ¹ ä¸€èµ·é˜…è¯» Langchain ç›¸å…³ç»„ä»¶çš„æºç ï¼Œä»¥ä¾¿æ›´å¥½åœ°é˜…è¯»å®˜æ–¹æ–‡æ¡£\n",
    "- ğŸŒ¹ æŒæ¡ Langchain æ–‡æ¡£ä¸­æœªæ›¾æåŠã€ç¿»çœ‹æºç æ‰çŸ¥æ™“çš„å®ç”¨æŠ€å·§\n",
    "- âœï¸ ä»é›¶å¼€å§‹é›†æˆæ™ºè°±å¤§æ¨¡å‹åˆ° LangChainï¼šè§£å†³æ™ºè°±å®˜æ–¹SDKä¸å…¼å®¹çš„é—®é¢˜\n",
    "- âœï¸ æŒ‰ LangChain æ¡†æ¶è‡ªå®šä¹‰ä¸€ä¸ªæ™ºèƒ½ä½“ï¼šå†ç°ã€Šæ‰‹æ’•AutoGPTã€‹ä¸­çš„æ™ºèƒ½ä½“\n",
    "- âœï¸ æœ€ç»ˆå®è·µï¼šå†ç°ã€Šæ‰‹æ’•AutoGPTã€‹ï¼šlangchain+è‡ªå®šä¹‰å¤§æ¨¡å‹+è‡ªå®šä¹‰æ™ºèƒ½ä½“"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff511b-10e1-43d4-92e0-829a1fe4d5d2",
   "metadata": {},
   "source": [
    "## å¼€åœºé—²èŠ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d9a3a8-46ff-4aa2-92b8-8a333ab5f00c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### â¤ï¸ æˆ‘å¯¹ langchain çš„ä¸ªäººè§‚ç‚¹\n",
    "\n",
    "1. langchain ä¸€å®šæ˜¯æœªæ¥è¡Œä¸šæ ‡å‡†\n",
    "    - langchain çš„æ¶æ„è§£è€¦éå¸¸æœ‰æ•ˆï¼Œåœ¨é¡¹ç›®ç®¡ç†ä¸­ä»·å€¼ä¸å¯ä¼°é‡\n",
    "    - éšç€æ¨¡å‹ç«äº‰ã€æ¨¡å‹è‡ªè®­æ™®åŠã€å¤šæ¨¡æ€æ¨¡å‹çš„å‘å±•ï¼Œå¿…é¡»å€ŸåŠ©å·²æœ‰ç”Ÿæ€ï¼Œlangchainåœ°ä½ä¼šè¶Šæ¥è¶Šå·©å›º\n",
    "2. langchain å·²ç»å¯ç”¨ï¼Œä½†æœ‰ä½¿ç”¨é—¨æ§›ï¼ˆä»¥ä¸‹æƒ…å†µä¼šæŒç»­å¾ˆä¹…ï¼‰\n",
    "    - ğŸŒ¹ å› ä¸ºç”Ÿæ€å‘å±•å¿«ï¼Œæ‰€ä»¥**æ–‡æ¡£è·Ÿä¸ä¸Šï¼Œéœ€è¦çœ‹æºç **\n",
    "    - âœï¸ å¾ˆå¤šç»„ä»¶ä»…èƒ½ä½œä¸ºç¤ºèŒƒï¼Œæ‰€ä»¥**æ— æ³•æ»¡è¶³è½åœ°éœ€æ±‚ï¼Œéœ€è¦è‡ªå®šä¹‰ langchain ç»„ä»¶**\n",
    "  \n",
    "**æ‰€ä»¥ï¼Œä»Šå¤©çš„åˆ†äº«ï¼š**\n",
    "- ä¸ŠåŠåœºï¼šä¸€èµ·çœ‹æºç ï¼Œé›†æˆè‡ªå·±çš„å¤§æ¨¡å‹åˆ° Langchain\n",
    "- ä¸‹åŠåœºï¼šä¸€èµ·çœ‹æºç ï¼Œè‡ªå®šä¹‰æ™ºèƒ½ä½“ï¼Œå†ç°ã€Šæ‰‹æ’•AutoGPTã€‹çš„æ¨ç†è¿‡ç¨‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554524d7-f9ce-4b70-be62-94ff88169a6e",
   "metadata": {},
   "source": [
    "### â¤ï¸ Langchain èµ„æº\n",
    "- [langchainæºä»£ç ](https://github.com/langchain-ai/)ï¼šæ·±åº¦æŒæ¡å¿…å¤‡çš„æ ¸å¿ƒèµ„æº\n",
    "- [langchainå®˜æ–¹æ–‡æ¡£](https://python.langchain.com/docs)ï¼šæ–‡æ¡£è¶Šæ¥è¶Šå¥½ï¼Œä¸æºä»£ç ç›¸äº’å°è¯\n",
    "- [langgraph example](https://github.com/langchain-ai/langgraph/tree/main/examples)ï¼šJupyter ä¾‹å­\n",
    "- [langchain AI](https://chat.langchain.com/?llm=anthropic_claude_2_1)ï¼šå…è´¹RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f539f3d-148f-4c18-a959-881dfc8ce4b7",
   "metadata": {},
   "source": [
    "### â¤ï¸ Langchain æºç ç»“æ„\n",
    "\n",
    "![](./langchain_ai.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab7f60d-7043-42e0-98c2-459355c52f19",
   "metadata": {},
   "source": [
    "### â¤ï¸ LangChain æ¨¡å—æºç æ¦‚è§ˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4481a6-d92d-4df6-8cfd-65e57c2463f5",
   "metadata": {},
   "source": [
    "[æ ¸å¿ƒæºç èµ„æº](https://github.com/langchain-ai/langchain/tree/master/libs)\n",
    "\n",
    "| æºç ä½ç½® | åŠŸèƒ½æè¿° |\n",
    "| :--- | :--- |\n",
    "| langchain/libs/langchain/langchain | æ¨¡å—å…¥å£ï¼Œä¼šå¯¼å…¥coreã€communityç­‰å…¶ä»–æ¨¡å— |\n",
    "| langchain/libs/core/langchain_core | æ ¸å¿ƒç»„ä»¶å’Œå…³é”®çš„åŸºç±»å®ç° |\n",
    "| langchain/libs/partners/openai/langchain_openai | åˆä½œä¼™ä¼´ï¼ˆå®˜æ–¹åˆä½œï¼‰ç»„ä»¶ |\n",
    "| langchain/libs/community/langchain_community | ç¤¾åŒºï¼ˆéå®˜æ–¹ï¼‰ç»„ä»¶ |\n",
    "| langchain/libs/experimental/langchain_experimental | è¯•éªŒæ€§åŠŸèƒ½ï¼ˆå‰æ²¿æ¢ç´¢ç»„ä»¶ï¼Œä¸å¯¹ç‰ˆæœ¬ç¨³å®šåšæ‰¿è¯ºï¼‰ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eff717-2f78-43a7-87c0-9d41f329206a",
   "metadata": {},
   "source": [
    "### ğŸŒ¹ æ¨èé˜…è¯»ï¼šä» Langchain çš„æœ€æ ¸å¿ƒç»„ä»¶ Runnable å¼€å§‹é˜…è¯»æºç \n",
    "\n",
    "[langchain_core/runnables/base.py#L104](https://github.com/langchain-ai/langchain/blob/ad77fa15eec4dae431171b7e8c13b8c1f9edec98/libs/core/langchain_core/runnables/base.py#L104)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8601bed1-2ed8-4582-95d5-b25a3568ba29",
   "metadata": {},
   "source": [
    "**ï¼ˆ1ï¼‰Runnableå®ç”¨æ–¹æ³•ï¼š**\n",
    "\n",
    "- Runnable\n",
    "    - assign()\n",
    "    - bind()\n",
    "    - with_config()\n",
    "    - get_name()\n",
    "    - get_graph()\n",
    "    - get_prompts()\n",
    "    - input_schema\n",
    "    - output_schema\n",
    "\n",
    "----\n",
    "**ï¼ˆ2ï¼‰RunnableSerializableå®ç”¨æ–¹æ³•ï¼š**\n",
    "\n",
    "- Runnable\n",
    "    - RunnableSerializable\n",
    "        - dumps()\n",
    "        - loads()\n",
    "\n",
    "----\n",
    "**ï¼ˆ3ï¼‰é…ç½®èƒ½åŠ›å­ç±»ï¼š**\n",
    "\n",
    "- Runnable\n",
    "    - RunnableSerializable\n",
    "        - RunnableBindingBase\n",
    "            - RunnableBindingï¼ˆå‘Runnableå®ä¾‹ä¼ é€’å‚æ•°ï¼‰\n",
    "        - DynamicRunnable\n",
    "            - RunnableConfigurableFields\n",
    "            - RunnableConfigurableAlternatives\n",
    "\n",
    "----\n",
    "**ï¼ˆ4ï¼‰æµç¨‹æ§åˆ¶å­ç±»ï¼š**\n",
    "\n",
    "- Runnable\n",
    "    - RunnableSerializable\n",
    "        - RunnablePassthroughï¼ˆä¼ é€’é¢å¤–è¾“å…¥ï¼‰\n",
    "        - RunnableSequenceï¼ˆå®ç°é¡ºåºæ‰§è¡Œï¼Œå¯ä»¥ç”¨é‡è½½çš„`|`ç¬¦å·æˆ–`RunnableSequence`æ¥æ„é€ ï¼‰\n",
    "        - RunnableParallelï¼ˆå®ç°å¹¶è¡Œæ‰§è¡Œï¼Œå¯ä»¥ç”¨`Dict`æˆ–`RunnableParallel`ç±»æ¥æ„é€ ï¼Œåˆ«å`RunnableMap`ï¼‰\n",
    "\n",
    "----\n",
    "**ï¼ˆ5ï¼‰å¤§æ¨¡å‹å­ç±»ï¼š**\n",
    "\n",
    "- Runnable\n",
    "    - RunnableSerializable\n",
    "        - BaseLanguageModel\n",
    "            - BaseLLMï¼ˆæ´¾ç”Ÿå…¶ä»–å¤§æ¨¡å‹ï¼‰\n",
    "                - BaseOpenAIï¼ˆæ´¾ç”Ÿå…¶ä»–å¤§æ¨¡å‹ï¼‰\n",
    "                    - OpenAI\n",
    "                - LLM\n",
    "                    - ...\n",
    "                - ...\n",
    "            - BaseChatModel\n",
    "                - ChatOpenAI\n",
    "                - ...\n",
    "\n",
    "----\n",
    "**ï¼ˆ6ï¼‰æç¤ºè¯­å­ç±»ï¼š**\n",
    "- Runnable\n",
    "    - RunnableSerializable\n",
    "        - BasePromptTemplate `[Dict, PromptValue]`\n",
    "            - StringPromptTemplateï¼ˆå­—ç¬¦ä¸²æ¨¡æ¿ï¼‰\n",
    "            - BaseChatPromptTemplateï¼ˆå¯¹è¯æ¨¡æ¿ï¼‰\n",
    "            - ImagePromptTemplate\n",
    "            - PipelinePromptTemplate\n",
    "\n",
    "----\n",
    "**ï¼ˆ7ï¼‰æ£€ç´¢å™¨å­ç±»ï¼š**\n",
    "- Runnable\n",
    "    - RunnableSerializable\n",
    "        - BaseRetriever `[RetrieverInput, RetrieverOutput]`ï¼ˆæ´¾ç”Ÿå„ç±»æ£€ç´¢å™¨ï¼‰\n",
    "\n",
    "----\n",
    "**ï¼ˆ8ï¼‰Toolå­ç±»ï¼š**\n",
    "- Runnable\n",
    "    - RunnableSerializable\n",
    "        - BaseTool `[Union[str, Dict], Any]`ï¼ˆæ´¾ç”Ÿå„ç±»å·¥å…·ï¼‰\n",
    "\n",
    "----\n",
    "**ï¼ˆ9ï¼‰è¾“å‡ºè§£æå­ç±»ï¼š**\n",
    "- Runnable\n",
    "    - RunnableSerializable\n",
    "        - BaseGenerationOutputParser `[Union[str, BaseMessage], T]`\n",
    "        - BaseOutputParserï¼ˆæ´¾ç”Ÿå„ç±»è¾“å‡ºè§£æï¼‰\n",
    "\n",
    "----\n",
    "**ï¼ˆ10ï¼‰è¾“å…¥èµ‹å€¼å­ç±»ï¼š**\n",
    "- Runnable\n",
    "    - RunnableSerializable\n",
    "        - RunnablePassthrough\n",
    "        - RunnableAssign\n",
    "        - RunnablePick\n",
    "\n",
    "----\n",
    "**ï¼ˆ11ï¼‰é—ç•™Chainå­ç±»ï¼š**\n",
    "- Runnable\n",
    "    - RunnableSerializable\n",
    "        - Chainï¼ˆç»“æ„åŒ–Runnableï¼‰\n",
    "            - AgentExecutorï¼ˆæ‰§è¡Œæ™ºèƒ½ä½“ï¼‰\n",
    "\n",
    "----\n",
    "**ï¼ˆ12ï¼‰å¿«é€Ÿè‡ªå®šä¹‰Runnableçš„å·¥å…·å­ç±»ï¼š**\n",
    "- Runnableï¼ˆï¼‰\n",
    "    - RunnableGeneratorï¼ˆå¸¸ç”¨äºå¤„ç†è¾“å‡ºå¯èƒ½æ˜¯è¿­ä»£å™¨ç»“æœçš„chainï¼‰\n",
    "    - RunnableLambdaï¼ˆå¸¸ç”¨äºåŒ…è£…æ™®é€šå‡½æ•°ï¼Œè£…é¥°å‡½æ•°@chainï¼‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01548080-b79b-4a88-9b9f-7af2ef9ae567",
   "metadata": {},
   "source": [
    "### â¤ï¸ ä»æºç ä¸­ä½“ä¼šï¼šLangChain æ ¸å¿ƒæ¡†æ¶çš„ä¸‰è½®è¿­ä»£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc361bf-be03-4dbe-bf58-03f11e20fef8",
   "metadata": {},
   "source": [
    "- Runnable + Chain æ—¶ä»£\n",
    "- Runnable + LCEL æ—¶ä»£\n",
    "- Runnable + LCEL + Langgraph æ—¶ä»£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c33759-7be9-4a3a-be4b-7e42b83d3777",
   "metadata": {},
   "source": [
    "### ğŸŒ¹ æœ¬èŠ‚æ¶‰åŠçš„ LangChain æºç \n",
    "\n",
    "- ğŸŒ¹ Runnableï¼š[langchain_core/runnables/base.py#L104](https://github.com/langchain-ai/langchain/blob/ad77fa15eec4dae431171b7e8c13b8c1f9edec98/libs/core/langchain_core/runnables/base.py#L104)\n",
    "- ğŸŒ¹ BaseLanguageModelï¼š[langchain_core/language_models/base.py](https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/language_models/base.py#L74-L81)\n",
    "- ğŸŒ¹ BaseChatModelï¼š[langchain_core/language_models/chat_models.py](https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/language_models/chat_models.py#L100)\n",
    "- ğŸŒ¹ å¯¹è¯æ¶ˆæ¯è¯­æ³•ç³–ï¼š[langchain_core/messages/utils.py](https://github.com/langchain-ai/langchain/blob/c93d4ea91cfcf55dfe871931d42aa22562f8dae2/libs/core/langchain_core/messages/utils.py#L130-L168)\n",
    "- ğŸŒ¹ æ ¸å¿ƒæºç èµ„æºï¼š[github.com/langchain-ai](https://github.com/langchain-ai/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e936103d-a47c-4455-81ca-10a8cbb8449a",
   "metadata": {},
   "source": [
    "# ï¼ˆä¸€ï¼‰è§£è¯»æºç ï¼Œé›†æˆè‡ªå·±çš„å¤§æ¨¡å‹åˆ° langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2eacc5-b64f-43dd-ab95-0afe81f827a6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>å¹²è´§ä»è¿™é‡Œå¼€å§‹ï¼</b><br>\n",
    "    æ¥ä¸‹æ¥çš„ä¾‹å­ä¸­ï¼Œä¼šç©¿æ’ langchian æºç è§£è¯»ã€‚\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4086972-8af4-41e8-9f1d-7d3a5e984bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# åŠ è½½ .env åˆ°ç¯å¢ƒå˜é‡\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a33f05-0ea9-427c-b851-6511e0b9fc29",
   "metadata": {},
   "source": [
    "## 1ã€å¤§æ¨¡å‹çš„ä¸€èˆ¬ç”¨æ³•å›é¡¾"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f07493-0a57-4c60-a846-e6b1b69bfa46",
   "metadata": {},
   "source": [
    "### âœï¸ ä½¿ç”¨ OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56f9e2e4-782e-466b-be9d-c23b0c30242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "6c4db118-7b74-4d8e-8d9c-e8976ff4294e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "çŸ¥é“ï¼Œã€Œå¤æ´›ç‰¹çƒ¦æ¼ã€æ˜¯ä¸€éƒ¨2015å¹´ä¸Šæ˜ çš„ä¸­å›½ç”µå½±ï¼Œç”±æ²ˆè…¾ã€é©¬ä¸½ç­‰ä¸»æ¼”ã€‚è¯¥ç”µå½±è®²è¿°äº†ä¸€ä¸ªå¹³å‡¡ä¸Šç­æ—å¤æ´›ç‰¹åœ¨å·¥ä½œå’Œç”Ÿæ´»ä¸­é‡åˆ°çš„ä¸€ç³»åˆ—çƒ¦æ¼å’Œå›°æ‰°ï¼Œä»¥å¹½é»˜æç¬‘çš„æ–¹å¼å±•ç°äº†ç°ä»£éƒ½å¸‚äººçš„ç”Ÿæ´»çŠ¶æ€å’Œæƒ…æ„Ÿå›°æƒ‘ã€‚è¿™éƒ¨ç”µå½±åœ¨ä¸­å›½å–å¾—äº†å·¨å¤§çš„ç¥¨æˆ¿æˆåŠŸï¼Œæ·±å—è§‚ä¼—å–œçˆ±ã€‚\n"
     ]
    }
   ],
   "source": [
    "# invoke\n",
    "text = \"ä½ çŸ¥é“ã€Œå¤æ´›ç‰¹çƒ¦æ¼ã€è¿™éƒ¨ç”µå½±å—ï¼Ÿ\"\n",
    "response = llm.invoke(text)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "4ad948cb-8584-43f4-9dce-b008fa6cbc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|çŸ¥|é“|çš„|ï¼Œ|ã€Œ|å¤|æ´›|ç‰¹|çƒ¦|æ¼|ã€|æ˜¯|ä¸€|éƒ¨|201|5|å¹´|ä¸Š|æ˜ |çš„|ä¸­å›½|ç”µ|å½±|ï¼Œ|ç”±|æ²ˆ|è…¾|ã€|é©¬|ä¸½|ç­‰|ä¸»|æ¼”|ã€‚|è¿™|éƒ¨|ç”µ|å½±|è®²|è¿°|äº†|ä¸€ä¸ª|å¹³|å‡¡|çš„|å°|äºº|ç‰©|å¤|æ´›|ç‰¹|åœ¨|ç”Ÿ|æ´»|ä¸­|é‡|åˆ°|å„|ç§|çƒ¦|æ¼|å’Œ|å›°|éš¾|ï¼Œ|ä»¥|å¹½|é»˜|æ|ç¬‘|çš„|æ–¹å¼|å±•|ç°|äº†|ä»–|çš„|ç”Ÿ|æ´»|ç»|å†|å’Œ|æˆ|é•¿|æ•…|äº‹|ã€‚|è¿™|éƒ¨|ç”µ|å½±|åœ¨|ä¸­å›½|å–|å¾—|äº†|å¾ˆ|é«˜|çš„|ç¥¨|æˆ¿|å’Œ|å£|ç¢‘|ï¼Œ|æ·±|å—|è§‚|ä¼—|å–œ|çˆ±|ã€‚||"
     ]
    }
   ],
   "source": [
    "# stream\n",
    "for chunk in llm.stream(text):\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d63eec-9f22-4f5b-94bf-40f721313ad1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>âš ï¸ æ€è€ƒ</b><br>\n",
    "    langchain æ”¯æŒçš„8ä¸ªæ–¹æ³•éƒ½åœ¨ä»€ä¹ˆåœºæ™¯ä¸‹ä½¿ç”¨ï¼Ÿ\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68d2136-fe1d-4723-87d3-162309f07230",
   "metadata": {},
   "source": [
    "- invokeï¼šæ ‡å‡†åŒ–è°ƒåº¦\n",
    "- batch: æ ‡å‡†åŒ–æ‰¹é‡è°ƒåº¦\n",
    "- streamï¼šæ ‡å‡†åŒ–æµå¼è¾“å‡º\n",
    "- ainvoke / astream / abatch: æ ‡å‡†åŒ–å¼‚æ­¥è°ƒåº¦\n",
    "- astream_log / astream_events: ä»é“¾ã€æ™ºèƒ½ä½“ã€langgraphæŒ‰ç…§namesã€tagsã€eventsæå–å„ç¯èŠ‚çš„æµå¼è¾“å‡º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c803877e-8f15-4a3b-88a5-fcfdc5b1dea5",
   "metadata": {},
   "source": [
    "### âœï¸ LCELï¼šLLM + Prompt + OutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "32dea0d5-8c5c-436d-b006-67b07eceae8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|çŸ¥|é“|ï¼Œ|å¤|æ´›|ç‰¹|çƒ¦|æ¼|æ˜¯|ä¸€|éƒ¨|ä¸­å›½|ç”µ|å½±|ï¼Œ|è®²|è¿°|äº†|ä¸€ä¸ª|ä¸­|å¹´|ç”·|å­|å¤|æ´›|ç‰¹|åœ¨|å®¶|åº­|å’Œ|å·¥|ä½œ|ä¸­|é¢|ä¸´|å„|ç§|å›°|æ‰°|å’Œ|çƒ¦|æ¼|çš„|æ•…|äº‹|ã€‚|è¿™|éƒ¨|ç”µ|å½±|åœ¨|ä¸­å›½|å–|å¾—|äº†|å·¨|å¤§|çš„|æˆåŠŸ|ï¼Œ|æˆ|ä¸º|äº†|ä¸€|éƒ¨|ç»|å…¸|çš„|å–œ|å‰§|ä½œ|å“|ã€‚||"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"ä½ çŸ¥é“{name}è¿™éƒ¨ç”µå½±å—?\")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "for chunk in chain.stream({\"name\": \"å¤æ´›ç‰¹çƒ¦æ¼\"}):\n",
    "    print(chunk, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73527d37-fdb8-4d76-b70c-4ce1c209a3ed",
   "metadata": {},
   "source": [
    "## 2ã€ç®€å•çš„å¼€å§‹ï¼šå®ç°ã€Œæ¥¼ä¸‹é‚»å±…è€å¤§çˆ·ã€AIå¤§æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc945190-d75b-49de-94f5-801ee3125488",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>âš ï¸ è¦æ·±åº¦ langchain å°±å¿…é¡»é˜…è¯»æºç ï¼š</b><br>\n",
    "    åƒé›†æˆè‡ªå·±çš„å¤§æ¨¡å‹åˆ° langchain è¿™æ ·çš„ä»»åŠ¡ï¼Œlangchain æ–‡æ¡£ä¸­åŸºæœ¬æœªæ›¾æåŠã€‚<br>\n",
    "    å› æ­¤ï¼Œå¿…é¡»ç¿»çœ‹æºç æ‰èƒ½ææ¸…æ¥šå®ç°çš„æœºç†ã€‚<br><br>\n",
    "    ä½† langchain çš„ğŸŒ¹é­…åŠ›ğŸŒ¹å°±åœ¨äºï¼šä½ å¯ä»¥å‚ä¸communityï¼Œæˆ–å»ºç«‹è‡ªå·±çš„community ï¼\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a3028e-da74-4247-af83-d76f03544797",
   "metadata": {},
   "source": [
    "### ğŸ¦œ éœ€æ±‚åˆ†æï¼šå‚è€ƒç”µå½±ç‰‡æ®µï¼ŒæŠŠã€Œéå¸¸æœ‰æ™ºæ…§çš„æ¥¼ä¸‹é‚»å±…è€å¤§çˆ·ã€å˜æˆ AIå¤§æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e6fe6d-6fdd-49ea-88ce-496eddd23e5b",
   "metadata": {},
   "source": [
    "1. ä½¿ç”¨å¤§æ¨¡å‹ï¼šæ¨¡æ‹Ÿä¸€ä¸ªå¤§æ¨¡å‹\n",
    "2. ç”Ÿæˆèƒ½åŠ›ï¼šæåŠé©¬å†¬æ¢…æ—¶ç”Ÿæˆæ‰“å²”é—²èŠï¼ˆé©¬ä»€ä¹ˆæ¢…ï¼Ÿé©¬å†¬ä»€ä¹ˆï¼Ÿä»€ä¹ˆå†¬æ¢…ï¼Ÿï¼‰ï¼Œå…¶ä½™ç”Ÿæˆâ€œå“¦...â€œ\n",
    "\n",
    "![é©¬ä»€ä¹ˆæ¢…ï¼Ÿé©¬å†¬ä»€ä¹ˆï¼Ÿä»€ä¹ˆå†¬æ¢…ï¼Ÿ](./madongmei.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c0526e-50a9-481f-ae14-6433fc114d25",
   "metadata": {},
   "source": [
    "### âœï¸ åŸºæœ¬å®ç°ï¼šinvoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "52b2c9ba-39af-488b-af00-d272b7bff551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "from typing import Any, Dict, Iterator, List, Optional, Union\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import HumanMessage, HumanMessageChunk, AIMessage, AIMessageChunk, BaseMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "2ebdb44f-afd7-4ffc-9419-56e144a6d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatWithOlderAI(BaseChatModel):\n",
    "    \"\"\"æ¨¡æ‹Ÿè·Ÿé©¬å†¬æ¢…æ¥¼ä¸‹é‚»å±…è€å¤§çˆ·çš„å¯¹è¯\"\"\"\n",
    "\n",
    "    # å¿…é¡»å®ç°\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"chat-with-neighber-older\"\n",
    "\n",
    "    # å¿…é¡»å®ç°\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        generations = [ChatGeneration(message=res) for res in self._ask_remote(messages)]\n",
    "        return ChatResult(generations=generations)\n",
    "\n",
    "    # æœ‰è€å¤§çˆ·çš„æ™ºæ…§è®¡æ—¶å‘¨æœŸ\n",
    "    i: int = 0\n",
    "\n",
    "    # è®¿é—®è¿œç¨‹å¤§æ¨¡å‹\n",
    "    # å½“ä½ éœ€è¦å®ç°è‡ªå·±çš„å¤§æ¨¡å‹æ—¶ï¼Œä¸»è¦æ˜¯æ›¿æ¢è¿™éƒ¨åˆ†\n",
    "    def _ask_remote(self, messages: List[BaseMessage]) -> List[BaseMessage]:\n",
    "        answers = [HumanMessage(m) for m in [\n",
    "            \"é©¬ä»€ä¹ˆæ¢…ï¼Ÿ\",\n",
    "            \"ä»€ä¹ˆå†¬æ¢…ï¼Ÿï¼Ÿ\",\n",
    "            \"é©¬ä¸œä»€ä¹ˆï¼Ÿï¼Ÿï¼Ÿ\",\n",
    "        ]]\n",
    "        \n",
    "        if(re.search(\"é©¬å†¬æ¢…\", messages[0].content)):\n",
    "            response = answers[self.i]\n",
    "            self.i = (self.i + 1) if self.i < (len(answers) - 1) else 0\n",
    "        else:\n",
    "            response = AIMessage(\"å“¦...\")\n",
    "            \n",
    "        return [response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "9fb12783-b794-4ce2-8ace-4bd781be835e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "å¤æ´›ï¼šå¤§çˆ·ï¼Œæ¥¼ä¸Š322ä½çš„æ˜¯é©¬å†¬æ¢…å®¶å—ï¼Ÿ\n",
      "å¤§çˆ·ï¼šé©¬ä»€ä¹ˆæ¢…ï¼Ÿ|\n",
      "\n",
      "å¤æ´›ï¼šé©¬å†¬æ¢…å•Š\n",
      "å¤§çˆ·ï¼šä»€ä¹ˆå†¬æ¢…ï¼Ÿï¼Ÿ|\n",
      "\n",
      "å¤æ´›ï¼šé©¬å†¬æ¢…ï¼\n",
      "å¤§çˆ·ï¼šé©¬ä¸œä»€ä¹ˆï¼Ÿï¼Ÿï¼Ÿ|\n",
      "\n",
      "å¤æ´›ï¼šæˆ‘æ˜¯è¯´é©¬å†¬æ¢…ï¼\n",
      "å¤§çˆ·ï¼šé©¬ä»€ä¹ˆæ¢…ï¼Ÿ|\n",
      "\n",
      "å¤æ´›ï¼šæ‚¨æ­‡ç€å§...\n",
      "å¤§çˆ·ï¼šå“¦...|"
     ]
    }
   ],
   "source": [
    "questions = [[HumanMessage(m)] for m in [\n",
    "    \"å¤§çˆ·ï¼Œæ¥¼ä¸Š322ä½çš„æ˜¯é©¬å†¬æ¢…å®¶å—ï¼Ÿ\",\n",
    "    \"é©¬å†¬æ¢…å•Š\",\n",
    "    \"é©¬å†¬æ¢…ï¼\",\n",
    "    \"æˆ‘æ˜¯è¯´é©¬å†¬æ¢…ï¼\",\n",
    "    \"æ‚¨æ­‡ç€å§...\",\n",
    "]]\n",
    "\n",
    "llm = ChatWithOlderAI()\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n\\nå¤æ´›ï¼š{question[0].content}\")\n",
    "    print(\"å¤§çˆ·ï¼š\", end=\"\")\n",
    "    # print(llm.invoke(question).content, end=\"\")\n",
    "    for chunk in llm.stream(question):\n",
    "        print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8231e5a-c8b9-4d8e-8e56-280ccc889fd7",
   "metadata": {},
   "source": [
    "### âœï¸ æ”¯æŒæµå¼è¾“å‡ºï¼šstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "fe2633f0-9002-4684-818a-44daed91c706",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamChatWithOlderAI(ChatWithOlderAI):\n",
    "    \"\"\"æ¨¡æ‹Ÿè·Ÿå¤§çˆ·çš„å¯¹è¯ï¼Œæ”¯æŒæµ\"\"\"\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        *args,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGenerationChunk]:\n",
    "        response = self._ask_remote(messages)\n",
    "        for chunk in response[0].content:\n",
    "            time.sleep(0.1)\n",
    "            yield ChatGenerationChunk(message=AIMessageChunk(content=chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "236c1cf8-0886-423f-9710-cd10ef7c6e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "å¤æ´›ï¼šå¤§çˆ·ï¼Œæ¥¼ä¸Š322ä½çš„æ˜¯é©¬å†¬æ¢…å®¶å—ï¼Ÿ\n",
      "å¤§çˆ·ï¼šé©¬|ä»€|ä¹ˆ|æ¢…|ï¼Ÿ|\n",
      "\n",
      "å¤æ´›ï¼šé©¬å†¬æ¢…å•Š\n",
      "å¤§çˆ·ï¼šä»€|ä¹ˆ|å†¬|æ¢…|ï¼Ÿ|ï¼Ÿ|\n",
      "\n",
      "å¤æ´›ï¼šé©¬å†¬æ¢…ï¼\n",
      "å¤§çˆ·ï¼šé©¬|ä¸œ|ä»€|ä¹ˆ|ï¼Ÿ|ï¼Ÿ|ï¼Ÿ|\n",
      "\n",
      "å¤æ´›ï¼šæˆ‘æ˜¯è¯´é©¬å†¬æ¢…ï¼\n",
      "å¤§çˆ·ï¼šé©¬|ä»€|ä¹ˆ|æ¢…|ï¼Ÿ|\n",
      "\n",
      "å¤æ´›ï¼šæ‚¨æ­‡ç€å§...\n",
      "å¤§çˆ·ï¼šå“¦|.|.|.|"
     ]
    }
   ],
   "source": [
    "llm_stream = StreamChatWithOlderAI()\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n\\nå¤æ´›ï¼š{question[0].content}\")\n",
    "    print(\"å¤§çˆ·ï¼š\", end=\"\")\n",
    "    for chunk in llm_stream.stream(question):\n",
    "        print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaa2f9d-2177-4c7e-b935-a947b675ee9a",
   "metadata": {},
   "source": [
    "### âœï¸  LCELï¼šPrompt + LLM + OutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "1a7d4000-66db-4be9-9dae-e9ab7e14cbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "å¤æ´›ï¼šæ¥¼ä¸Š322ä½çš„æ˜¯é©¬å†¬æ¢…å®¶å—ï¼Ÿ\n",
      "å¤§çˆ·ï¼šé©¬|ä»€|ä¹ˆ|æ¢…|ï¼Ÿ|\n",
      "\n",
      "å¤æ´›ï¼šé©¬å†¬æ¢…å•Š\n",
      "å¤§çˆ·ï¼šä»€|ä¹ˆ|å†¬|æ¢…|ï¼Ÿ|ï¼Ÿ|\n",
      "\n",
      "å¤æ´›ï¼šé©¬å†¬æ¢…ï¼\n",
      "å¤§çˆ·ï¼šé©¬|ä¸œ|ä»€|ä¹ˆ|ï¼Ÿ|ï¼Ÿ|ï¼Ÿ|\n",
      "\n",
      "å¤æ´›ï¼šæˆ‘æ˜¯è¯´é©¬å†¬æ¢…ï¼\n",
      "å¤§çˆ·ï¼šé©¬|ä»€|ä¹ˆ|æ¢…|ï¼Ÿ|\n",
      "\n",
      "å¤æ´›ï¼šå¤§çˆ·æ‚¨æ­‡ç€å§...\n",
      "å¤§çˆ·ï¼šå“¦|.|.|.|"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "questions_text = [\n",
    "    \"æ¥¼ä¸Š322ä½çš„æ˜¯é©¬å†¬æ¢…å®¶å—ï¼Ÿ\",\n",
    "    \"é©¬å†¬æ¢…å•Š\",\n",
    "    \"é©¬å†¬æ¢…ï¼\",\n",
    "    \"æˆ‘æ˜¯è¯´é©¬å†¬æ¢…ï¼\",\n",
    "    \"å¤§çˆ·æ‚¨æ­‡ç€å§...\"\n",
    "]\n",
    "\n",
    "llm_stream = StreamChatWithOlderAI()\n",
    "prompt = PromptTemplate.from_template(\"å¤§çˆ·ï¼Œ{question}\")\n",
    "chain = prompt | llm_stream | StrOutputParser()\n",
    "\n",
    "for question in questions_text:\n",
    "    print(f\"\\n\\nå¤æ´›ï¼š{question}\")\n",
    "    print(\"å¤§çˆ·ï¼š\", end=\"\")\n",
    "    for chunk in chain.stream({\"question\": question}):\n",
    "        print(chunk, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae412a-cfd5-4160-80df-a6265560cd34",
   "metadata": {},
   "source": [
    "### ğŸŒ¹ å®¡è§†é“¾ä¸Šå„èŠ‚ç‚¹çš„è¾“å…¥è¾“å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "4aeecb41-c94c-4046-817f-8642d0ecb937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StreamChatWithOlderAI |  \n",
      "+-----------------------+  \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "3c055383-7ea1-4db2-b638-82d93df772d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], template='å¤§çˆ·ï¼Œ{question}')"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c05f8e-d9e2-4a26-9558-aaa2dbcc8ad9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# llm_stream.input_schema.schema()\n",
    "# chain.input_schema.schema()\n",
    "# prompt.output_schema.schema()\n",
    "# prompt.invoke({\"question\":\"ä½ å¥½\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08bc9d3-ecbc-4327-99bd-0d1cb97389f2",
   "metadata": {},
   "source": [
    "### ğŸŒ¹ é˜…è¯»æºç ï¼šå®ç° langchain å¤§æ¨¡å‹çš„åŸç†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd62fc73-8277-4bea-97f6-d79a08dd64e9",
   "metadata": {},
   "source": [
    "#### ï¼ˆ1ï¼‰BaseLanguageModel\n",
    "\n",
    "æ¥è‡ªï¼š[langchain_core/language_models/base.py](https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/language_models/base.py#L74-L81)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31c2efe-c70d-4616-8b99-337f4ac4ea53",
   "metadata": {},
   "source": [
    "```python\n",
    "class BaseLanguageModel(\n",
    "    RunnableSerializable[LanguageModelInput, LanguageModelOutputVar], ABC\n",
    "):\n",
    "    \"\"\"Abstract base class for interfacing with language models.\n",
    "\n",
    "    All language model wrappers inherit from BaseLanguageModel.\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c54270-3787-4248-85d7-fce89e39bbe6",
   "metadata": {},
   "source": [
    "#### ï¼ˆ2ï¼‰BaseChatModel\n",
    "\n",
    "æ¥è‡ªï¼š[langchain_core/language_models/chat_models.py](https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/language_models/chat_models.py#L100)\n",
    "\n",
    "**æ ¸å¿ƒé€»è¾‘ï¼š**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736c153a-fc04-47bb-96d1-8a57741f1de1",
   "metadata": {},
   "source": [
    "```python\n",
    "class BaseChatModel(BaseLanguageModel[BaseMessage], ABC):\n",
    "    \"\"\"Base class for Chat models.\"\"\"\n",
    "\n",
    "    ...\n",
    "\n",
    "    def invoke(...) -> BaseMessage:\n",
    "        # ...\n",
    "        self.generate_prompt(...)\n",
    "        # generate_prompt >> generate >> _generate\n",
    "\n",
    "    def ainvoke(...) -> BaseMessage:\n",
    "        # ...\n",
    "        self.agenerate_prompt(...)\n",
    "        # agenerate_prompt >> agenerate >> _agenerate >> _generate\n",
    "    \n",
    "    def stream(...) -> Iterator[BaseMessageChunk]:\n",
    "        # ...\n",
    "        if type(self)._stream == BaseChatModel._stream:\n",
    "            # model doesn't implement streaming, so use default implementation\n",
    "            yield cast(\n",
    "                BaseMessageChunk, self.invoke(input, config=config, stop=stop, **kwargs)\n",
    "            )\n",
    "        # ...\n",
    "\n",
    "    async def astream(...) -> AsyncIterator[BaseMessageChunk]:\n",
    "        # åœ¨#19332åˆå¹¶ä¸­ï¼Œ_astreamæ–¹æ³•å®ç°å·²ç»è¢«ç®€åŒ–\n",
    "        # https://github.com/langchain-ai/langchain/pull/19332/commits/afbe6ac659e41ab5f4a6f4dcaa33511e9e59e4d5\n",
    "        if (\n",
    "            type(self)._astream is BaseChatModel._astream\n",
    "            and type(self)._stream is BaseChatModel._stream\n",
    "        ):\n",
    "            # No async or sync stream is implemented, so fall back to ainvoke\n",
    "            yield cast(\n",
    "                BaseMessageChunk,\n",
    "                await self.ainvoke(input, config=config, stop=stop, **kwargs),\n",
    "            )\n",
    "        # ...\n",
    "\n",
    "    # bacth, abatch, astream_log, astream_events \n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28df1cf1-2efe-4bf0-8f6b-4bd752084f9f",
   "metadata": {},
   "source": [
    "**å¿…é¡»å®ç°çš„éƒ¨åˆ†ï¼š**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa90a6e-8255-4e6f-9876-2f4cf16d2041",
   "metadata": {},
   "source": [
    "```python\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"chat-with-neighber-older\"\n",
    "        \n",
    "    ## ******** invoke / ainvoke / batch / abatch **********\n",
    "    @abstractmethod\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        \"\"\"Top Level call\"\"\"\n",
    "\n",
    "    ## ******** stream / astream / astream_log / astream_events **********\n",
    "    def _stream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGenerationChunk]:\n",
    "        raise NotImplementedError()        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78830db-43ef-4dc4-b5cb-c0a83ac4e3ef",
   "metadata": {},
   "source": [
    "### ğŸŒ¹ æ¨èé˜…è¯»ï¼šlangchain å†…ç½®å®ç°çš„ FakeLLM æºç \n",
    "\n",
    "- [FakeListLLM](https://github.com/langchain-ai/langchain/blob/8595c3ab59371d9932310eb12a2c3220afe3ba84/libs/core/langchain_core/language_models/fake.py#L14)\n",
    "- [FakeStreamingListLLM](https://github.com/langchain-ai/langchain/blob/8595c3ab59371d9932310eb12a2c3220afe3ba84/libs/core/langchain_core/language_models/fake.py#L61)\n",
    "- [FakeMessagesListChatModel](https://github.com/langchain-ai/langchain/blob/8595c3ab59371d9932310eb12a2c3220afe3ba84/libs/core/langchain_core/language_models/fake_chat_models.py#L16)\n",
    "- [FakeChatModel](https://github.com/langchain-ai/langchain/blob/8595c3ab59371d9932310eb12a2c3220afe3ba84/libs/core/langchain_core/language_models/fake_chat_models.py#L120C7-L120C20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f664542-8ea0-4cba-ba61-735c0fea6b04",
   "metadata": {},
   "source": [
    "## 3ã€é›†æˆæ™ºè°±å¤§æ¨¡å‹åˆ° Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9e4176-391c-41a4-bc21-988f7b295843",
   "metadata": {},
   "source": [
    "### ğŸ¦œ éœ€æ±‚åˆ†æï¼šç»“åˆæ™ºè°±å®˜æ–¹æ–‡æ¡£ï¼Œç¼–å†™ChatZhipuAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ff16f8-4f04-4639-9cbb-5aead5c88076",
   "metadata": {},
   "source": [
    "- [æ™ºè°±AIå®˜æ–¹çš„Pythonæ¥å£æ–‡æ¡£](https://maas.aminer.cn/dev/api#sdk)\n",
    "- [Langchainä¸­å·²æœ‰çš„æ™ºè°±AIç»„ä»¶ï¼ˆæ—§ç‰ˆæœ¬ï¼‰](https://python.langchain.com/docs/integrations/chat/zhipuai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859d217d-27e3-4802-9ea6-e5d585a2bd01",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>âš ï¸ æ™ºè°±AIé‡åˆ°çš„å°´å°¬ï¼š</b><br>\n",
    "    å› ä¸º pydantic ç‰ˆæœ¬å…¼å®¹çš„é—®é¢˜ï¼Œæ™ºè°±å®˜æ–¹SDKå‡çº§åˆ°4.0ä¹‹å, Langchainç›¸åº”çš„åŒ…ä¸€ç›´æ²¡æœ‰æ›´æ–°ã€‚<br>\n",
    "    æˆªæ­¢åˆ°2024å¹´3æœˆ29æ—¥ï¼Œä»ç„¶ä¸å¯ç”¨ï¼ˆå¦‚æœçªç„¶å¯ç”¨äº†ä¹Ÿä¸è¦ç´§ï¼Œæœ¬è¯¾å†…å®¹åŸæœ¬å°±æ˜¯æŠ›è½¬å¼•ç‰ï¼‰\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b7ebf3-ec67-4cbc-8c13-3cba71d7461f",
   "metadata": {},
   "source": [
    "#### ï¼ˆ1ï¼‰æ™ºè°±å®˜æ–¹å¯ç”¨æ¥å£\n",
    "- ç›´æ¥è°ƒç”¨ï¼šå¯å®ç° invoke\n",
    "- å¼‚æ­¥è°ƒç”¨ï¼ˆå…ˆè°ƒç”¨ï¼Œå†æŸ¥è¯¢ç»“æœï¼‰ï¼šé€‚åˆå®ç° ainvoke / batch / abatch\n",
    "- æµå¼è°ƒç”¨ï¼ˆSSEï¼ŒServer-Send Events)ï¼šé€‚åˆå®ç° stream / astream / stream_log / stream_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf4066-d16d-4307-93d5-d54f56cdd3a2",
   "metadata": {},
   "source": [
    "#### ï¼ˆ2ï¼‰æ™ºè°±å®˜æ–¹æ”¯æŒèƒ½åŠ›\n",
    "- æ”¯æŒå·¥å…·ï¼šæœ¬åœ°å·¥å…·å›è°ƒ / äº‘ä¸Šæ£€ç´¢ / äº‘ä¸Šäº’è”ç½‘æœç´¢\n",
    "- æ”¯æŒè¯†å›¾\n",
    "- æ”¯æŒç”Ÿå›¾"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f4f39d-46f4-401f-93c3-b6a59a687754",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>âš ï¸ æ€è€ƒï¼š</b><br>\n",
    "    èƒ½å¦å®ç°ä¸€ä¸ªè‡ªå®šä¹‰å¤§æ¨¡å‹ï¼Œæ”¯æŒå¤šæ¨¡æ€èƒ½åŠ›ï¼Ÿ<br>\n",
    "    input: List[Union[æ–‡å­—, å›¾åƒ]] -> Union[æ–‡å­—, å›¾åƒ]\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dbbbff-1483-487a-8648-141b5f64fc99",
   "metadata": {},
   "source": [
    "#### ï¼ˆ3ï¼‰æ™ºè°±å®˜æ–¹é€Ÿç‡é™åˆ¶\n",
    "\n",
    "[æ™ºè°±AIå®˜æ–¹æ–‡æ¡£ä¸­å¯¹é€Ÿç‡é™åˆ¶çš„è¯´æ˜](https://maas.aminer.cn/dev/howuse/rate-limits/why?tab=5)ï¼š\n",
    "\n",
    "> å½“å‰æˆ‘ä»¬åŸºäºç”¨æˆ·çš„æœˆåº¦ API è°ƒç”¨æ¶ˆè€—é‡‘é¢æƒ…å†µå°†é€Ÿç‡æ§åˆ¶åˆ†ä¸º6ç§ç­‰çº§ã€‚\n",
    ">\n",
    "> æ¶ˆè€—é‡‘é¢é€‰å–é€»è¾‘ï¼šæˆ‘ä»¬ä¼šé€‰å–ç”¨æˆ·å½“å‰æœˆä»½1å·ï½t-1æ—¥çš„è°ƒç”¨ API æ¨ç†æ¶ˆè€—æ€»é‡‘é¢å’Œç”¨æˆ·ä¸Šä¸ªæœˆçš„ API è°ƒç”¨æ¶ˆè€—æ€»é‡‘é¢åšæ¯”è¾ƒï¼Œå–æ›´é«˜é‡‘é¢ä½œä¸ºç”¨æˆ·å½“å‰çš„ API æ¶ˆè€—é‡‘é¢ã€‚\n",
    ">\n",
    "> ç‰¹åˆ«çš„ï¼Œè‹¥æ‚¨ä»æœªæ›¾ä»˜è´¹å……å€¼/è´­ä¹°è¿‡èµ„æºåŒ…ï¼Œåˆ™ä¼šå½’ä¸ºå…è´¹çº§åˆ«ã€‚\n",
    "\n",
    "**æ•´ç†GLM4æ¨¡å‹ä½¿ç”¨é™åˆ¶å¦‚ä¸‹ï¼š**\n",
    "|ç”¨æˆ·ç­‰çº§|ä½¿ç”¨é‡|GLM4å¹¶å‘é™åˆ¶|\n",
    "|:---|:---|:---|\n",
    "|å…è´¹|apiè°ƒç”¨æ¶ˆè€—0å…ƒ-50å…ƒ/æ¯æœˆï¼ˆä¸å«ï¼‰|5|\n",
    "|ä½¿ç”¨é‡1|apiè°ƒç”¨æ¶ˆè€—50å…ƒ-500å…ƒ/æ¯æœˆï¼ˆä¸å«ï¼‰|10|\n",
    "|ä½¿ç”¨é‡2|apiè°ƒç”¨æ¶ˆè€—500å…ƒ-5000å…ƒ/æ¯æœˆï¼ˆä¸å«ï¼‰|20|\n",
    "|ä½¿ç”¨é‡3|apiè°ƒç”¨æ¶ˆè€—5000å…ƒ-10000å…ƒ/æ¯æœˆï¼ˆä¸å«ï¼‰|30|\n",
    "|ä½¿ç”¨é‡4|apiè°ƒç”¨æ¶ˆè€—10000å…ƒ-30000å…ƒ/æ¯æœˆï¼ˆä¸å«ï¼‰|100|\n",
    "|ä½¿ç”¨é‡5|apiè°ƒç”¨æ¶ˆè€—30000å…ƒä»¥ä¸Š/æ¯æœˆ|200|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3941a68e-e6de-40f9-8ecb-610556b92296",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>âš ï¸ æ€è€ƒï¼š</b><br>\n",
    "    èƒ½å¦å®ç°ä¸€ä¸ªè‡ªå®šä¹‰å¤§æ¨¡å‹ï¼Œé€šè¿‡å¤šä¸ªä½ç”¨é‡è´¦æˆ·æ± çš„ç®¡ç†æœºåˆ¶æ¥æé«˜å¯ç”¨çš„è°ƒç”¨é€Ÿç‡ï¼Ÿ<br>\n",
    "    é€Ÿç‡é™åˆ¶å±äºæœåŠ¡ç«¯é™çº§ï¼Œè°ƒç”¨æ—¶è‡ªåŠ¨æŒ‰ç…§é€Ÿç‡é™åˆ¶æ’é˜Ÿï¼Œå½±å“ç”¨æˆ·ä½“éªŒã€‚\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e18cd-e44e-4f28-a8a2-819f719c4daf",
   "metadata": {},
   "source": [
    "### âœï¸ æµ‹è¯•å®˜æ–¹ä¾‹å­"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c317160f-22fe-4210-bc69-ed83abacfeca",
   "metadata": {},
   "source": [
    "çœ‹å®˜æ–¹ä¾‹å­ï¼š[https://github.com/MetaGLM/zhipuai-sdk-python-v4](https://github.com/MetaGLM/zhipuai-sdk-python-v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9dad0716-95a2-4966-bf90-07d33be54cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai import ZhipuAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9681792b-ab7d-4269-b9d3-8f55a25ce0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='ä½ å¥½ï¼Œæˆ‘å«æ™ºè°±æ¸…è¨€ï¼Œæ˜¯æ¸…åå¤§å­¦ KEG å®éªŒå®¤å’Œæ™ºè°± AI å…¬å¸äº 2023 å¹´å…±åŒè®­ç»ƒçš„è¯­è¨€æ¨¡å‹ã€‚æˆ‘çš„ä»»åŠ¡æ˜¯é’ˆå¯¹ç”¨æˆ·çš„é—®é¢˜å’Œè¦æ±‚æä¾›é€‚å½“çš„ç­”å¤å’Œæ”¯æŒã€‚' role='assistant' tool_calls=None\n"
     ]
    }
   ],
   "source": [
    "## çœ‹çœ‹å®˜æ–¹çš„ä¾‹å­æ˜¯å¦èƒ½æ­£ç¡®è¿è¡Œ\n",
    "client = ZhipuAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"ä½ å«ä»€ä¹ˆåå­—\"},\n",
    "    ],\n",
    ")\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9835dc40-1269-4415-8127-70f62f8ea0a0",
   "metadata": {},
   "source": [
    "### âœï¸ åŒ…è£…ä¸ºä¸€ä¸ªå‡½æ•°è°ƒç”¨ï¼ˆå‡è£…æˆ‘ä¸æƒ³ç›´æ¥ç”¨ langchainï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "9c782a4d-663b-423c-8cd7-3d63a7893e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is your name?'"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask_zhipu(question: str) -> str:\n",
    "    client = ZhipuAI()\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘æœºå™¨äººï¼Œæˆ‘è¯´ä¸­æ–‡ä½ å°±ç›´æ¥ç¿»è¯‘æˆè‹±æ–‡ï¼Œæˆ‘è¯´è‹±æ–‡ä½ å°±ç›´æ¥ç¿»è¯‘ä¸ºä¸­æ–‡ã€‚ä¸è¦è¾“å‡ºå…¶ä»–ï¼Œä¸è¦å•°å—¦ã€‚\"},\n",
    "        {\"role\": \"user\", \"content\": \"ä½ å¥½\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"hello\"},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    return(response.choices[0].message.content)\n",
    "\n",
    "ask_zhipu(\"ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8f6f0e-3891-4a30-91a8-afcfd2cf5d08",
   "metadata": {},
   "source": [
    "### âœï¸ æ”¯æŒä¸ Prompt åä½œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d65800-bc21-489a-9dd9-84b50b8e1be4",
   "metadata": {},
   "source": [
    "#### ï¼ˆ1ï¼‰èƒ½å¦å®ç°å¦‚ä¸‹åœºæ™¯ï¼Ÿï¼ˆä¼¼ä¹ä½¿ç”¨ langchain ä¹Ÿæ²¡é‚£ä¹ˆåï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2572d41a-b2a6-4b81-9426-f083081f86ac",
   "metadata": {},
   "source": [
    "```python\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "chain.invoke({\"question\": \"ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ\"})\n",
    "```\n",
    "\n",
    "åŸºæœ¬æ€è·¯ï¼š\n",
    "\n",
    "Promptè¾“å…¥ï¼ˆæŒ‰langchainæ ‡å‡†ï¼‰ <br>\n",
    "â¬‡ï¸ <br>\n",
    "Promptè¾“å‡ºï¼ˆæŒ‰langchainæ ‡å‡†ï¼‰ <br>\n",
    "â¬‡ï¸ <br>\n",
    "LLMè¾“å…¥ï¼ˆæŒ‰å¤§æ¨¡å‹æ ‡å‡†ï¼‰ <br>\n",
    "â¬‡ï¸ <br>\n",
    "LLMè¾“å‡ºï¼ˆæŒ‰langchainæ ‡å‡†ï¼‰\n",
    "â¬‡ï¸ <br>\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b5a333-e3e2-493b-8525-baf37a2f11ac",
   "metadata": {},
   "source": [
    "#### ï¼ˆ2ï¼‰æ„é€  Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd50c3d-5a21-4d6b-96e2-76a2e5ae5566",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>âš ï¸ æ€è€ƒ</b><br>\n",
    "    ä¸‹é¢ä»£ç ä¸­ï¼Œ ä¸ºä»€ä¹ˆ from_messages æ”¯æŒ systemã€humanã€ai è¿™äº›åå­—ï¼Ÿè¿˜æœ‰å…¶ä»–åå­—å—ï¼Ÿ\n",
    "</div>\n",
    "\n",
    "**ğŸŒ æç¤ºï¼š**\n",
    "- æ–‡æ¡£ä¸­æ²¡æœ‰ï¼Œè¦çœ‹æºç ï¼š[langchain_core/messages/utils.py](https://github.com/langchain-ai/langchain/blob/c93d4ea91cfcf55dfe871931d42aa22562f8dae2/libs/core/langchain_core/messages/utils.py#L130-L168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "024270e5-a030-421f-8af7-af64c1251dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘æœºå™¨äººï¼Œæˆ‘è¯´ä¸­æ–‡ä½ å°±ç›´æ¥ç¿»è¯‘æˆè‹±æ–‡ï¼Œæˆ‘è¯´è‹±æ–‡ä½ å°±ç›´æ¥ç¿»è¯‘ä¸ºä¸­æ–‡ã€‚ä¸è¦è¾“å‡ºå…¶ä»–ï¼Œä¸è¦å•°å—¦ã€‚'), HumanMessage(content='ä½ å¥½'), AIMessage(content='hello'), HumanMessage(content='ä½ å«ä»€åå­—ï¼Ÿ')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘æœºå™¨äººï¼Œæˆ‘è¯´ä¸­æ–‡ä½ å°±ç›´æ¥ç¿»è¯‘æˆè‹±æ–‡ï¼Œæˆ‘è¯´è‹±æ–‡ä½ å°±ç›´æ¥ç¿»è¯‘ä¸ºä¸­æ–‡ã€‚ä¸è¦è¾“å‡ºå…¶ä»–ï¼Œä¸è¦å•°å—¦ã€‚\"),\n",
    "    (\"human\", \"ä½ å¥½\"),\n",
    "    (\"ai\", \"hello\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "prompt.invoke({\"question\":\"ä½ å«ä»€åå­—ï¼Ÿ\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d90cc73-c09f-44c0-92bf-4bee04489894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘æœºå™¨äººï¼Œæˆ‘è¯´ä¸­æ–‡ä½ å°±ç›´æ¥ç¿»è¯‘æˆè‹±æ–‡ï¼Œæˆ‘è¯´è‹±æ–‡ä½ å°±ç›´æ¥ç¿»è¯‘ä¸ºä¸­æ–‡ã€‚ä¸è¦è¾“å‡ºå…¶ä»–ï¼Œä¸è¦å•°å—¦ã€‚'),\n",
       " HumanMessage(content='ä½ å¥½'),\n",
       " AIMessage(content='hello'),\n",
       " HumanMessage(content='ä½ å«ä»€åå­—ï¼Ÿ')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"question\":\"ä½ å«ä»€åå­—ï¼Ÿ\"}).to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae249420-7b4a-453e-be1a-47dbbde27a05",
   "metadata": {},
   "source": [
    "#### ï¼ˆ3ï¼‰ä» Prompt è¾“å‡ºæ ¼å¼ï¼Œè½¬æ¢åˆ°å¤§æ¨¡å‹çš„è¾“å…¥æ ¼å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a48adc15-0507-4950-bb27-f034e5a5ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.adapters.openai import convert_message_to_dict\n",
    "from langchain_core.messages import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a24c1316-3741-49a7-85da-806b5fcd2e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘æœºå™¨äººï¼Œæˆ‘è¯´ä¸­æ–‡ä½ å°±ç›´æ¥ç¿»è¯‘æˆè‹±æ–‡ï¼Œæˆ‘è¯´è‹±æ–‡ä½ å°±ç›´æ¥ç¿»è¯‘ä¸ºä¸­æ–‡ã€‚ä¸è¦è¾“å‡ºå…¶ä»–ï¼Œä¸è¦å•°å—¦ã€‚'},\n",
       " {'role': 'user', 'content': 'ä½ å¥½'},\n",
       " {'role': 'assistant', 'content': 'hello'},\n",
       " {'role': 'user', 'content': 'ä½ å«ä»€åå­—ï¼Ÿ'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[convert_message_to_dict(m) for m in prompt.invoke({\"question\":\"ä½ å«ä»€åå­—ï¼Ÿ\"}).to_messages()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7648a6-6ab7-4a62-81b3-d0b02c553ea2",
   "metadata": {},
   "source": [
    "### âœï¸ ç›´æ¥ç”¨ RunnableLambda è¾¾åˆ°ç›®çš„ï¼ˆå‡è£…æˆ‘ä¸æƒ³ç”¨ langchain çš„å¤§æ¨¡å‹åŸºç±»ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63e5a0a3-433d-48e7-950b-fc3d8a445e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain\n",
    "from typing import List\n",
    "from langchain_core.prompt_values import ChatPromptValue \n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "@chain\n",
    "def ask_zhipu(promptValue: ChatPromptValue) -> AIMessage:\n",
    "    client = ZhipuAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4\",\n",
    "        messages=[convert_message_to_dict(m) for m in promptValue.to_messages()],\n",
    "    )\n",
    "    return(AIMessage(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "367b9894-8a00-4913-b542-ed0f6e73ec91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is your name?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | ask_zhipu | StrOutputParser()\n",
    "chain.invoke({\"question\": \"ä½ å«ä»€åå­—ï¼Ÿ\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41dae6d9-9301-4c56-bc78-96f1ad2f758c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "  +--------------------+   \n",
      "  | ChatPromptTemplate |   \n",
      "  +--------------------+   \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "  +-------------------+    \n",
      "  | Lambda(ask_zhipu) |    \n",
      "  +-------------------+    \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "# çœ‹çœ‹å½“å‰é“¾çš„ç»“æ„\n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa150d60-9630-4597-bd2f-16524238f899",
   "metadata": {},
   "source": [
    "### âœï¸ åŸºäº BaseChatModel è¾¾åˆ°ç›®çš„ï¼ˆå‡è£…æˆ‘å¼€å§‹æƒ³è·å¾— LCEL çš„è¯¸å¤šå¥½å¤„ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67137baa-a298-4530-bcbb-f88f5fa4077c",
   "metadata": {},
   "source": [
    "#### ï¼ˆ1ï¼‰ä»å¤§æ¨¡å‹çš„è¾“å‡ºæ ¼å¼ï¼Œè½¬æ¢åˆ° langchain çš„æ ‡å‡†è¾“å‡ºæ ¼å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d46fb809-bfbd-4035-9de4-0a1d52a6da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.adapters.openai import convert_dict_to_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6daa2ea-e6a4-4289-a9c7-8707489433ef",
   "metadata": {},
   "source": [
    "#### ï¼ˆ2ï¼‰å®ç°æ”¯æŒ invoke çš„ç‰ˆæœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0374ab1-7121-4513-869e-bbc5ab56302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from typing import Any, Dict, Iterator, List, Optional, cast, Mapping\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be2ac9a7-62e2-45c8-a0b5-5b0d97824ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniZhipuAI(BaseChatModel):\n",
    "    \"\"\"æ”¯æŒæœ€æ–°çš„æ™ºè°±API\"\"\"\n",
    "\n",
    "    client: Optional[ZhipuAI] = None\n",
    "\n",
    "    def __init__(self, *args: Any, **kwargs: Any) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.client = ZhipuAI()\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return the type of chat model.\"\"\"\n",
    "        return \"zhipuai\"\n",
    "\n",
    "    def _ask_remote(self, messages, streaming=False, **kwargs):\n",
    "        # ä»langchainæ¶ˆæ¯æ ¼å¼ï¼Œè½¬æ¢åˆ°æ™ºè°±AIè¾“å…¥çš„æ ¼å¼\n",
    "        dict_zhipu = [convert_message_to_dict(m) for m in messages]\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"glm-4\",\n",
    "            messages=dict_zhipu,\n",
    "            stream=streaming,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # ä»æ™ºè°±AIè¾“å‡ºçš„æ ¼å¼ï¼Œè½¬æ¢åˆ°langchainçš„æ¶ˆæ¯æ ¼å¼\n",
    "        if not isinstance(response, dict):\n",
    "            response = response.dict()\n",
    "        return [convert_dict_to_message(c[\"message\"]) for c in response[\"choices\"]]\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        stream: Optional[bool] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        \"\"\"å®ç° ZhiputAI çš„åŒæ­¥è°ƒç”¨\"\"\"\n",
    "\n",
    "        # é—®æ™ºè°±AIï¼Œå¹¶å¾—åˆ°å›å¤\n",
    "        responses = self._ask_remote(messages, streaming=False, **kwargs)\n",
    "\n",
    "        return ChatResult(\n",
    "            generations=[ChatGeneration(message=m) for m in responses]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb34bc36-229e-4c44-a02f-f6537a89a67b",
   "metadata": {},
   "source": [
    "#### ï¼ˆ3ï¼‰ç”¨èµ·æ¥ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fee54840-3dfb-4d6d-ae4b-6a07ba3b2d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ä¸­ç¾åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„ç«äº‰éå¸¸æ¿€çƒˆã€‚ä¸­å›½èƒ½èµ¶ä¸Šå—ï¼Ÿ')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä¸ªä¸­è‹±äº’è¯‘æœºå™¨äººï¼Œåªè´Ÿè´£ç¿»è¯‘ï¼Œä¸è¦è¯•å›¾å¯¹é—®é¢˜åšè§£ç­”ã€‚æˆ‘è¯´ä¸­æ–‡ä½ å°±ç›´æ¥ç¿»è¯‘æˆè‹±æ–‡ï¼Œæˆ‘è¯´è‹±æ–‡ä½ å°±ç›´æ¥ç¿»è¯‘ä¸ºä¸­æ–‡ã€‚ä¸è¦è¾“å‡ºå…¶ä»–ï¼Œä¸è¦å•°å—¦ã€‚\"),\n",
    "    (\"human\", \"ä½ å¥½\"),\n",
    "    (\"ai\", \"hello\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "llm_zhipu = MiniZhipuAI()\n",
    "chain = prompt | llm_zhipu\n",
    "\n",
    "chain.invoke({\"question\": \"The competition between China and the United States in the AI field is very intense. Can China catch up?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7467f98a-f6c6-4443-b0ef-ef906cf26111",
   "metadata": {},
   "source": [
    "### âœï¸ å°è¯•åœ¨æ™ºèƒ½ä½“ä¸­ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f853191a-48d5-4385-a79b-d1ebaf4d9c69",
   "metadata": {},
   "source": [
    "#### ï¼ˆ1ï¼‰å®šä¹‰ä¸€ä¸ªç®€å•å·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2adeaac3-3dd6-4a5e-9591-7da1b31660c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool, convert_to_openai_function\n",
    "import re\n",
    "\n",
    "@tool\n",
    "def ask_neighber(query: str) -> str:\n",
    "    \"\"\"æˆ‘æ˜¯é©¬å†¬æ¢…çš„é‚»å±…è€å¤§çˆ·ï¼Œå…³äºå¥¹çš„äº‹æƒ…ä½ å¯ä»¥é—®æˆ‘\"\"\"\n",
    "    if(re.search(\"é©¬å†¬æ¢…\", query)):\n",
    "        return \"æ¥¼ä¸Š322\"\n",
    "    else:\n",
    "        return \"æˆ‘ä¸æ¸…æ¥š\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c272f4-0dca-4499-afce-3a91c93d413a",
   "metadata": {},
   "source": [
    "#### ï¼ˆ2ï¼‰ä½œä¸º openai é£æ ¼çš„å›è°ƒå·¥å…·ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98d510aa-b493-4920-968b-909cfdfd2933",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_zhipu = MiniZhipuAI().bind(tools=[convert_to_openai_tool(ask_neighber)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52acb9b5-9948-4de6-b364-c1da895d1a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_8516976669838980971', 'function': {'arguments': '{\"query\":\"é©¬å†¬æ¢…\"}', 'name': 'ask_neighber'}, 'type': 'function'}]})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_zhipu.invoke(\"å‘Šè¯‰æˆ‘é©¬å†¬æ¢…åœ¨å“ªä¸ªæˆ¿é—´ï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9eb8a61d-573c-468d-b9df-62516c521e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'æ¥¼ä¸Š322'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_neighber.invoke({\"query\":\"é©¬å†¬æ¢…\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7486f46b-8c36-49ba-aec8-5340206443fe",
   "metadata": {},
   "source": [
    "#### ï¼ˆ3ï¼‰é›†æˆåˆ°æ™ºèƒ½ä½“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5c09ac0d-f981-4c50-a2c7-fc41f082618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, Tool, create_openai_tools_agent\n",
    "from langchain import hub\n",
    "\n",
    "def create_neighber(llm):\n",
    "    tools = [ask_neighber]\n",
    "    prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "    return AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc382cdb-9ffc-40fa-a16c-41b5eb371c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `ask_neighber` with `{'query': 'é©¬å†¬æ¢…ä½å“ªé‡Œ'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mæ¥¼ä¸Š322\u001b[0m\u001b[32;1m\u001b[1;3mæ ¹æ®æˆ‘çš„æŸ¥è¯¢ç»“æœï¼Œé©¬å†¬æ¢…ä½åœ¨æ¥¼ä¸Š322æˆ¿é—´ã€‚å¸Œæœ›è¿™ä¸ªä¿¡æ¯å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ã€‚\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'é©¬å†¬æ¢…ä½å“ªé‡Œ', 'output': 'æ ¹æ®æˆ‘çš„æŸ¥è¯¢ç»“æœï¼Œé©¬å†¬æ¢…ä½åœ¨æ¥¼ä¸Š322æˆ¿é—´ã€‚å¸Œæœ›è¿™ä¸ªä¿¡æ¯å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ã€‚'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_neighber(MiniZhipuAI()).invoke({\"input\":\"é©¬å†¬æ¢…ä½å“ªé‡Œ\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77770d1-c8fa-4967-8142-e91d012652e9",
   "metadata": {},
   "source": [
    "### âœï¸ ç°åœ¨å¯ä»¥è½»æ¾åˆ‡æ¢æ™ºèƒ½ä½“ä¸­çš„å¤§æ¨¡å‹ï¼ˆå‡è£…æˆ‘å¯¹ langchian å¾ˆæ»¡æ„ï¼ğŸ‘ğŸ‘ğŸ‘ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eb52bc-0c67-40e7-9bef-d97680e21614",
   "metadata": {},
   "source": [
    "#### ï¼ˆ1ï¼‰ä½¿ç”¨ langchain_zhipu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ff552d-78cd-41c0-a302-b9a695c2887d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>âš ï¸ æ€è€ƒï¼š</b><br>\n",
    "    ä¸Šé¢ä»£ç å·²ç»ç›¸å¯¹å®Œæ•´å®ç°äº†ä¸€ä¸ª langchain å¤§æ¨¡å‹ï¼›ä½†ç¼ºå°‘å¾ˆå¤šç»†èŠ‚æ§åˆ¶ï¼Œå¯ä»¥å°è¯•è‡ªå·±åŠ¨æ‰‹æ·»åŠ ï¼    \n",
    "</div>\n",
    "\n",
    "**ğŸŒ å‚è€ƒï¼š**\n",
    "- [æŸ¥çœ‹ langchain_zhpu ä¸­çš„å®ç°æºç ](https://github.com/arcstep/langchain_zhipuai/blob/e55af13eed673bc409ffdb143030e6cc0b2af27c/langchain_zhipu/chat.py#L304-L354) [![PyPI version](https://img.shields.io/pypi/v/langchain_zhipu.svg)](https://pypi.org/project/langchain_zhipu/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91fddecd-fd0c-41f7-b262-114a36d54145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `ask_neighber` with `{'query': 'é©¬å†¬æ¢…ä½å“ªé‡Œ'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mæ¥¼ä¸Š322\u001b[0m\u001b[32;1m\u001b[1;3mæ ¹æ®æˆ‘çš„æŸ¥è¯¢ç»“æœï¼Œé©¬å†¬æ¢…ä½åœ¨æ¥¼ä¸Š322æˆ¿é—´ã€‚\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'é©¬å†¬æ¢…ä½å“ªé‡Œ', 'output': 'æ ¹æ®æˆ‘çš„æŸ¥è¯¢ç»“æœï¼Œé©¬å†¬æ¢…ä½åœ¨æ¥¼ä¸Š322æˆ¿é—´ã€‚'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_zhipu import ChatZhipuAI\n",
    "\n",
    "create_neighber(ChatZhipuAI()).invoke({\"input\":\"é©¬å†¬æ¢…ä½å“ªé‡Œ\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299ea67a-56ae-4b60-a2be-6a7021c973eb",
   "metadata": {},
   "source": [
    "#### ï¼ˆ2ï¼‰ä½¿ç”¨ langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb69b8-33a7-4f1e-9ac5-0e41972241d1",
   "metadata": {},
   "source": [
    "**è¿™ä¸ç›´æ¥ä½¿ç”¨OpenAIç±»ä¼¼ï¼š**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc557ddb-fa06-4fbf-90da-d9418123dab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `ask_neighber` with `{'query': 'é©¬å†¬æ¢…'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mæ¥¼ä¸Š322\u001b[0m\u001b[32;1m\u001b[1;3mé©¬å†¬æ¢…ä½åœ¨æ¥¼ä¸Š322æˆ¿é—´ã€‚\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'é©¬å†¬æ¢…ä½å“ªé‡Œ', 'output': 'é©¬å†¬æ¢…ä½åœ¨æ¥¼ä¸Š322æˆ¿é—´ã€‚'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "create_neighber(ChatOpenAI()).invoke({\"input\":\"é©¬å†¬æ¢…ä½å“ªé‡Œ\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1160eb50-4574-46fa-b9d4-aeaa0c773a6e",
   "metadata": {},
   "source": [
    "# â¤ï¸ çŸ¥è¯†ç‚¹å°ç»“"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5b3b01-8f7f-4ade-a669-4c6c94ac1d2a",
   "metadata": {},
   "source": [
    "1. å¦‚æœè¦é›†æˆè‡ªå·±çš„å¤§æ¨¡å‹åˆ° langchain ï¼Œä» `BaseChatModel` ç»§æ‰¿æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹\n",
    "2. BaseChatModel è‡³å°‘è¦æ±‚ä½ å®ç° `_generate` æ–¹æ³•ï¼Œå¦‚æœè¡¥å…… `_stream` æ–¹æ³•ï¼Œå°±å¯ä»¥æä¾›åˆ°å…¨é¢èƒ½åŠ›\n",
    "3. `ChatPromptTemplate.from_messages` å¯ä»¥ä½¿ç”¨è¯­æ³•ç³–ï¼š system, humanï¼ˆæˆ–userï¼‰, aiï¼ˆæˆ–assistantï¼‰ç­‰\n",
    "4. å€¼å¾—è®°ä½å‡ ä¸ªä»æºç è§‚å¯Ÿåˆ°çš„å®ç”¨æ–¹æ³•ï¼š`get_graph().print_ascii()`ï¼Œ`input_schema.schema()` ç­‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8715a3f-1bc0-487c-a033-16abce0a313e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchani-book-py3.10-ipykernel",
   "language": "python",
   "name": "langchani-book-py3.10-ipykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
