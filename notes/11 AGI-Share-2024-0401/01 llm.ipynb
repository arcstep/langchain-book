{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e5b5362-cf90-4d8a-9c70-3db56e3f8dcc",
   "metadata": {},
   "source": [
    "# 🦜🔗 解读LangChain核心源码：关于LLM和Agent（上）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09652ad-45cf-4152-8794-2639a92a4ac2",
   "metadata": {},
   "source": [
    "# 课程开始"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bac26c1-1878-43d9-984d-bebacabccefb",
   "metadata": {},
   "source": [
    "## 这节课会带给你"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eb7b7a-d4a8-4e8d-b757-a55062c3fad8",
   "metadata": {},
   "source": [
    "- 🌹 一起阅读 Langchain 相关组件的源码，以便更好地阅读官方文档\n",
    "- 🌹 掌握 Langchain 文档中未曾提及、翻看源码才知晓的实用技巧\n",
    "- ✍️ 从零开始集成智谱大模型到 LangChain：解决智谱官方SDK不兼容的问题\n",
    "- ✍️ 按 LangChain 框架自定义一个智能体：再现《手撕AutoGPT》中的智能体\n",
    "- ✍️ 最终实践：再现《手撕AutoGPT》：langchain+自定义大模型+自定义智能体"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff511b-10e1-43d4-92e0-829a1fe4d5d2",
   "metadata": {},
   "source": [
    "## 开场闲聊"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554524d7-f9ce-4b70-be62-94ff88169a6e",
   "metadata": {},
   "source": [
    "### ❤️ Langchain 资源\n",
    "- [langchain源代码](https://github.com/langchain-ai/)：深度掌握必备的核心资源\n",
    "- [langchain官方文档](https://python.langchain.com/docs)：文档越来越好，与源代码相互印证\n",
    "- [langgraph example](https://github.com/langchain-ai/langgraph/tree/main/examples)：Jupyter 例子\n",
    "- [langchain AI](https://chat.langchain.com/?llm=anthropic_claude_2_1)：免费RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f539f3d-148f-4c18-a959-881dfc8ce4b7",
   "metadata": {},
   "source": [
    "### ❤️ Langchain 源码结构\n",
    "\n",
    "![](./langchain_ai.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab7f60d-7043-42e0-98c2-459355c52f19",
   "metadata": {},
   "source": [
    "### ❤️ LangChain 模块源码概览"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4481a6-d92d-4df6-8cfd-65e57c2463f5",
   "metadata": {},
   "source": [
    "[langchain模块源码结构](https://github.com/langchain-ai/langchain/tree/master/libs)\n",
    "\n",
    "| 源码位置 | 功能描述 |\n",
    "| :--- | :--- |\n",
    "| langchain/libs/langchain/langchain | 模块入口，会导入core、community等其他模块 |\n",
    "| langchain/libs/core/langchain_core | 核心组件和关键的基类实现 |\n",
    "| langchain/libs/partners/openai/langchain_openai | 合作伙伴（官方合作）组件 |\n",
    "| langchain/libs/community/langchain_community | 社区（非官方）组件 |\n",
    "| langchain/libs/experimental/langchain_experimental | 试验性功能（前沿探索组件，不对版本稳定做承诺） |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eff717-2f78-43a7-87c0-9d41f329206a",
   "metadata": {},
   "source": [
    "### 🌹 推荐阅读：从 Langchain 的最核心组件 Runnable 开始阅读源码\n",
    "\n",
    "[langchain_core/runnables/base.py#L104](https://github.com/langchain-ai/langchain/blob/ad77fa15eec4dae431171b7e8c13b8c1f9edec98/libs/core/langchain_core/runnables/base.py#L104)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8601bed1-2ed8-4582-95d5-b25a3568ba29",
   "metadata": {},
   "source": [
    "**（1）Runnable实用方法：**\n",
    "\n",
    "- Runnable\n",
    "    - assign()\n",
    "    - bind()\n",
    "    - with_config()\n",
    "    - get_name()\n",
    "    - get_graph()\n",
    "    - get_prompts()\n",
    "    - input_schema\n",
    "    - output_schema\n",
    "\n",
    "----\n",
    "**（2）RunnableSerializable实用方法：**\n",
    "\n",
    "- Runnable\n",
    "    - RunnableSerializable\n",
    "        - dumps()\n",
    "        - loads()\n",
    "\n",
    "----\n",
    "**（3）配置能力子类：**\n",
    "\n",
    "- Runnable\n",
    "    - RunnableSerializable\n",
    "        - RunnableBindingBase\n",
    "            - RunnableBinding（向Runnable实例传递参数）\n",
    "        - DynamicRunnable\n",
    "            - RunnableConfigurableFields\n",
    "            - RunnableConfigurableAlternatives\n",
    "\n",
    "----\n",
    "**（4）流程控制子类：**\n",
    "\n",
    "- Runnable\n",
    "    - RunnableSerializable\n",
    "        - RunnablePassthrough（传递额外输入）\n",
    "        - RunnableSequence（实现顺序执行，可以用重载的`|`符号或`RunnableSequence`来构造）\n",
    "        - RunnableParallel（实现并行执行，可以用`Dict`或`RunnableParallel`类来构造，别名`RunnableMap`）\n",
    "\n",
    "----\n",
    "**（5）大模型子类：**\n",
    "\n",
    "- Runnable\n",
    "    - RunnableSerializable\n",
    "        - BaseLanguageModel\n",
    "            - BaseLLM（派生其他大模型）\n",
    "                - BaseOpenAI（派生其他大模型）\n",
    "                    - OpenAI\n",
    "                - LLM\n",
    "                    - ...\n",
    "                - ...\n",
    "            - BaseChatModel\n",
    "                - ChatOpenAI\n",
    "                - ...\n",
    "\n",
    "----\n",
    "**（6）提示语子类：**\n",
    "- Runnable\n",
    "    - RunnableSerializable\n",
    "        - BasePromptTemplate `[Dict, PromptValue]`\n",
    "            - StringPromptTemplate（字符串模板）\n",
    "            - BaseChatPromptTemplate（对话模板）\n",
    "            - ImagePromptTemplate\n",
    "            - PipelinePromptTemplate\n",
    "\n",
    "----\n",
    "**（7）检索器子类：**\n",
    "- Runnable\n",
    "    - RunnableSerializable\n",
    "        - BaseRetriever `[RetrieverInput, RetrieverOutput]`（派生各类检索器）\n",
    "\n",
    "----\n",
    "**（8）Tool子类：**\n",
    "- Runnable\n",
    "    - RunnableSerializable\n",
    "        - BaseTool `[Union[str, Dict], Any]`（派生各类工具）\n",
    "\n",
    "----\n",
    "**（9）输出解析子类：**\n",
    "- Runnable\n",
    "    - RunnableSerializable\n",
    "        - BaseGenerationOutputParser `[Union[str, BaseMessage], T]`\n",
    "        - BaseOutputParser（派生各类输出解析）\n",
    "\n",
    "----\n",
    "**（10）输入赋值子类：**\n",
    "- Runnable\n",
    "    - RunnableSerializable\n",
    "        - RunnablePassthrough\n",
    "        - RunnableAssign\n",
    "        - RunnablePick\n",
    "\n",
    "----\n",
    "**（11）遗留Chain子类：**\n",
    "- Runnable\n",
    "    - RunnableSerializable\n",
    "        - Chain（结构化Runnable）\n",
    "            - AgentExecutor（执行智能体）\n",
    "\n",
    "----\n",
    "**（12）快速自定义Runnable的工具子类：**\n",
    "- Runnable（）\n",
    "    - RunnableGenerator（常用于处理输出可能是迭代器结果的chain）\n",
    "    - RunnableLambda（常用于包装普通函数，装饰函数@chain）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01548080-b79b-4a88-9b9f-7af2ef9ae567",
   "metadata": {},
   "source": [
    "### ❤️ 从源码中体会：LangChain 核心框架的三轮迭代"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc361bf-be03-4dbe-bf58-03f11e20fef8",
   "metadata": {},
   "source": [
    "- Runnable + Chain 时代\n",
    "- Runnable + LCEL 时代\n",
    "- Runnable + LCEL + Langgraph 时代"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34ff500-2591-4c74-80e2-c53d56877270",
   "metadata": {},
   "source": [
    "### ❤️ 我对 langchain 的个人观点\n",
    "\n",
    "1. langchain 一定是未来行业标准\n",
    "    - langchain 的架构解耦非常有效，在项目管理中价值不可估量\n",
    "    - 随着模型竞争、模型自训普及、多模态模型的发展，必须借助已有生态，langchain地位会越来越巩固\n",
    "2. langchain 已经可用，但有使用门槛（以下情况会持续很久）\n",
    "    - 🌹 因为生态发展快，所以**文档跟不上，需要看源码**\n",
    "    - ✍️ 很多组件仅能作为示范，所以**无法满足落地需求，需要自定义 langchain 组件**\n",
    "  \n",
    "**所以，今天的分享：**\n",
    "- 上半场：一起看源码，集成自己的大模型到 Langchain\n",
    "- 下半场：一起看源码，自定义智能体，再现《手撕AutoGPT》的推理过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e936103d-a47c-4455-81ca-10a8cbb8449a",
   "metadata": {},
   "source": [
    "# （一）解读源码，集成自己的大模型到 langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2eacc5-b64f-43dd-ab95-0afe81f827a6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>干货从这里开始！</b><br>\n",
    "    接下来的例子中，会穿插 langchian 源码解读。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4086972-8af4-41e8-9f1d-7d3a5e984bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载 .env 到环境变量\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a33f05-0ea9-427c-b851-6511e0b9fc29",
   "metadata": {},
   "source": [
    "## 1、大模型的一般用法回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f07493-0a57-4c60-a846-e6b1b69bfa46",
   "metadata": {},
   "source": [
    "### ✍️ 使用 OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56f9e2e4-782e-466b-be9d-c23b0c30242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "6c4db118-7b74-4d8e-8d9c-e8976ff4294e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "知道，「夏洛特烦恼」是一部2015年上映的中国电影，由沈腾、马丽等主演。该电影讲述了一个平凡上班族夏洛特在工作和生活中遇到的一系列烦恼和困扰，以幽默搞笑的方式展现了现代都市人的生活状态和情感困惑。这部电影在中国取得了巨大的票房成功，深受观众喜爱。\n"
     ]
    }
   ],
   "source": [
    "# invoke\n",
    "text = \"你知道「夏洛特烦恼」这部电影吗？\"\n",
    "response = llm.invoke(text)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "4ad948cb-8584-43f4-9dce-b008fa6cbc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|知|道|的|，|「|夏|洛|特|烦|恼|」|是|一|部|201|5|年|上|映|的|中国|电|影|，|由|沈|腾|、|马|丽|等|主|演|。|这|部|电|影|讲|述|了|一个|平|凡|的|小|人|物|夏|洛|特|在|生|活|中|遇|到|各|种|烦|恼|和|困|难|，|以|幽|默|搞|笑|的|方式|展|现|了|他|的|生|活|经|历|和|成|长|故|事|。|这|部|电|影|在|中国|取|得|了|很|高|的|票|房|和|口|碑|，|深|受|观|众|喜|爱|。||"
     ]
    }
   ],
   "source": [
    "# stream\n",
    "for chunk in llm.stream(text):\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d63eec-9f22-4f5b-94bf-40f721313ad1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>⚠️ 思考</b><br>\n",
    "    langchain 支持的8个方法都在什么场景下使用？\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68d2136-fe1d-4723-87d3-162309f07230",
   "metadata": {},
   "source": [
    "- invoke：标准化调度\n",
    "- batch: 标准化批量调度\n",
    "- stream：标准化流式输出\n",
    "- ainvoke / astream / abatch: 标准化异步调度\n",
    "- astream_log / astream_events: 从链、智能体、langgraph按照names、tags、events提取各环节的流式输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c803877e-8f15-4a3b-88a5-fcfdc5b1dea5",
   "metadata": {},
   "source": [
    "### ✍️ LCEL：LLM + Prompt + OutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "32dea0d5-8c5c-436d-b006-67b07eceae8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|知|道|，|夏|洛|特|烦|恼|是|一|部|中国|电|影|，|讲|述|了|一个|中|年|男|子|夏|洛|特|在|家|庭|和|工|作|中|面|临|各|种|困|扰|和|烦|恼|的|故|事|。|这|部|电|影|在|中国|取|得|了|巨|大|的|成功|，|成|为|了|一|部|经|典|的|喜|剧|作|品|。||"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"你知道{name}这部电影吗?\")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "for chunk in chain.stream({\"name\": \"夏洛特烦恼\"}):\n",
    "    print(chunk, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73527d37-fdb8-4d76-b70c-4ce1c209a3ed",
   "metadata": {},
   "source": [
    "## 2、简单的开始：实现「楼下邻居老大爷」AI大模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc945190-d75b-49de-94f5-801ee3125488",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>⚠️ 要深度 langchain 就必须阅读源码：</b><br>\n",
    "    像集成自己的大模型到 langchain 这样的任务，langchain 文档中基本未曾提及。<br>\n",
    "    因此，必须翻看源码才能搞清楚实现的机理。<br><br>\n",
    "    但 langchain 的🌹魅力🌹就在于：你可以参与community，或建立自己的community ！\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a3028e-da74-4247-af83-d76f03544797",
   "metadata": {},
   "source": [
    "### 🦜 需求分析：参考电影片段，把「非常有智慧的楼下邻居老大爷」变成 AI大模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e6fe6d-6fdd-49ea-88ce-496eddd23e5b",
   "metadata": {},
   "source": [
    "1. 使用大模型：模拟一个大模型\n",
    "2. 生成能力：提及马冬梅时生成打岔闲聊（马什么梅？马冬什么？什么冬梅？），其余生成“哦...“\n",
    "\n",
    "![马什么梅？马冬什么？什么冬梅？](./madongmei.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c0526e-50a9-481f-ae14-6433fc114d25",
   "metadata": {},
   "source": [
    "### ✍️ 基本实现：invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "52b2c9ba-39af-488b-af00-d272b7bff551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "from typing import Any, Dict, Iterator, List, Optional, Union\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import HumanMessage, HumanMessageChunk, AIMessage, AIMessageChunk, BaseMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "2ebdb44f-afd7-4ffc-9419-56e144a6d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatWithOlderAI(BaseChatModel):\n",
    "    \"\"\"模拟跟马冬梅楼下邻居老大爷的对话\"\"\"\n",
    "\n",
    "    # 必须实现\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"chat-with-neighber-older\"\n",
    "\n",
    "    # 必须实现\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        generations = [ChatGeneration(message=res) for res in self._ask_remote(messages)]\n",
    "        return ChatResult(generations=generations)\n",
    "\n",
    "    # 有老大爷的智慧计时周期\n",
    "    i: int = 0\n",
    "\n",
    "    # 访问远程大模型\n",
    "    # 当你需要实现自己的大模型时，主要是替换这部分\n",
    "    def _ask_remote(self, messages: List[BaseMessage]) -> List[BaseMessage]:\n",
    "        answers = [HumanMessage(m) for m in [\n",
    "            \"马什么梅？\",\n",
    "            \"什么冬梅？？\",\n",
    "            \"马东什么？？？\",\n",
    "        ]]\n",
    "        \n",
    "        if(re.search(\"马冬梅\", messages[0].content)):\n",
    "            response = answers[self.i]\n",
    "            self.i = (self.i + 1) if self.i < (len(answers) - 1) else 0\n",
    "        else:\n",
    "            response = AIMessage(\"哦...\")\n",
    "            \n",
    "        return [response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "9fb12783-b794-4ce2-8ace-4bd781be835e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "夏洛：大爷，楼上322住的是马冬梅家吗？\n",
      "大爷：马什么梅？|\n",
      "\n",
      "夏洛：马冬梅啊\n",
      "大爷：什么冬梅？？|\n",
      "\n",
      "夏洛：马冬梅！\n",
      "大爷：马东什么？？？|\n",
      "\n",
      "夏洛：我是说马冬梅！\n",
      "大爷：马什么梅？|\n",
      "\n",
      "夏洛：您歇着吧...\n",
      "大爷：哦...|"
     ]
    }
   ],
   "source": [
    "questions = [[HumanMessage(m)] for m in [\n",
    "    \"大爷，楼上322住的是马冬梅家吗？\",\n",
    "    \"马冬梅啊\",\n",
    "    \"马冬梅！\",\n",
    "    \"我是说马冬梅！\",\n",
    "    \"您歇着吧...\",\n",
    "]]\n",
    "\n",
    "llm = ChatWithOlderAI()\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n\\n夏洛：{question[0].content}\")\n",
    "    print(\"大爷：\", end=\"\")\n",
    "    # print(llm.invoke(question).content, end=\"\")\n",
    "    for chunk in llm.stream(question):\n",
    "        print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8231e5a-c8b9-4d8e-8e56-280ccc889fd7",
   "metadata": {},
   "source": [
    "### ✍️ 支持流式输出：stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "fe2633f0-9002-4684-818a-44daed91c706",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamChatWithOlderAI(ChatWithOlderAI):\n",
    "    \"\"\"模拟跟大爷的对话，支持流\"\"\"\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        *args,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGenerationChunk]:\n",
    "        response = self._ask_remote(messages)\n",
    "        for chunk in response[0].content:\n",
    "            time.sleep(0.1)\n",
    "            yield ChatGenerationChunk(message=AIMessageChunk(content=chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "236c1cf8-0886-423f-9710-cd10ef7c6e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "夏洛：大爷，楼上322住的是马冬梅家吗？\n",
      "大爷：马|什|么|梅|？|\n",
      "\n",
      "夏洛：马冬梅啊\n",
      "大爷：什|么|冬|梅|？|？|\n",
      "\n",
      "夏洛：马冬梅！\n",
      "大爷：马|东|什|么|？|？|？|\n",
      "\n",
      "夏洛：我是说马冬梅！\n",
      "大爷：马|什|么|梅|？|\n",
      "\n",
      "夏洛：您歇着吧...\n",
      "大爷：哦|.|.|.|"
     ]
    }
   ],
   "source": [
    "llm_stream = StreamChatWithOlderAI()\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n\\n夏洛：{question[0].content}\")\n",
    "    print(\"大爷：\", end=\"\")\n",
    "    for chunk in llm_stream.stream(question):\n",
    "        print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaa2f9d-2177-4c7e-b935-a947b675ee9a",
   "metadata": {},
   "source": [
    "### ✍️  LCEL：Prompt + LLM + OutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "1a7d4000-66db-4be9-9dae-e9ab7e14cbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "夏洛：楼上322住的是马冬梅家吗？\n",
      "大爷：马|什|么|梅|？|\n",
      "\n",
      "夏洛：马冬梅啊\n",
      "大爷：什|么|冬|梅|？|？|\n",
      "\n",
      "夏洛：马冬梅！\n",
      "大爷：马|东|什|么|？|？|？|\n",
      "\n",
      "夏洛：我是说马冬梅！\n",
      "大爷：马|什|么|梅|？|\n",
      "\n",
      "夏洛：大爷您歇着吧...\n",
      "大爷：哦|.|.|.|"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "questions_text = [\n",
    "    \"楼上322住的是马冬梅家吗？\",\n",
    "    \"马冬梅啊\",\n",
    "    \"马冬梅！\",\n",
    "    \"我是说马冬梅！\",\n",
    "    \"大爷您歇着吧...\"\n",
    "]\n",
    "\n",
    "llm_stream = StreamChatWithOlderAI()\n",
    "prompt = PromptTemplate.from_template(\"大爷，{question}\")\n",
    "chain = prompt | llm_stream | StrOutputParser()\n",
    "\n",
    "for question in questions_text:\n",
    "    print(f\"\\n\\n夏洛：{question}\")\n",
    "    print(\"大爷：\", end=\"\")\n",
    "    for chunk in chain.stream({\"question\": question}):\n",
    "        print(chunk, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae412a-cfd5-4160-80df-a6265560cd34",
   "metadata": {},
   "source": [
    "### 🌹 审视链上各节点的输入输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "4aeecb41-c94c-4046-817f-8642d0ecb937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StreamChatWithOlderAI |  \n",
      "+-----------------------+  \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "3c055383-7ea1-4db2-b638-82d93df772d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], template='大爷，{question}')"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c05f8e-d9e2-4a26-9558-aaa2dbcc8ad9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# llm_stream.input_schema.schema()\n",
    "# chain.input_schema.schema()\n",
    "# prompt.output_schema.schema()\n",
    "# prompt.invoke({\"question\":\"你好\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08bc9d3-ecbc-4327-99bd-0d1cb97389f2",
   "metadata": {},
   "source": [
    "### 🌹 阅读源码：实现 langchain 大模型的原理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd62fc73-8277-4bea-97f6-d79a08dd64e9",
   "metadata": {},
   "source": [
    "#### （1）BaseLanguageModel\n",
    "\n",
    "来自：[langchain_core/language_models/base.py](https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/language_models/base.py#L74-L81)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31c2efe-c70d-4616-8b99-337f4ac4ea53",
   "metadata": {},
   "source": [
    "```python\n",
    "class BaseLanguageModel(\n",
    "    RunnableSerializable[LanguageModelInput, LanguageModelOutputVar], ABC\n",
    "):\n",
    "    \"\"\"Abstract base class for interfacing with language models.\n",
    "\n",
    "    All language model wrappers inherit from BaseLanguageModel.\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c54270-3787-4248-85d7-fce89e39bbe6",
   "metadata": {},
   "source": [
    "#### （2）BaseChatModel\n",
    "\n",
    "来自：[langchain_core/language_models/chat_models.py](https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/language_models/chat_models.py#L100)\n",
    "\n",
    "**核心逻辑：**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736c153a-fc04-47bb-96d1-8a57741f1de1",
   "metadata": {},
   "source": [
    "```python\n",
    "class BaseChatModel(BaseLanguageModel[BaseMessage], ABC):\n",
    "    \"\"\"Base class for Chat models.\"\"\"\n",
    "\n",
    "    ...\n",
    "\n",
    "    def invoke(...) -> BaseMessage:\n",
    "        # ...\n",
    "        self.generate_prompt(...)\n",
    "        # generate_prompt >> generate >> _generate\n",
    "\n",
    "    def ainvoke(...) -> BaseMessage:\n",
    "        # ...\n",
    "        self.agenerate_prompt(...)\n",
    "        # agenerate_prompt >> agenerate >> _agenerate >> _generate\n",
    "    \n",
    "    def stream(...) -> Iterator[BaseMessageChunk]:\n",
    "        # ...\n",
    "        if type(self)._stream == BaseChatModel._stream:\n",
    "            # model doesn't implement streaming, so use default implementation\n",
    "            yield cast(\n",
    "                BaseMessageChunk, self.invoke(input, config=config, stop=stop, **kwargs)\n",
    "            )\n",
    "        # ...\n",
    "\n",
    "    async def astream(...) -> AsyncIterator[BaseMessageChunk]:\n",
    "        # 在#19332合并中，_astream方法实现已经被简化\n",
    "        # https://github.com/langchain-ai/langchain/pull/19332/commits/afbe6ac659e41ab5f4a6f4dcaa33511e9e59e4d5\n",
    "        if (\n",
    "            type(self)._astream is BaseChatModel._astream\n",
    "            and type(self)._stream is BaseChatModel._stream\n",
    "        ):\n",
    "            # No async or sync stream is implemented, so fall back to ainvoke\n",
    "            yield cast(\n",
    "                BaseMessageChunk,\n",
    "                await self.ainvoke(input, config=config, stop=stop, **kwargs),\n",
    "            )\n",
    "        # ...\n",
    "\n",
    "    # bacth, abatch, astream_log, astream_events \n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28df1cf1-2efe-4bf0-8f6b-4bd752084f9f",
   "metadata": {},
   "source": [
    "**必须实现的部分：**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa90a6e-8255-4e6f-9876-2f4cf16d2041",
   "metadata": {},
   "source": [
    "```python\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"chat-with-neighber-older\"\n",
    "        \n",
    "    ## ******** invoke / ainvoke / batch / abatch **********\n",
    "    @abstractmethod\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        \"\"\"Top Level call\"\"\"\n",
    "\n",
    "    ## ******** stream / astream / astream_log / astream_events **********\n",
    "    def _stream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGenerationChunk]:\n",
    "        raise NotImplementedError()        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78830db-43ef-4dc4-b5cb-c0a83ac4e3ef",
   "metadata": {},
   "source": [
    "### 🌹 推荐阅读：langchain 内置实现的 FakeLLM 源码\n",
    "\n",
    "- [FakeListLLM](https://github.com/langchain-ai/langchain/blob/8595c3ab59371d9932310eb12a2c3220afe3ba84/libs/core/langchain_core/language_models/fake.py#L14)\n",
    "- [FakeStreamingListLLM](https://github.com/langchain-ai/langchain/blob/8595c3ab59371d9932310eb12a2c3220afe3ba84/libs/core/langchain_core/language_models/fake.py#L61)\n",
    "- [FakeMessagesListChatModel](https://github.com/langchain-ai/langchain/blob/8595c3ab59371d9932310eb12a2c3220afe3ba84/libs/core/langchain_core/language_models/fake_chat_models.py#L16)\n",
    "- [FakeChatModel](https://github.com/langchain-ai/langchain/blob/8595c3ab59371d9932310eb12a2c3220afe3ba84/libs/core/langchain_core/language_models/fake_chat_models.py#L120C7-L120C20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f664542-8ea0-4cba-ba61-735c0fea6b04",
   "metadata": {},
   "source": [
    "## 3、集成智谱大模型到 Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9e4176-391c-41a4-bc21-988f7b295843",
   "metadata": {},
   "source": [
    "### 🦜 需求分析：结合智谱官方文档，编写ChatZhipuAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ff16f8-4f04-4639-9cbb-5aead5c88076",
   "metadata": {},
   "source": [
    "- [智谱AI官方的Python接口文档](https://maas.aminer.cn/dev/api#sdk)\n",
    "- [Langchain中已有的智谱AI组件（旧版本）](https://python.langchain.com/docs/integrations/chat/zhipuai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859d217d-27e3-4802-9ea6-e5d585a2bd01",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>⚠️ 智谱AI遇到的尴尬：</b><br>\n",
    "    因为 pydantic 版本兼容的问题，智谱官方SDK升级到4.0之后, Langchain相应的包一直没有更新。<br>\n",
    "    截止到2024年3月29日，仍然不可用（如果突然可用了也不要紧，本课内容原本就是抛转引玉）\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b7ebf3-ec67-4cbc-8c13-3cba71d7461f",
   "metadata": {},
   "source": [
    "#### （1）智谱官方可用接口\n",
    "- 直接调用：可实现 invoke\n",
    "- 异步调用（先调用，再查询结果）：适合实现 ainvoke / batch / abatch\n",
    "- 流式调用（SSE，Server-Send Events)：适合实现 stream / astream / stream_log / stream_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf4066-d16d-4307-93d5-d54f56cdd3a2",
   "metadata": {},
   "source": [
    "#### （2）智谱官方支持能力\n",
    "- 支持工具：本地工具回调 / 云上检索 / 云上互联网搜索\n",
    "- 支持识图\n",
    "- 支持生图"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f4f39d-46f4-401f-93c3-b6a59a687754",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>⚠️ 思考：</b><br>\n",
    "    能否实现一个自定义大模型，支持多模态能力？<br>\n",
    "    input: List[Union[文字, 图像]] -> Union[文字, 图像]\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dbbbff-1483-487a-8648-141b5f64fc99",
   "metadata": {},
   "source": [
    "#### （3）智谱官方速率限制\n",
    "\n",
    "[智谱AI官方文档中对速率限制的说明](https://maas.aminer.cn/dev/howuse/rate-limits/why?tab=5)：\n",
    "\n",
    "> 当前我们基于用户的月度 API 调用消耗金额情况将速率控制分为6种等级。\n",
    ">\n",
    "> 消耗金额选取逻辑：我们会选取用户当前月份1号～t-1日的调用 API 推理消耗总金额和用户上个月的 API 调用消耗总金额做比较，取更高金额作为用户当前的 API 消耗金额。\n",
    ">\n",
    "> 特别的，若您从未曾付费充值/购买过资源包，则会归为免费级别。\n",
    "\n",
    "**整理GLM4模型使用限制如下：**\n",
    "|用户等级|使用量|GLM4并发限制|\n",
    "|:---|:---|:---|\n",
    "|免费|api调用消耗0元-50元/每月（不含）|5|\n",
    "|使用量1|api调用消耗50元-500元/每月（不含）|10|\n",
    "|使用量2|api调用消耗500元-5000元/每月（不含）|20|\n",
    "|使用量3|api调用消耗5000元-10000元/每月（不含）|30|\n",
    "|使用量4|api调用消耗10000元-30000元/每月（不含）|100|\n",
    "|使用量5|api调用消耗30000元以上/每月|200|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3941a68e-e6de-40f9-8ecb-610556b92296",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>⚠️ 思考：</b><br>\n",
    "    能否实现一个自定义大模型，通过多个低用量账户池的管理机制来提高可用的调用速率？<br>\n",
    "    速率限制属于服务端降级，调用时自动按照速率限制排队，影响用户体验。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e18cd-e44e-4f28-a8a2-819f719c4daf",
   "metadata": {},
   "source": [
    "### ✍️ 测试官方例子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c317160f-22fe-4210-bc69-ed83abacfeca",
   "metadata": {},
   "source": [
    "看官方例子：[https://github.com/MetaGLM/zhipuai-sdk-python-v4](https://github.com/MetaGLM/zhipuai-sdk-python-v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dad0716-95a2-4966-bf90-07d33be54cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai import ZhipuAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "9681792b-ab7d-4269-b9d3-8f55a25ce0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='你好，我叫智谱清言，是基于智谱AI公司于2023年训练的ChatGLM开发的。很高兴见到你，欢迎问我任何问题。' role='assistant' tool_calls=None\n"
     ]
    }
   ],
   "source": [
    "## 看看官方的例子是否能正确运行\n",
    "client = ZhipuAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"你叫什么名字\"},\n",
    "    ],\n",
    ")\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9835dc40-1269-4415-8127-70f62f8ea0a0",
   "metadata": {},
   "source": [
    "### ✍️ 包装为一个函数调用（假装我不想直接用 langchain）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "9c782a4d-663b-423c-8cd7-3d63a7893e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is your name?'"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask_zhipu(question: str) -> str:\n",
    "    client = ZhipuAI()\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一个翻译机器人，我说中文你就直接翻译成英文，我说英文你就直接翻译为中文。不要输出其他，不要啰嗦。\"},\n",
    "        {\"role\": \"user\", \"content\": \"你好\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"hello\"},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    return(response.choices[0].message.content)\n",
    "\n",
    "ask_zhipu(\"你叫什么名字？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8f6f0e-3891-4a30-91a8-afcfd2cf5d08",
   "metadata": {},
   "source": [
    "### ✍️ 支持与 Prompt 协作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d65800-bc21-489a-9dd9-84b50b8e1be4",
   "metadata": {},
   "source": [
    "#### （1）能否实现如下场景？（似乎使用 langchain 也没那么坏）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2572d41a-b2a6-4b81-9426-f083081f86ac",
   "metadata": {},
   "source": [
    "```python\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "chain.invoke({\"question\": \"你叫什么名字？\"})\n",
    "```\n",
    "\n",
    "基本思路：\n",
    "\n",
    "Prompt输入（按langchain标准） <br>\n",
    "⬇️ <br>\n",
    "Prompt输出（按langchain标准） <br>\n",
    "⬇️ <br>\n",
    "LLM输入（按大模型标准） <br>\n",
    "⬇️ <br>\n",
    "LLM输出（按langchain标准）\n",
    "⬇️ <br>\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b5a333-e3e2-493b-8525-baf37a2f11ac",
   "metadata": {},
   "source": [
    "#### （2）构造 Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd50c3d-5a21-4d6b-96e2-76a2e5ae5566",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>⚠️ 思考</b><br>\n",
    "    下面代码中， 为什么 from_messages 支持 system、human、ai 这些名字？还有其他名字吗？\n",
    "</div>\n",
    "\n",
    "**🌞 提示：**\n",
    "- 文档中没有，要看源码：[langchain_core/messages/utils.py](https://github.com/langchain-ai/langchain/blob/c93d4ea91cfcf55dfe871931d42aa22562f8dae2/libs/core/langchain_core/messages/utils.py#L130-L168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "024270e5-a030-421f-8af7-af64c1251dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='你是一个翻译机器人，我说中文你就直接翻译成英文，我说英文你就直接翻译为中文。不要输出其他，不要啰嗦。'), HumanMessage(content='你好'), AIMessage(content='hello'), HumanMessage(content='你叫什名字？')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个翻译机器人，我说中文你就直接翻译成英文，我说英文你就直接翻译为中文。不要输出其他，不要啰嗦。\"),\n",
    "    (\"human\", \"你好\"),\n",
    "    (\"ai\", \"hello\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "prompt.invoke({\"question\":\"你叫什名字？\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d90cc73-c09f-44c0-92bf-4bee04489894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='你是一个翻译机器人，我说中文你就直接翻译成英文，我说英文你就直接翻译为中文。不要输出其他，不要啰嗦。'),\n",
       " HumanMessage(content='你好'),\n",
       " AIMessage(content='hello'),\n",
       " HumanMessage(content='你叫什名字？')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"question\":\"你叫什名字？\"}).to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae249420-7b4a-453e-be1a-47dbbde27a05",
   "metadata": {},
   "source": [
    "#### （3）从 Prompt 输出格式，转换到大模型的输入格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a48adc15-0507-4950-bb27-f034e5a5ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.adapters.openai import convert_message_to_dict\n",
    "from langchain_core.messages import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a24c1316-3741-49a7-85da-806b5fcd2e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '你是一个翻译机器人，我说中文你就直接翻译成英文，我说英文你就直接翻译为中文。不要输出其他，不要啰嗦。'},\n",
       " {'role': 'user', 'content': '你好'},\n",
       " {'role': 'assistant', 'content': 'hello'},\n",
       " {'role': 'user', 'content': '你叫什名字？'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[convert_message_to_dict(m) for m in prompt.invoke({\"question\":\"你叫什名字？\"}).to_messages()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7648a6-6ab7-4a62-81b3-d0b02c553ea2",
   "metadata": {},
   "source": [
    "### ✍️ 直接用 RunnableLambda 达到目的（假装我不想用 langchain 的大模型基类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63e5a0a3-433d-48e7-950b-fc3d8a445e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain\n",
    "from typing import List\n",
    "from langchain_core.prompt_values import ChatPromptValue \n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "@chain\n",
    "def ask_zhipu(promptValue: ChatPromptValue) -> AIMessage:\n",
    "    client = ZhipuAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4\",\n",
    "        messages=[convert_message_to_dict(m) for m in promptValue.to_messages()],\n",
    "    )\n",
    "    return(AIMessage(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "367b9894-8a00-4913-b542-ed0f6e73ec91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is your name?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | ask_zhipu | StrOutputParser()\n",
    "chain.invoke({\"question\": \"你叫什名字？\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41dae6d9-9301-4c56-bc78-96f1ad2f758c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "  +--------------------+   \n",
      "  | ChatPromptTemplate |   \n",
      "  +--------------------+   \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "  +-------------------+    \n",
      "  | Lambda(ask_zhipu) |    \n",
      "  +-------------------+    \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "# 看看当前链的结构\n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa150d60-9630-4597-bd2f-16524238f899",
   "metadata": {},
   "source": [
    "### ✍️ 基于 BaseChatModel 达到目的（假装我开始想获得 LCEL 的诸多好处）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67137baa-a298-4530-bcbb-f88f5fa4077c",
   "metadata": {},
   "source": [
    "#### （1）从大模型的输出格式，转换到 langchain 的标准输出格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d46fb809-bfbd-4035-9de4-0a1d52a6da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.adapters.openai import convert_dict_to_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6daa2ea-e6a4-4289-a9c7-8707489433ef",
   "metadata": {},
   "source": [
    "#### （2）实现支持 invoke 的版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0374ab1-7121-4513-869e-bbc5ab56302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from typing import Any, Dict, Iterator, List, Optional, cast, Mapping\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be2ac9a7-62e2-45c8-a0b5-5b0d97824ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniZhipuAI(BaseChatModel):\n",
    "    \"\"\"支持最新的智谱API\"\"\"\n",
    "\n",
    "    client: Optional[ZhipuAI] = None\n",
    "\n",
    "    def __init__(self, *args: Any, **kwargs: Any) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.client = ZhipuAI()\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return the type of chat model.\"\"\"\n",
    "        return \"zhipuai\"\n",
    "\n",
    "    def _ask_remote(self, messages, streaming=False, **kwargs):\n",
    "        # 从langchain消息格式，转换到智谱AI输入的格式\n",
    "        dict_zhipu = [convert_message_to_dict(m) for m in messages]\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"glm-4\",\n",
    "            messages=dict_zhipu,\n",
    "            stream=streaming,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # 从智谱AI输出的格式，转换到langchain的消息格式\n",
    "        if not isinstance(response, dict):\n",
    "            response = response.dict()\n",
    "        return [convert_dict_to_message(c[\"message\"]) for c in response[\"choices\"]]\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        stream: Optional[bool] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        \"\"\"实现 ZhiputAI 的同步调用\"\"\"\n",
    "\n",
    "        # 问智谱AI，并得到回复\n",
    "        responses = self._ask_remote(messages, streaming=False, **kwargs)\n",
    "\n",
    "        return ChatResult(\n",
    "            generations=[ChatGeneration(message=m) for m in responses]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb34bc36-229e-4c44-a02f-f6537a89a67b",
   "metadata": {},
   "source": [
    "#### （3）用起来！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fee54840-3dfb-4d6d-ae4b-6a07ba3b2d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='中美在人工智能领域的竞争非常激烈。中国能赶上吗？')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个中英互译机器人，只负责翻译，不要试图对问题做解答。我说中文你就直接翻译成英文，我说英文你就直接翻译为中文。不要输出其他，不要啰嗦。\"),\n",
    "    (\"human\", \"你好\"),\n",
    "    (\"ai\", \"hello\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "llm_zhipu = MiniZhipuAI()\n",
    "chain = prompt | llm_zhipu\n",
    "\n",
    "chain.invoke({\"question\": \"The competition between China and the United States in the AI field is very intense. Can China catch up?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7467f98a-f6c6-4443-b0ef-ef906cf26111",
   "metadata": {},
   "source": [
    "### ✍️ 尝试在智能体中使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f853191a-48d5-4385-a79b-d1ebaf4d9c69",
   "metadata": {},
   "source": [
    "#### （1）定义一个简单工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2adeaac3-3dd6-4a5e-9591-7da1b31660c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool, convert_to_openai_function\n",
    "import re\n",
    "\n",
    "@tool\n",
    "def ask_neighber(query: str) -> str:\n",
    "    \"\"\"我是马冬梅的邻居老大爷，关于她的事情你可以问我\"\"\"\n",
    "    if(re.search(\"马冬梅\", query)):\n",
    "        return \"楼上322\"\n",
    "    else:\n",
    "        return \"我不清楚\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c272f4-0dca-4499-afce-3a91c93d413a",
   "metadata": {},
   "source": [
    "#### （2）作为 openai 风格的回调工具使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98d510aa-b493-4920-968b-909cfdfd2933",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_zhipu = MiniZhipuAI().bind(tools=[convert_to_openai_tool(ask_neighber)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52acb9b5-9948-4de6-b364-c1da895d1a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_8516976669838980971', 'function': {'arguments': '{\"query\":\"马冬梅\"}', 'name': 'ask_neighber'}, 'type': 'function'}]})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_zhipu.invoke(\"告诉我马冬梅在哪个房间？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9eb8a61d-573c-468d-b9df-62516c521e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'楼上322'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_neighber.invoke({\"query\":\"马冬梅\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7486f46b-8c36-49ba-aec8-5340206443fe",
   "metadata": {},
   "source": [
    "#### （3）集成到智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5c09ac0d-f981-4c50-a2c7-fc41f082618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, Tool, create_openai_tools_agent\n",
    "from langchain import hub\n",
    "\n",
    "def create_neighber(llm):\n",
    "    tools = [ask_neighber]\n",
    "    prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "    return AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc382cdb-9ffc-40fa-a16c-41b5eb371c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `ask_neighber` with `{'query': '马冬梅住哪里'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m楼上322\u001b[0m\u001b[32;1m\u001b[1;3m根据我的查询结果，马冬梅住在楼上322房间。希望这个信息对您有所帮助。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '马冬梅住哪里', 'output': '根据我的查询结果，马冬梅住在楼上322房间。希望这个信息对您有所帮助。'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_neighber(MiniZhipuAI()).invoke({\"input\":\"马冬梅住哪里\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77770d1-c8fa-4967-8142-e91d012652e9",
   "metadata": {},
   "source": [
    "### ✍️ 现在可以轻松切换智能体中的大模型（假装我对 langchian 很满意！👍👍👍）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eb52bc-0c67-40e7-9bef-d97680e21614",
   "metadata": {},
   "source": [
    "#### （1）使用 langchain_zhipu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ff552d-78cd-41c0-a302-b9a695c2887d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>⚠️ 思考：</b><br>\n",
    "    上面代码已经相对完整实现了一个 langchain 大模型；但缺少很多细节控制，可以尝试自己动手添加！    \n",
    "</div>\n",
    "\n",
    "**🌞 参考：**\n",
    "- [查看 langchain_zhpu 中的实现源码](https://github.com/arcstep/langchain_zhipuai/blob/e55af13eed673bc409ffdb143030e6cc0b2af27c/langchain_zhipu/chat.py#L304-L354) [![PyPI version](https://img.shields.io/pypi/v/langchain_zhipu.svg)](https://pypi.org/project/langchain_zhipu/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91fddecd-fd0c-41f7-b262-114a36d54145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `ask_neighber` with `{'query': '马冬梅住哪里'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m楼上322\u001b[0m\u001b[32;1m\u001b[1;3m根据我的查询结果，马冬梅住在楼上322房间。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '马冬梅住哪里', 'output': '根据我的查询结果，马冬梅住在楼上322房间。'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_zhipu import ChatZhipuAI\n",
    "\n",
    "create_neighber(ChatZhipuAI()).invoke({\"input\":\"马冬梅住哪里\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299ea67a-56ae-4b60-a2be-6a7021c973eb",
   "metadata": {},
   "source": [
    "#### （2）使用 langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb69b8-33a7-4f1e-9ac5-0e41972241d1",
   "metadata": {},
   "source": [
    "**这与直接使用OpenAI类似：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc557ddb-fa06-4fbf-90da-d9418123dab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `ask_neighber` with `{'query': '马冬梅'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m楼上322\u001b[0m\u001b[32;1m\u001b[1;3m马冬梅住在楼上322房间。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '马冬梅住哪里', 'output': '马冬梅住在楼上322房间。'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "create_neighber(ChatOpenAI()).invoke({\"input\":\"马冬梅住哪里\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1160eb50-4574-46fa-b9d4-aeaa0c773a6e",
   "metadata": {},
   "source": [
    "# ❤️ 知识点小结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5b3b01-8f7f-4ade-a669-4c6c94ac1d2a",
   "metadata": {},
   "source": [
    "1. 如果要集成自己的大模型到 langchain ，从 `BaseChatModel` 继承是一个很好的起点\n",
    "2. BaseChatModel 至少要求你实现 `_generate` 方法，如果补充 `_stream` 方法，就可以提供到全面能力\n",
    "3. `ChatPromptTemplate.from_messages` 可以使用语法糖： system, human（或user）, ai（或assistant）等\n",
    "4. 值得记住几个从源码观察到的实用方法：`get_graph().print_ascii()`，`input_schema.schema()` 等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8715a3f-1bc0-487c-a033-16abce0a313e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchani-book-py3.10-ipykernel",
   "language": "python",
   "name": "langchani-book-py3.10-ipykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
