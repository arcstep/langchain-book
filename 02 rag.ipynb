{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faeaf030-e07f-4fa9-a140-1aeb881b4d3f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef551542-90b1-432f-85ed-2557494fc2ea",
   "metadata": {},
   "source": [
    "## ç¯å¢ƒå‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b460fd58-7b4f-479b-abd2-933e76287fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNABLE_BASE_URL:  http://localhost:8000\n"
     ]
    }
   ],
   "source": [
    "from Utils import gpt35, gpt4, tongyi, langchain_docs_extractor, WebPageObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb5d29-00d9-48b9-8a31-4995d7e9e060",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt35(\"\"\"æˆ‘åœ¨jupyterlabä¸­æ— æ³•åŠ è½½ä¿®æ”¹è¿‡çš„pythonä»£ç ï¼Œæ˜¯æœ‰ç¼“å­˜å—ï¼Ÿ\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb313ba6-1144-4c87-8462-00da91015303",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## ä»langchainå®˜ç½‘æ”¶é›†æ–‡æ¡£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cb8302-9e18-403a-a973-036b24878a89",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>å…¼å®¹æ€§é—®é¢˜ï¼š</b><br/>\n",
    "    è¾ƒæ–°çš„BeautifulSoupç‰ˆæœ¬æ˜¯4.12.3ï¼Œä¸python3.10å…¼å®¹æ€§è¾ƒå¥½ï¼Œæ— æ³•é€‚åº”3.9æˆ–3.12ï¼Œå¦åˆ™æ— æ³•æ‰¾åˆ°lxmlæˆ–html5libã€‚\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e26cc43-dbdb-4266-9251-684d775c8b11",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from langchain_community.document_loaders.sitemap import SitemapLoader\n",
    "from langchain_core.utils.html import PREFIXES_TO_IGNORE_REGEX, SUFFIXES_TO_IGNORE_REGEX\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf0df822-c1c8-4dc5-a470-35369773b093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»…åœ¨jupyterä¸­éœ€è¦\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312d6df7-3d83-4395-bd2e-daf3bd2ab434",
   "metadata": {},
   "source": [
    "### æå–langchainçš„Docsæ–‡æ¡£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fdca71b-7659-40d6-b883-dad693020da3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def metadata_extractor(meta: dict, soup: BeautifulSoup) -> dict:\n",
    "    title = soup.find(\"title\")\n",
    "    description = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "    html = soup.find(\"html\")\n",
    "    return {\n",
    "        \"source\": meta[\"loc\"],\n",
    "        \"title\": title.get_text() if title else \"\",\n",
    "        \"description\": description.get(\"content\", \"\") if description else \"\",\n",
    "        \"language\": html.get(\"lang\", \"\") if html else \"\",\n",
    "        **meta,\n",
    "    }\n",
    "\n",
    "def load_langchain_docs():\n",
    "    return SitemapLoader(\n",
    "        \"https://python.langchain.com/sitemap.xml\",\n",
    "        filter_urls=[\"https://python.langchain.com/\"],\n",
    "        parsing_function=langchain_docs_extractor,\n",
    "        default_parser=\"lxml\",\n",
    "        bs_kwargs={\n",
    "            \"parse_only\": SoupStrainer(\n",
    "                name=(\"article\", \"title\", \"html\", \"lang\", \"content\")\n",
    "            ),\n",
    "        },\n",
    "        meta_function=metadata_extractor,\n",
    "    ).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a198ccb-c59b-4dff-bf12-d6cecb052787",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 1180/1180 [07:51<00:00,  2.50it/s]\n"
     ]
    }
   ],
   "source": [
    "langchain_docs = load_langchain_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdee01e-b9d3-412c-9b89-3adc3951fd0d",
   "metadata": {},
   "source": [
    "### æå–langchainçš„APIæ–‡æ¡£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99d61a25-ef1c-4676-87e1-0dd41f7f2097",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simple_extractor(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n",
    "\n",
    "def load_api_docs():\n",
    "    return RecursiveUrlLoader(\n",
    "        url=\"https://api.python.langchain.com/en/stable/langchain_api_reference.html\",\n",
    "        max_depth=8,\n",
    "        extractor=simple_extractor,\n",
    "        prevent_outside=True,\n",
    "        use_async=True,\n",
    "        timeout=600,\n",
    "        # Drop trailing / to avoid duplicate pages.\n",
    "        link_regex=(\n",
    "            f\"href=[\\\"']{PREFIXES_TO_IGNORE_REGEX}((?:{SUFFIXES_TO_IGNORE_REGEX}.)*?)\"\n",
    "            r\"(?:[\\#'\\\"]|\\/[\\#'\\\"])\"\n",
    "        ),\n",
    "        check_response_status=True,\n",
    "        exclude_dirs=(\n",
    "            \"https://api.python.langchain.com/en/latest/_sources\",\n",
    "            \"https://api.python.langchain.com/en/latest/_modules\",\n",
    "        ),\n",
    "    ).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8b1d7d8-ba21-469a-b0ac-328312eae2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_docs = load_api_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42af4439-a487-49dc-8e7b-45563f0c7602",
   "metadata": {},
   "source": [
    "### æå–langsmithçš„docsæ–‡æ¡£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b305d2d1-3d36-4319-8ec0-1944ffa4219d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_langsmith_docs():\n",
    "    return RecursiveUrlLoader(\n",
    "        url=\"https://docs.smith.langchain.com/\",\n",
    "        max_depth=8,\n",
    "        extractor=simple_extractor,\n",
    "        prevent_outside=True,\n",
    "        use_async=True,\n",
    "        timeout=600,\n",
    "        # Drop trailing / to avoid duplicate pages.\n",
    "        link_regex=(\n",
    "            f\"href=[\\\"']{PREFIXES_TO_IGNORE_REGEX}((?:{SUFFIXES_TO_IGNORE_REGEX}.)*?)\"\n",
    "            r\"(?:[\\#'\\\"]|\\/[\\#'\\\"])\"\n",
    "        ),\n",
    "        check_response_status=True,\n",
    "    ).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba2064c4-f308-4c20-89b3-5a8be285a73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f5/rlf27f4n6wzc_k4x7y4vzm5h0000gn/T/ipykernel_31512/320920142.py:2: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  soup = BeautifulSoup(html, \"lxml\")\n",
      "/Users/xuehongwei/.pyenv/versions/3.10.0/lib/python3.10/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    }
   ],
   "source": [
    "langsmith_docs = load_langsmith_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63aaad0-3196-4956-bd7f-386543e23813",
   "metadata": {},
   "source": [
    "### å…¥åº“åˆ°duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4af2dc8-dde3-4c4f-bcd9-678bfdcf6941",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_store = WebPageObj(db_name = \"data/web_pages.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e572f490-9632-4727-939d-5c3f73719468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"
     ]
    }
   ],
   "source": [
    "# https://python.langchain.com/\n",
    "for d in langchain_docs:\n",
    "    print(\".\", end = \"\")\n",
    "    web_store.upsert(d, topic = \"langchain_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a1c8d9c-30e0-434c-9f58-80f542ae3ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_store.upsert(api_docs[0], topic = \"langchain_api_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5602f86f-3f70-47a8-81c1-6645e668cea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................."
     ]
    }
   ],
   "source": [
    "# https://docs.smith.langchain.com/\n",
    "for d in langsmith_docs:\n",
    "    print(\".\", end = \"\")\n",
    "    web_store.upsert(d, topic = \"langsmith_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ec75586-dc56-4773-8469-88390d8c3088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WebPage(source='https://api.python.langchain.com/en/stable/langchain_api_reference.html', topic='langchain_api_docs', title='langchain 0.1.4 â€” ğŸ¦œğŸ”— LangChain 0.1.4', description='', language='en', loc='', changefreq='', priority='', page_content='langchain 0.1.4 â€” ğŸ¦œğŸ”— LangChain 0.1.4\\n\\nLangChain\\n\\nCore\\n\\nCommunity\\n\\nExperimental\\n\\ngoogle-vertexai\\n\\nrobocorp\\n\\ngoogle-genai\\n\\nanthropic\\n\\nnvidia-trt\\n\\nopenai\\n\\nmistralai\\n\\ntogether\\n\\nnvidia-ai-endpoints\\n\\nexa\\n\\nPartner libs\\n\\ngoogle-vertexai\\nrobocorp\\ngoogle-genai\\nanthropic\\nnvidia-trt\\nopenai\\nmistralai\\ntogether\\nnvidia-ai-endpoints\\nexa\\n\\nDocs\\n\\nToggle Menu\\n\\nPrev\\nUp\\nNext\\n\\nlangchain 0.1.4\\nlangchain.agents\\nClasses\\nFunctions\\n\\nlangchain.callbacks\\nClasses\\n\\nlangchain.chains\\nClasses\\nFunctions\\n\\nlangchain.embeddings\\nClasses\\nFunctions\\n\\nlangchain.evaluation\\nClasses\\nFunctions\\n\\nlangchain.hub\\nFunctions\\n\\nlangchain.indexes\\nClasses\\nFunctions\\n\\nlangchain.memory\\nClasses\\nFunctions\\n\\nlangchain.model_laboratory\\nClasses\\n\\nlangchain.output_parsers\\nClasses\\nFunctions\\n\\nlangchain.prompts\\nClasses\\nFunctions\\n\\nlangchain.retrievers\\nClasses\\nFunctions\\n\\nlangchain.runnables\\nClasses\\n\\nlangchain.smith\\nClasses\\nFunctions\\n\\nlangchain.storage\\nClasses\\n\\nlangchain.text_splitter\\nClasses\\nFunctions\\n\\nlangchain.tools\\nClasses\\nFunctions\\n\\nlangchain.utils\\nFunctions\\n\\nlangchain 0.1.4Â¶\\n\\nlangchain.agentsÂ¶\\nAgent is a class that uses an LLM to choose a sequence of actions to take.\\nIn Chains, a sequence of actions is hardcoded. In Agents,\\na language model is used as a reasoning engine to determine which actions\\nto take and in which order.\\nAgents select and use Tools and Toolkits for actions.\\nClass hierarchy:\\nBaseSingleActionAgent --> LLMSingleActionAgent\\n                          OpenAIFunctionsAgent\\n                          XMLAgent\\n                          Agent --> <name>Agent  # Examples: ZeroShotAgent, ChatAgent\\n\\nBaseMultiActionAgent  --> OpenAIMultiFunctionsAgent\\n\\nMain helpers:\\nAgentType, AgentExecutor, AgentOutputParser, AgentExecutorIterator,\\nAgentAction, AgentFinish\\n\\nClassesÂ¶\\n\\nagents.agent.Agent\\n[Deprecated]  Agent that calls the language model and deciding the action.\\n\\nagents.agent.AgentExecutor\\nAgent that is using tools.\\n\\nagents.agent.AgentOutputParser\\nBase class for parsing agent output into agent action/finish.\\n\\nagents.agent.BaseMultiActionAgent\\nBase Multi Action Agent class.\\n\\nagents.agent.BaseSingleActionAgent\\nBase Single Action Agent class.\\n\\nagents.agent.ExceptionTool\\nTool that just returns the query.\\n\\nagents.agent.LLMSingleActionAgent\\n[Deprecated]  Base class for single action agents.[Deprecated] Base class for single action agents.\\n\\nagents.agent.MultiActionAgentOutputParser\\nBase class for parsing agent output into agent actions/finish.\\n\\nagents.agent.RunnableAgent\\nAgent powered by runnables.\\n\\nagents.agent.RunnableMultiActionAgent\\nAgent powered by runnables.\\n\\nagents.agent_iterator.AgentExecutorIterator(...)\\nIterator for AgentExecutor.\\n\\nagents.agent_toolkits.vectorstore.toolkit.VectorStoreInfo\\nInformation about a VectorStore.\\n\\nagents.agent_toolkits.vectorstore.toolkit.VectorStoreRouterToolkit\\nToolkit for routing between Vector Stores.\\n\\nagents.agent_toolkits.vectorstore.toolkit.VectorStoreToolkit\\nToolkit for interacting with a Vector Store.\\n\\nagents.agent_types.AgentType(value[,\\xa0names,\\xa0...])\\n[Deprecated]  An enum for agent types.\\n\\nagents.chat.base.ChatAgent\\n[Deprecated]  Chat Agent.[Deprecated] Chat Agent.\\n\\nagents.chat.output_parser.ChatOutputParser\\nOutput parser for the chat agent.\\n\\nagents.conversational.base.ConversationalAgent\\n[Deprecated]  An agent that holds a conversation in addition to using tools.[Deprecated] An agent that holds a conversation in addition to using tools.\\n\\nagents.conversational.output_parser.ConvoOutputParser\\nOutput parser for the conversational agent.\\n\\nagents.conversational_chat.base.ConversationalChatAgent\\n[Deprecated]  An agent designed to hold a conversation in addition to using tools.[Deprecated] An agent designed to hold a conversation in addition to using tools.\\n\\nagents.conversational_chat.output_parser.ConvoOutputParser\\nOutput parser for the conversational agent.\\n\\nagents.mrkl.base.ChainConfig(action_name,\\xa0...)\\nConfiguration for chain to use in MRKL system.\\n\\nagents.mrkl.base.MRKLChain\\n[Deprecated]  [Deprecated] Chain that implements the MRKL system.[Deprecated] [Deprecated] Chain that implements the MRKL system.\\n\\nagents.mrkl.base.ZeroShotAgent\\n[Deprecated]  Agent for the MRKL chain.[Deprecated] Agent for the MRKL chain.\\n\\nagents.mrkl.output_parser.MRKLOutputParser\\nMRKL Output parser for the chat agent.\\n\\nagents.openai_assistant.base.OpenAIAssistantAction\\nAgentAction with info needed to submit custom tool output to existing run.\\n\\nagents.openai_assistant.base.OpenAIAssistantFinish\\nAgentFinish with run and thread metadata.\\n\\nagents.openai_assistant.base.OpenAIAssistantRunnable\\nRun an OpenAI Assistant.\\n\\nagents.openai_functions_agent.agent_token_buffer_memory.AgentTokenBufferMemory\\nMemory used to save agent output AND intermediate steps.\\n\\nagents.openai_functions_agent.base.OpenAIFunctionsAgent\\n[Deprecated]  An Agent driven by OpenAIs function powered API.\\n\\nagents.openai_functions_multi_agent.base.OpenAIMultiFunctionsAgent\\n[Deprecated]  An Agent driven by OpenAIs function powered API.\\n\\nagents.output_parsers.json.JSONAgentOutputParser\\nParses tool invocations and final answers in JSON format.\\n\\nagents.output_parsers.openai_functions.OpenAIFunctionsAgentOutputParser\\nParses a message into agent action/finish.\\n\\nagents.output_parsers.openai_tools.OpenAIToolAgentAction\\nOverride init to support instantiation by position for backward compat.\\n\\nagents.output_parsers.openai_tools.OpenAIToolsAgentOutputParser\\nParses a message into agent actions/finish.\\n\\nagents.output_parsers.react_json_single_input.ReActJsonSingleInputOutputParser\\nParses ReAct-style LLM calls that have a single tool input in json format.\\n\\nagents.output_parsers.react_single_input.ReActSingleInputOutputParser\\nParses ReAct-style LLM calls that have a single tool input.\\n\\nagents.output_parsers.self_ask.SelfAskOutputParser\\nParses self-ask style LLM calls.\\n\\nagents.output_parsers.xml.XMLAgentOutputParser\\nParses tool invocations and final answers in XML format.\\n\\nagents.react.base.DocstoreExplorer(docstore)\\n[Deprecated]  Class to assist with exploration of a document store.[Deprecated] Class to assist with exploration of a document store.\\n\\nagents.react.base.ReActChain\\n[Deprecated]  [Deprecated] Chain that implements the ReAct paper.[Deprecated] [Deprecated] Chain that implements the ReAct paper.\\n\\nagents.react.base.ReActDocstoreAgent\\n[Deprecated]  Agent for the ReAct chain.[Deprecated] Agent for the ReAct chain.\\n\\nagents.react.base.ReActTextWorldAgent\\n[Deprecated]  Agent for the ReAct TextWorld chain.[Deprecated] Agent for the ReAct TextWorld chain.\\n\\nagents.react.output_parser.ReActOutputParser\\nOutput parser for the ReAct agent.\\n\\nagents.schema.AgentScratchPadChatPromptTemplate\\nChat prompt template for the agent scratchpad.\\n\\nagents.self_ask_with_search.base.SelfAskWithSearchAgent\\n[Deprecated]  Agent for the self-ask-with-search paper.[Deprecated] Agent for the self-ask-with-search paper.\\n\\nagents.self_ask_with_search.base.SelfAskWithSearchChain\\n[Deprecated]  [Deprecated] Chain that does self-ask with search.[Deprecated] [Deprecated] Chain that does self-ask with search.\\n\\nagents.structured_chat.base.StructuredChatAgent\\n[Deprecated]  Structured Chat Agent.[Deprecated] Structured Chat Agent.\\n\\nagents.structured_chat.output_parser.StructuredChatOutputParser\\nOutput parser for the structured chat agent.\\n\\nagents.structured_chat.output_parser.StructuredChatOutputParserWithRetries\\nOutput parser with retries for the structured chat agent.\\n\\nagents.tools.InvalidTool\\nTool that is run when invalid tool name is encountered by agent.\\n\\nagents.xml.base.XMLAgent\\n[Deprecated]  Agent that uses XML tags.\\n\\nFunctionsÂ¶\\n\\nagents.agent_toolkits.conversational_retrieval.openai_functions.create_conversational_retrieval_agent(...)\\nA convenience method for creating a conversational retrieval agent.\\n\\nagents.agent_toolkits.vectorstore.base.create_vectorstore_agent(...)\\nConstruct a VectorStore agent from an LLM and tools.\\n\\nagents.agent_toolkits.vectorstore.base.create_vectorstore_router_agent(...)\\nConstruct a VectorStore router agent from an LLM and tools.\\n\\nagents.format_scratchpad.log.format_log_to_str(...)\\nConstruct the scratchpad that lets the agent continue its thought process.\\n\\nagents.format_scratchpad.log_to_messages.format_log_to_messages(...)\\nConstruct the scratchpad that lets the agent continue its thought process.\\n\\nagents.format_scratchpad.openai_functions.format_to_openai_function_messages(...)\\nConvert (AgentAction, tool output) tuples into FunctionMessages.\\n\\nagents.format_scratchpad.openai_functions.format_to_openai_functions(...)\\nConvert (AgentAction, tool output) tuples into FunctionMessages.\\n\\nagents.format_scratchpad.openai_tools.format_to_openai_tool_messages(...)\\nConvert (AgentAction, tool output) tuples into FunctionMessages.\\n\\nagents.format_scratchpad.xml.format_xml(...)\\nFormat the intermediate steps as XML.\\n\\nagents.initialize.initialize_agent(tools,\\xa0llm)\\n[Deprecated]  Load an agent executor given tools and LLM.\\n\\nagents.json_chat.base.create_json_chat_agent(...)\\nCreate an agent that uses JSON to format its logic, build for Chat Models.\\n\\nagents.load_tools.get_all_tool_names()\\nGet a list of all possible tool names.\\n\\nagents.load_tools.load_huggingface_tool(...)\\nLoads a tool from the HuggingFace Hub.\\n\\nagents.load_tools.load_tools(tool_names[,\\xa0...])\\nLoad tools based on their name.\\n\\nagents.loading.load_agent(path,\\xa0**kwargs)\\n[Deprecated]  Unified method for loading an agent from LangChainHub or local fs.\\n\\nagents.loading.load_agent_from_config(config)\\n[Deprecated]  Load agent from Config Dict.\\n\\nagents.openai_functions_agent.base.create_openai_functions_agent(...)\\nCreate an agent that uses OpenAI function calling.\\n\\nagents.openai_tools.base.create_openai_tools_agent(...)\\nCreate an agent that uses OpenAI tools.\\n\\nagents.output_parsers.openai_tools.parse_ai_message_to_openai_tool_action(message)\\nParse an AI message potentially containing tool_calls.\\n\\nagents.react.agent.create_react_agent(llm,\\xa0...)\\nCreate an agent that uses ReAct prompting.\\n\\nagents.self_ask_with_search.base.create_self_ask_with_search_agent(...)\\nCreate an agent that uses self-ask with search prompting.\\n\\nagents.structured_chat.base.create_structured_chat_agent(...)\\nCreate an agent aimed at supporting tools with multiple inputs.\\n\\nagents.utils.validate_tools_single_input(...)\\nValidate tools for single input.\\n\\nagents.xml.base.create_xml_agent(llm,\\xa0tools,\\xa0...)\\nCreate an agent that uses XML to format its logic.\\n\\nlangchain.callbacksÂ¶\\nCallback handlers allow listening to events in LangChain.\\nClass hierarchy:\\nBaseCallbackHandler --> <name>CallbackHandler  # Example: AimCallbackHandler\\n\\nClassesÂ¶\\n\\ncallbacks.file.FileCallbackHandler(filename)\\nCallback Handler that writes to a file.\\n\\ncallbacks.streaming_aiter.AsyncIteratorCallbackHandler()\\nCallback handler that returns an async iterator.\\n\\ncallbacks.streaming_aiter_final_only.AsyncFinalIteratorCallbackHandler(*)\\nCallback handler that returns an async iterator.\\n\\ncallbacks.streaming_stdout_final_only.FinalStreamingStdOutCallbackHandler(*)\\nCallback handler for streaming in agents.\\n\\ncallbacks.tracers.logging.LoggingCallbackHandler(logger)\\nTracer that logs via the input Logger.\\n\\nlangchain.chainsÂ¶\\nChains are easily reusable components linked together.\\nChains encode a sequence of calls to components like models, document retrievers,\\nother Chains, etc., and provide a simple interface to this sequence.\\nThe Chain interface makes it easy to create apps that are:\\n\\nStateful: add Memory to any Chain to give it state,\\nObservable: pass Callbacks to a Chain to execute additional functionality,\\nlike logging, outside the main sequence of component calls,\\nComposable: combine Chains with other components, including other Chains.\\n\\nClass hierarchy:\\nChain --> <name>Chain  # Examples: LLMChain, MapReduceChain, RouterChain\\n\\nClassesÂ¶\\n\\nchains.api.base.APIChain\\nChain that makes API calls and summarizes the responses to answer a question.\\n\\nchains.api.openapi.chain.OpenAPIEndpointChain\\nChain interacts with an OpenAPI endpoint using natural language.\\n\\nchains.api.openapi.requests_chain.APIRequesterChain\\nGet the request parser.\\n\\nchains.api.openapi.requests_chain.APIRequesterOutputParser\\nParse the request and error tags.\\n\\nchains.api.openapi.response_chain.APIResponderChain\\nGet the response parser.\\n\\nchains.api.openapi.response_chain.APIResponderOutputParser\\nParse the response and error tags.\\n\\nchains.base.Chain\\nAbstract base class for creating structured sequences of calls to components.\\n\\nchains.combine_documents.base.AnalyzeDocumentChain\\nChain that splits documents, then analyzes it in pieces.\\n\\nchains.combine_documents.base.BaseCombineDocumentsChain\\nBase interface for chains combining documents.\\n\\nchains.combine_documents.map_reduce.MapReduceDocumentsChain\\nCombining documents by mapping a chain over them, then combining results.\\n\\nchains.combine_documents.map_rerank.MapRerankDocumentsChain\\nCombining documents by mapping a chain over them, then reranking results.\\n\\nchains.combine_documents.reduce.AsyncCombineDocsProtocol(...)\\nInterface for the combine_docs method.\\n\\nchains.combine_documents.reduce.CombineDocsProtocol(...)\\nInterface for the combine_docs method.\\n\\nchains.combine_documents.reduce.ReduceDocumentsChain\\nCombine documents by recursively reducing them.\\n\\nchains.combine_documents.refine.RefineDocumentsChain\\nCombine documents by doing a first pass and then refining on more documents.\\n\\nchains.combine_documents.stuff.StuffDocumentsChain\\nChain that combines documents by stuffing into context.\\n\\nchains.constitutional_ai.base.ConstitutionalChain\\nChain for applying constitutional principles.\\n\\nchains.constitutional_ai.models.ConstitutionalPrinciple\\nClass for a constitutional principle.\\n\\nchains.conversation.base.ConversationChain\\nChain to have a conversation and load context from memory.\\n\\nchains.conversational_retrieval.base.BaseConversationalRetrievalChain\\nChain for chatting with an index.\\n\\nchains.conversational_retrieval.base.ChatVectorDBChain\\nChain for chatting with a vector database.\\n\\nchains.conversational_retrieval.base.ConversationalRetrievalChain\\nChain for having a conversation based on retrieved documents.\\n\\nchains.conversational_retrieval.base.InputType\\nInput type for ConversationalRetrievalChain.\\n\\nchains.elasticsearch_database.base.ElasticsearchDatabaseChain\\nChain for interacting with Elasticsearch Database.\\n\\nchains.flare.base.FlareChain\\nChain that combines a retriever, a question generator, and a response generator.\\n\\nchains.flare.base.QuestionGeneratorChain\\nChain that generates questions from uncertain spans.\\n\\nchains.flare.prompts.FinishedOutputParser\\nOutput parser that checks if the output is finished.\\n\\nchains.graph_qa.arangodb.ArangoGraphQAChain\\nChain for question-answering against a graph by generating AQL statements.\\n\\nchains.graph_qa.base.GraphQAChain\\nChain for question-answering against a graph.\\n\\nchains.graph_qa.cypher.GraphCypherQAChain\\nChain for question-answering against a graph by generating Cypher statements.\\n\\nchains.graph_qa.cypher_utils.CypherQueryCorrector(schemas)\\nUsed to correct relationship direction in generated Cypher statements.\\n\\nchains.graph_qa.cypher_utils.Schema(...)\\nCreate new instance of Schema(left_node, relation, right_node)\\n\\nchains.graph_qa.falkordb.FalkorDBQAChain\\nChain for question-answering against a graph by generating Cypher statements.\\n\\nchains.graph_qa.hugegraph.HugeGraphQAChain\\nChain for question-answering against a graph by generating gremlin statements.\\n\\nchains.graph_qa.kuzu.KuzuQAChain\\nQuestion-answering against a graph by generating Cypher statements for KÃ¹zu.\\n\\nchains.graph_qa.nebulagraph.NebulaGraphQAChain\\nChain for question-answering against a graph by generating nGQL statements.\\n\\nchains.graph_qa.neptune_cypher.NeptuneOpenCypherQAChain\\nChain for question-answering against a Neptune graph by generating openCypher statements.\\n\\nchains.graph_qa.sparql.GraphSparqlQAChain\\nQuestion-answering against an RDF or OWL graph by generating SPARQL statements.\\n\\nchains.hyde.base.HypotheticalDocumentEmbedder\\nGenerate hypothetical document for query, and then embed that.\\n\\nchains.llm.LLMChain\\nChain to run queries against LLMs.\\n\\nchains.llm_checker.base.LLMCheckerChain\\nChain for question-answering with self-verification.\\n\\nchains.llm_math.base.LLMMathChain\\nChain that interprets a prompt and executes python code to do math.\\n\\nchains.llm_requests.LLMRequestsChain\\nChain that requests a URL and then uses an LLM to parse results.\\n\\nchains.llm_summarization_checker.base.LLMSummarizationCheckerChain\\nChain for question-answering with self-verification.\\n\\nchains.mapreduce.MapReduceChain\\nMap-reduce chain.\\n\\nchains.moderation.OpenAIModerationChain\\nPass input through a moderation endpoint.\\n\\nchains.natbot.base.NatBotChain\\nImplement an LLM driven browser.\\n\\nchains.natbot.crawler.Crawler()\\nA crawler for web pages.\\n\\nchains.natbot.crawler.ElementInViewPort\\nA typed dictionary containing information about elements in the viewport.\\n\\nchains.openai_functions.citation_fuzzy_match.FactWithEvidence\\nClass representing a single statement.\\n\\nchains.openai_functions.citation_fuzzy_match.QuestionAnswer\\nA question and its answer as a list of facts each one should have a source.\\n\\nchains.openai_functions.openapi.SimpleRequestChain\\nChain for making a simple request to an API endpoint.\\n\\nchains.openai_functions.qa_with_structure.AnswerWithSources\\nAn answer to the question, with sources.\\n\\nchains.prompt_selector.BasePromptSelector\\nBase class for prompt selectors.\\n\\nchains.prompt_selector.ConditionalPromptSelector\\nPrompt collection that goes through conditionals.\\n\\nchains.qa_generation.base.QAGenerationChain\\nBase class for question-answer generation chains.\\n\\nchains.qa_with_sources.base.BaseQAWithSourcesChain\\nQuestion answering chain with sources over documents.\\n\\nchains.qa_with_sources.base.QAWithSourcesChain\\nQuestion answering with sources over documents.\\n\\nchains.qa_with_sources.loading.LoadingCallable(...)\\nInterface for loading the combine documents chain.\\n\\nchains.qa_with_sources.retrieval.RetrievalQAWithSourcesChain\\nQuestion-answering with sources over an index.\\n\\nchains.qa_with_sources.vector_db.VectorDBQAWithSourcesChain\\nQuestion-answering with sources over a vector database.\\n\\nchains.query_constructor.base.StructuredQueryOutputParser\\nOutput parser that parses a structured query.\\n\\nchains.query_constructor.ir.Comparator(value)\\nEnumerator of the comparison operators.\\n\\nchains.query_constructor.ir.Comparison\\nA comparison to a value.\\n\\nchains.query_constructor.ir.Expr\\nBase class for all expressions.\\n\\nchains.query_constructor.ir.FilterDirective\\nA filtering expression.\\n\\nchains.query_constructor.ir.Operation\\nA logical operation over other directives.\\n\\nchains.query_constructor.ir.Operator(value)\\nEnumerator of the operations.\\n\\nchains.query_constructor.ir.StructuredQuery\\nA structured query.\\n\\nchains.query_constructor.ir.Visitor()\\nDefines interface for IR translation using visitor pattern.\\n\\nchains.query_constructor.parser.ISO8601Date\\nA date in ISO 8601 format (YYYY-MM-DD).\\n\\nchains.query_constructor.schema.AttributeInfo\\nInformation about a data source attribute.\\n\\nchains.retrieval_qa.base.BaseRetrievalQA\\nBase class for question-answering chains.\\n\\nchains.retrieval_qa.base.RetrievalQA\\nChain for question-answering against an index.\\n\\nchains.retrieval_qa.base.VectorDBQA\\nChain for question-answering against a vector database.\\n\\nchains.router.base.MultiRouteChain\\nUse a single chain to route an input to one of multiple candidate chains.\\n\\nchains.router.base.Route(destination,\\xa0...)\\nCreate new instance of Route(destination, next_inputs)\\n\\nchains.router.base.RouterChain\\nChain that outputs the name of a destination chain and the inputs to it.\\n\\nchains.router.embedding_router.EmbeddingRouterChain\\nChain that uses embeddings to route between options.\\n\\nchains.router.llm_router.LLMRouterChain\\nA router chain that uses an LLM chain to perform routing.\\n\\nchains.router.llm_router.RouterOutputParser\\nParser for output of router chain in the multi-prompt chain.\\n\\nchains.router.multi_prompt.MultiPromptChain\\nA multi-route chain that uses an LLM router chain to choose amongst prompts.\\n\\nchains.router.multi_retrieval_qa.MultiRetrievalQAChain\\nA multi-route chain that uses an LLM router chain to choose amongst retrieval qa chains.\\n\\nchains.sequential.SequentialChain\\nChain where the outputs of one chain feed directly into next.\\n\\nchains.sequential.SimpleSequentialChain\\nSimple chain where the outputs of one step feed directly into next.\\n\\nchains.sql_database.query.SQLInput\\nInput for a SQL Chain.\\n\\nchains.sql_database.query.SQLInputWithTables\\nInput for a SQL Chain.\\n\\nchains.transform.TransformChain\\nChain that transforms the chain output.\\n\\nFunctionsÂ¶\\n\\nchains.combine_documents.reduce.acollapse_docs(...)\\nExecute a collapse function on a set of documents and merge their metadatas.\\n\\nchains.combine_documents.reduce.collapse_docs(...)\\nExecute a collapse function on a set of documents and merge their metadatas.\\n\\nchains.combine_documents.reduce.split_list_of_docs(...)\\nSplit Documents into subsets that each meet a cumulative length constraint.\\n\\nchains.combine_documents.stuff.create_stuff_documents_chain(...)\\nCreate a chain for passing a list of Documents to a model.\\n\\nchains.ernie_functions.base.convert_python_function_to_ernie_function(...)\\nConvert a Python function to an Ernie function-calling API compatible dict.\\n\\nchains.ernie_functions.base.convert_to_ernie_function(...)\\nConvert a raw function/class to an Ernie function.\\n\\nchains.ernie_functions.base.create_ernie_fn_chain(...)\\n[Legacy] Create an LLM chain that uses Ernie functions.\\n\\nchains.ernie_functions.base.create_ernie_fn_runnable(...)\\nCreate a runnable sequence that uses Ernie functions.\\n\\nchains.ernie_functions.base.create_structured_output_chain(...)\\n[Legacy] Create an LLMChain that uses an Ernie function to get a structured output.\\n\\nchains.ernie_functions.base.create_structured_output_runnable(...)\\nCreate a runnable that uses an Ernie function to get a structured output.\\n\\nchains.ernie_functions.base.get_ernie_output_parser(...)\\nGet the appropriate function output parser given the user functions.\\n\\nchains.example_generator.generate_example(...)\\nReturn another example given a list of examples for a prompt.\\n\\nchains.graph_qa.cypher.construct_schema(...)\\nFilter the schema based on included or excluded types\\n\\nchains.graph_qa.cypher.extract_cypher(text)\\nExtract Cypher code from a text.\\n\\nchains.graph_qa.falkordb.extract_cypher(text)\\nExtract Cypher code from a text.\\n\\nchains.graph_qa.neptune_cypher.extract_cypher(text)\\nExtract Cypher code from text using Regex.\\n\\nchains.graph_qa.neptune_cypher.trim_query(query)\\nTrim the query to only include Cypher keywords.\\n\\nchains.graph_qa.neptune_cypher.use_simple_prompt(llm)\\nDecides whether to use the simple prompt\\n\\nchains.history_aware_retriever.create_history_aware_retriever(...)\\nCreate a chain that takes conversation history and returns documents.\\n\\nchains.loading.load_chain(path,\\xa0**kwargs)\\nUnified method for loading a chain from LangChainHub or local fs.\\n\\nchains.loading.load_chain_from_config(...)\\nLoad chain from Config Dict.\\n\\nchains.openai_functions.base.create_openai_fn_chain(...)\\n[Deprecated]  [Legacy] Create an LLM chain that uses OpenAI functions.\\n\\nchains.openai_functions.base.create_openai_fn_runnable(...)\\nCreate a runnable sequence that uses OpenAI functions.\\n\\nchains.openai_functions.base.create_structured_output_chain(...)\\n[Deprecated]  [Legacy] Create an LLMChain that uses an OpenAI function to get a structured output.\\n\\nchains.openai_functions.base.create_structured_output_runnable(...)\\nCreate a runnable that uses an OpenAI function to get a structured output.\\n\\nchains.openai_functions.base.get_openai_output_parser(...)\\nGet the appropriate function output parser given the user functions.\\n\\nchains.openai_functions.citation_fuzzy_match.create_citation_fuzzy_match_chain(llm)\\nCreate a citation fuzzy match chain.\\n\\nchains.openai_functions.extraction.create_extraction_chain(...)\\nCreates a chain that extracts information from a passage.\\n\\nchains.openai_functions.extraction.create_extraction_chain_pydantic(...)\\nCreates a chain that extracts information from a passage using pydantic schema.\\n\\nchains.openai_functions.openapi.get_openapi_chain(spec)\\nCreate a chain for querying an API from a OpenAPI spec.\\n\\nchains.openai_functions.openapi.openapi_spec_to_openai_fn(spec)\\nConvert a valid OpenAPI spec to the JSON Schema format expected for OpenAI\\n\\nchains.openai_functions.qa_with_structure.create_qa_with_sources_chain(llm)\\nCreate a question answering chain that returns an answer with sources.\\n\\nchains.openai_functions.qa_with_structure.create_qa_with_structure_chain(...)\\nCreate a question answering chain that returns an answer with sources\\n\\nchains.openai_functions.tagging.create_tagging_chain(...)\\nCreates a chain that extracts information from a passage\\n\\nchains.openai_functions.tagging.create_tagging_chain_pydantic(...)\\nCreates a chain that extracts information from a passage\\n\\nchains.openai_functions.utils.get_llm_kwargs(...)\\nReturns the kwargs for the LLMChain constructor.\\n\\nchains.openai_tools.extraction.create_extraction_chain_pydantic(...)\\nCreates a chain that extracts information from a passage.\\n\\nchains.prompt_selector.is_chat_model(llm)\\nCheck if the language model is a chat model.\\n\\nchains.prompt_selector.is_llm(llm)\\nCheck if the language model is a LLM.\\n\\nchains.qa_with_sources.loading.load_qa_with_sources_chain(llm)\\nLoad a question answering with sources chain.\\n\\nchains.query_constructor.base.construct_examples(...)\\nConstruct examples from input-output pairs.\\n\\nchains.query_constructor.base.fix_filter_directive(...)\\nFix invalid filter directive.\\n\\nchains.query_constructor.base.get_query_constructor_prompt(...)\\nCreate query construction prompt.\\n\\nchains.query_constructor.base.load_query_constructor_chain(...)\\nLoad a query constructor chain.\\n\\nchains.query_constructor.base.load_query_constructor_runnable(...)\\nLoad a query constructor runnable chain.\\n\\nchains.query_constructor.parser.get_parser([...])\\nReturns a parser for the query language.\\n\\nchains.query_constructor.parser.v_args(...)\\nDummy decorator for when lark is not installed.\\n\\nchains.retrieval.create_retrieval_chain(...)\\nCreate retrieval chain that retrieves documents and then passes them on.\\n\\nchains.sql_database.query.create_sql_query_chain(llm,\\xa0db)\\nCreate a chain that generates SQL queries.\\n\\nlangchain.embeddingsÂ¶\\nEmbedding models  are wrappers around embedding models\\nfrom different APIs and services.\\nEmbedding models can be LLMs or not.\\nClass hierarchy:\\nEmbeddings --> <name>Embeddings  # Examples: OpenAIEmbeddings, HuggingFaceEmbeddings\\n\\nClassesÂ¶\\n\\nembeddings.cache.CacheBackedEmbeddings(...)\\nInterface for caching results from embedding models.\\n\\nFunctionsÂ¶\\n\\nlangchain.evaluationÂ¶\\nEvaluation chains for grading LLM and Chain outputs.\\nThis module contains off-the-shelf evaluation chains for grading the output of\\nLangChain primitives such as language models and chains.\\nLoading an evaluator\\nTo load an evaluator, you can use the load_evaluators or\\nload_evaluator functions with the\\nnames of the evaluators to load.\\nfrom langchain.evaluation import load_evaluator\\n\\nevaluator = load_evaluator(\"qa\")\\nevaluator.evaluate_strings(\\n    prediction=\"We sold more than 40,000 units last week\",\\n    input=\"How many units did we sell last week?\",\\n    reference=\"We sold 32,378 units\",\\n)\\n\\nThe evaluator must be one of EvaluatorType.\\nDatasets\\nTo load one of the LangChain HuggingFace datasets, you can use the load_dataset function with the\\nname of the dataset to load.\\nfrom langchain.evaluation import load_dataset\\nds = load_dataset(\"llm-math\")\\n\\nSome common use cases for evaluation include:\\n\\nGrading the accuracy of a response against ground truth answers: QAEvalChain\\nComparing the output of two models: PairwiseStringEvalChain or LabeledPairwiseStringEvalChain when there is additionally a reference label.\\nJudging the efficacy of an agentâ€™s tool usage: TrajectoryEvalChain\\nChecking whether an output complies with a set of criteria: CriteriaEvalChain or LabeledCriteriaEvalChain when there is additionally a reference label.\\nComputing semantic difference between a prediction and reference: EmbeddingDistanceEvalChain or between two predictions: PairwiseEmbeddingDistanceEvalChain\\nMeasuring the string distance between a prediction and reference StringDistanceEvalChain or between two predictions PairwiseStringDistanceEvalChain\\n\\nLow-level API\\nThese evaluators implement one of the following interfaces:\\n\\nStringEvaluator: Evaluate a prediction string against a reference label and/or input context.\\nPairwiseStringEvaluator: Evaluate two prediction strings against each other. Useful for scoring preferences, measuring similarity between two chain or llm agents, or comparing outputs on similar inputs.\\nAgentTrajectoryEvaluator Evaluate the full sequence of actions taken by an agent.\\n\\nThese interfaces enable easier composability and usage within a higher level evaluation framework.\\n\\nClassesÂ¶\\n\\nevaluation.agents.trajectory_eval_chain.TrajectoryEval\\nA named tuple containing the score and reasoning for a trajectory.\\n\\nevaluation.agents.trajectory_eval_chain.TrajectoryEvalChain\\nA chain for evaluating ReAct style agents.\\n\\nevaluation.agents.trajectory_eval_chain.TrajectoryOutputParser\\nTrajectory output parser.\\n\\nevaluation.comparison.eval_chain.LabeledPairwiseStringEvalChain\\nA chain for comparing two outputs, such as the outputs\\n\\nevaluation.comparison.eval_chain.PairwiseStringEvalChain\\nA chain for comparing two outputs, such as the outputs\\n\\nevaluation.comparison.eval_chain.PairwiseStringResultOutputParser\\nA parser for the output of the PairwiseStringEvalChain.\\n\\nevaluation.criteria.eval_chain.Criteria(value)\\nA Criteria to evaluate.\\n\\nevaluation.criteria.eval_chain.CriteriaEvalChain\\nLLM Chain for evaluating runs against criteria.\\n\\nevaluation.criteria.eval_chain.CriteriaResultOutputParser\\nA parser for the output of the CriteriaEvalChain.\\n\\nevaluation.criteria.eval_chain.LabeledCriteriaEvalChain\\nCriteria evaluation chain that requires references.\\n\\nevaluation.embedding_distance.base.EmbeddingDistance(value)\\nEmbedding Distance Metric.\\n\\nevaluation.embedding_distance.base.EmbeddingDistanceEvalChain\\nUse embedding distances to score semantic difference between a prediction and reference.\\n\\nevaluation.embedding_distance.base.PairwiseEmbeddingDistanceEvalChain\\nUse embedding distances to score semantic difference between two predictions.\\n\\nevaluation.exact_match.base.ExactMatchStringEvaluator(*)\\nCompute an exact match between the prediction and the reference.\\n\\nevaluation.parsing.base.JsonEqualityEvaluator([...])\\nEvaluates whether the prediction is equal to the reference after\\n\\nevaluation.parsing.base.JsonValidityEvaluator(...)\\nEvaluates whether the prediction is valid JSON.\\n\\nevaluation.parsing.json_distance.JsonEditDistanceEvaluator([...])\\nAn evaluator that calculates the edit distance between JSON strings.\\n\\nevaluation.parsing.json_schema.JsonSchemaEvaluator(...)\\nAn evaluator that validates a JSON prediction against a JSON schema reference.\\n\\nevaluation.qa.eval_chain.ContextQAEvalChain\\nLLM Chain for evaluating QA w/o GT based on context\\n\\nevaluation.qa.eval_chain.CotQAEvalChain\\nLLM Chain for evaluating QA using chain of thought reasoning.\\n\\nevaluation.qa.eval_chain.QAEvalChain\\nLLM Chain for evaluating question answering.\\n\\nevaluation.qa.generate_chain.QAGenerateChain\\nLLM Chain for generating examples for question answering.\\n\\nevaluation.regex_match.base.RegexMatchStringEvaluator(*)\\nCompute a regex match between the prediction and the reference.\\n\\nevaluation.schema.AgentTrajectoryEvaluator()\\nInterface for evaluating agent trajectories.\\n\\nevaluation.schema.EvaluatorType(value[,\\xa0...])\\nThe types of the evaluators.\\n\\nevaluation.schema.LLMEvalChain\\nA base class for evaluators that use an LLM.\\n\\nevaluation.schema.PairwiseStringEvaluator()\\nCompare the output of two models (or two outputs of the same model).\\n\\nevaluation.schema.StringEvaluator()\\nGrade, tag, or otherwise evaluate predictions relative to their inputs and/or reference labels.\\n\\nevaluation.scoring.eval_chain.LabeledScoreStringEvalChain\\nA chain for scoring the output of a model on a scale of 1-10.\\n\\nevaluation.scoring.eval_chain.ScoreStringEvalChain\\nA chain for scoring on a scale of 1-10 the output of a model.\\n\\nevaluation.scoring.eval_chain.ScoreStringResultOutputParser\\nA parser for the output of the ScoreStringEvalChain.\\n\\nevaluation.string_distance.base.PairwiseStringDistanceEvalChain\\nCompute string edit distances between two predictions.\\n\\nevaluation.string_distance.base.StringDistance(value)\\nDistance metric to use.\\n\\nevaluation.string_distance.base.StringDistanceEvalChain\\nCompute string distances between the prediction and the reference.\\n\\nFunctionsÂ¶\\n\\nevaluation.comparison.eval_chain.resolve_pairwise_criteria(...)\\nResolve the criteria for the pairwise evaluator.\\n\\nevaluation.criteria.eval_chain.resolve_criteria(...)\\nResolve the criteria to evaluate.\\n\\nevaluation.loading.load_dataset(uri)\\nLoad a dataset from the LangChainDatasets on HuggingFace.\\n\\nevaluation.loading.load_evaluator(evaluator,\\xa0*)\\nLoad the requested evaluation chain specified by a string.\\n\\nevaluation.loading.load_evaluators(evaluators,\\xa0*)\\nLoad evaluators specified by a list of evaluator types.\\n\\nevaluation.scoring.eval_chain.resolve_criteria(...)\\nResolve the criteria for the pairwise evaluator.\\n\\nlangchain.hubÂ¶\\nInterface with the LangChain Hub.\\n\\nFunctionsÂ¶\\n\\nhub.pull(owner_repo_commit,\\xa0*[,\\xa0api_url,\\xa0...])\\nPulls an object from the hub and returns it as a LangChain object.\\n\\nhub.push(repo_full_name,\\xa0object,\\xa0*[,\\xa0...])\\nPushes an object to the hub and returns the URL it can be viewed at in a browser.\\n\\nlangchain.indexesÂ¶\\nCode to support various indexing workflows.\\nProvides code to:\\n\\nCreate knowledge graphs from data.\\nSupport indexing workflows from LangChain data loaders to vectorstores.\\n\\nFor indexing workflows, this code is used to avoid writing duplicated content\\ninto the vectostore and to avoid over-writing content if itâ€™s unchanged.\\nImportantly, this keeps on working even if the content being written is derived\\nvia a set of transformations from some source content (e.g., indexing children\\ndocuments that were derived from parent documents by chunking.)\\n\\nClassesÂ¶\\n\\nindexes.base.RecordManager(namespace)\\nAn abstract base class representing the interface for a record manager.\\n\\nindexes.graph.GraphIndexCreator\\nFunctionality to create graph index.\\n\\nindexes.vectorstore.VectorStoreIndexWrapper\\nWrapper around a vectorstore for easy access.\\n\\nindexes.vectorstore.VectorstoreIndexCreator\\nLogic for creating indexes.\\n\\nFunctionsÂ¶\\n\\nlangchain.memoryÂ¶\\nMemory maintains Chain state, incorporating context from past runs.\\nClass hierarchy for Memory:\\nBaseMemory --> BaseChatMemory --> <name>Memory  # Examples: ZepMemory, MotorheadMemory\\n\\nMain helpers:\\nBaseChatMessageHistory\\n\\nChat Message History stores the chat message history in different stores.\\nClass hierarchy for ChatMessageHistory:\\nBaseChatMessageHistory --> <name>ChatMessageHistory  # Example: ZepChatMessageHistory\\n\\nMain helpers:\\nAIMessage, BaseMessage, HumanMessage\\n\\nClassesÂ¶\\n\\nmemory.buffer.ConversationBufferMemory\\nBuffer for storing conversation memory.\\n\\nmemory.buffer.ConversationStringBufferMemory\\nBuffer for storing conversation memory.\\n\\nmemory.buffer_window.ConversationBufferWindowMemory\\nBuffer for storing conversation memory inside a limited size window.\\n\\nmemory.chat_memory.BaseChatMemory\\nAbstract base class for chat memory.\\n\\nmemory.combined.CombinedMemory\\nCombining multiple memories\\' data together.\\n\\nmemory.entity.BaseEntityStore\\nAbstract base class for Entity store.\\n\\nmemory.entity.ConversationEntityMemory\\nEntity extractor & summarizer memory.\\n\\nmemory.entity.InMemoryEntityStore\\nIn-memory Entity store.\\n\\nmemory.entity.RedisEntityStore\\nRedis-backed Entity store.\\n\\nmemory.entity.SQLiteEntityStore\\nSQLite-backed Entity store\\n\\nmemory.entity.UpstashRedisEntityStore\\nUpstash Redis backed Entity store.\\n\\nmemory.kg.ConversationKGMemory\\nKnowledge graph conversation memory.\\n\\nmemory.motorhead_memory.MotorheadMemory\\nChat message memory backed by Motorhead service.\\n\\nmemory.readonly.ReadOnlySharedMemory\\nA memory wrapper that is read-only and cannot be changed.\\n\\nmemory.simple.SimpleMemory\\nSimple memory for storing context or other information that shouldn\\'t ever change between prompts.\\n\\nmemory.summary.ConversationSummaryMemory\\nConversation summarizer to chat memory.\\n\\nmemory.summary.SummarizerMixin\\nMixin for summarizer.\\n\\nmemory.summary_buffer.ConversationSummaryBufferMemory\\nBuffer with summarizer for storing conversation memory.\\n\\nmemory.token_buffer.ConversationTokenBufferMemory\\nConversation chat memory with token limit.\\n\\nmemory.vectorstore.VectorStoreRetrieverMemory\\nVectorStoreRetriever-backed memory.\\n\\nmemory.zep_memory.ZepMemory\\nPersist your chain history to the Zep MemoryStore.\\n\\nFunctionsÂ¶\\n\\nmemory.utils.get_prompt_input_key(inputs,\\xa0...)\\nGet the prompt input key.\\n\\nlangchain.model_laboratoryÂ¶\\nExperiment with different models.\\n\\nClassesÂ¶\\n\\nmodel_laboratory.ModelLaboratory(chains[,\\xa0names])\\nExperiment with different models.\\n\\nlangchain.output_parsersÂ¶\\nOutputParser classes parse the output of an LLM call.\\nClass hierarchy:\\nBaseLLMOutputParser --> BaseOutputParser --> <name>OutputParser  # ListOutputParser, PydanticOutputParser\\n\\nMain helpers:\\nSerializable, Generation, PromptValue\\n\\nClassesÂ¶\\n\\noutput_parsers.boolean.BooleanOutputParser\\nParse the output of an LLM call to a boolean.\\n\\noutput_parsers.combining.CombiningOutputParser\\nCombine multiple output parsers into one.\\n\\noutput_parsers.datetime.DatetimeOutputParser\\nParse the output of an LLM call to a datetime.\\n\\noutput_parsers.enum.EnumOutputParser\\nParse an output that is one of a set of values.\\n\\noutput_parsers.fix.OutputFixingParser\\nWraps a parser and tries to fix parsing errors.\\n\\noutput_parsers.openai_functions.JsonKeyOutputFunctionsParser\\nParse an output as the element of the Json object.\\n\\noutput_parsers.openai_functions.JsonOutputFunctionsParser\\nParse an output as the Json object.\\n\\noutput_parsers.openai_functions.OutputFunctionsParser\\nParse an output that is one of sets of values.\\n\\noutput_parsers.openai_functions.PydanticAttrOutputFunctionsParser\\nParse an output as an attribute of a pydantic object.\\n\\noutput_parsers.openai_functions.PydanticOutputFunctionsParser\\nParse an output as a pydantic object.\\n\\noutput_parsers.openai_tools.JsonOutputKeyToolsParser\\nParse tools from OpenAI response.\\n\\noutput_parsers.openai_tools.JsonOutputToolsParser\\nParse tools from OpenAI response.\\n\\noutput_parsers.openai_tools.PydanticToolsParser\\nParse tools from OpenAI response.\\n\\noutput_parsers.pandas_dataframe.PandasDataFrameOutputParser\\nParse an output using Pandas DataFrame format.\\n\\noutput_parsers.pydantic.PydanticOutputParser\\nParse an output using a pydantic model.\\n\\noutput_parsers.regex.RegexParser\\nParse the output of an LLM call using a regex.\\n\\noutput_parsers.regex_dict.RegexDictParser\\nParse the output of an LLM call into a Dictionary using a regex.\\n\\noutput_parsers.retry.RetryOutputParser\\nWraps a parser and tries to fix parsing errors.\\n\\noutput_parsers.retry.RetryWithErrorOutputParser\\nWraps a parser and tries to fix parsing errors.\\n\\noutput_parsers.structured.ResponseSchema\\nA schema for a response from a structured output parser.\\n\\noutput_parsers.structured.StructuredOutputParser\\nParse the output of an LLM call to a structured output.\\n\\noutput_parsers.yaml.YamlOutputParser\\nParse YAML output using a pydantic model.\\n\\nFunctionsÂ¶\\n\\noutput_parsers.loading.load_output_parser(config)\\nLoad an output parser.\\n\\nlangchain.promptsÂ¶\\nPrompt is the input to the model.\\nPrompt is often constructed\\nfrom multiple components. Prompt classes and functions make constructing\\n\\nand working with prompts easy.\\n\\nClass hierarchy:\\nBasePromptTemplate --> PipelinePromptTemplate\\n                       StringPromptTemplate --> PromptTemplate\\n                                                FewShotPromptTemplate\\n                                                FewShotPromptWithTemplates\\n                       BaseChatPromptTemplate --> AutoGPTPrompt\\n                                                  ChatPromptTemplate --> AgentScratchPadChatPromptTemplate\\n\\nBaseMessagePromptTemplate --> MessagesPlaceholder\\n                              BaseStringMessagePromptTemplate --> ChatMessagePromptTemplate\\n                                                                  HumanMessagePromptTemplate\\n                                                                  AIMessagePromptTemplate\\n                                                                  SystemMessagePromptTemplate\\n\\nPromptValue --> StringPromptValue\\n                ChatPromptValue\\n\\nClassesÂ¶\\n\\nprompts.example_selector.ngram_overlap.NGramOverlapExampleSelector\\nSelect and order examples based on ngram overlap score (sentence_bleu score).\\n\\nFunctionsÂ¶\\n\\nprompts.example_selector.ngram_overlap.ngram_overlap_score(...)\\nCompute ngram overlap score of source and example as sentence_bleu score.\\n\\nlangchain.retrieversÂ¶\\nRetriever class returns Documents given a text query.\\nIt is more general than a vector store. A retriever does not need to be able to\\nstore documents, only to return (or retrieve) it. Vector stores can be used as\\nthe backbone of a retriever, but there are other types of retrievers as well.\\nClass hierarchy:\\nBaseRetriever --> <name>Retriever  # Examples: ArxivRetriever, MergerRetriever\\n\\nMain helpers:\\nDocument, Serializable, Callbacks,\\nCallbackManagerForRetrieverRun, AsyncCallbackManagerForRetrieverRun\\n\\nClassesÂ¶\\n\\nretrievers.contextual_compression.ContextualCompressionRetriever\\nRetriever that wraps a base retriever and compresses the results.\\n\\nretrievers.document_compressors.base.BaseDocumentCompressor\\nBase class for document compressors.\\n\\nretrievers.document_compressors.base.DocumentCompressorPipeline\\nDocument compressor that uses a pipeline of Transformers.\\n\\nretrievers.document_compressors.chain_extract.LLMChainExtractor\\nDocument compressor that uses an LLM chain to extract the relevant parts of documents.\\n\\nretrievers.document_compressors.chain_extract.NoOutputParser\\nParse outputs that could return a null string of some sort.\\n\\nretrievers.document_compressors.chain_filter.LLMChainFilter\\nFilter that drops documents that aren\\'t relevant to the query.\\n\\nretrievers.document_compressors.cohere_rerank.CohereRerank\\nDocument compressor that uses Cohere Rerank API.\\n\\nretrievers.document_compressors.embeddings_filter.EmbeddingsFilter\\nDocument compressor that uses embeddings to drop documents unrelated to the query.\\n\\nretrievers.ensemble.EnsembleRetriever\\nRetriever that ensembles the multiple retrievers.\\n\\nretrievers.merger_retriever.MergerRetriever\\nRetriever that merges the results of multiple retrievers.\\n\\nretrievers.multi_query.LineList\\nList of lines.\\n\\nretrievers.multi_query.LineListOutputParser\\nOutput parser for a list of lines.\\n\\nretrievers.multi_query.MultiQueryRetriever\\nGiven a query, use an LLM to write a set of queries.\\n\\nretrievers.multi_vector.MultiVectorRetriever\\nRetrieve from a set of multiple embeddings for the same document.\\n\\nretrievers.multi_vector.SearchType(value[,\\xa0...])\\nEnumerator of the types of search to perform.\\n\\nretrievers.parent_document_retriever.ParentDocumentRetriever\\nRetrieve small chunks then retrieve their parent documents.\\n\\nretrievers.re_phraser.RePhraseQueryRetriever\\nGiven a query, use an LLM to re-phrase it.\\n\\nretrievers.self_query.astradb.AstraDBTranslator()\\nTranslate AstraDB internal query language elements to valid filters.\\n\\nretrievers.self_query.base.SelfQueryRetriever\\nRetriever that uses a vector store and an LLM to generate the vector store queries.\\n\\nretrievers.self_query.chroma.ChromaTranslator()\\nTranslate Chroma internal query language elements to valid filters.\\n\\nretrievers.self_query.dashvector.DashvectorTranslator()\\nLogic for converting internal query language elements to valid filters.\\n\\nretrievers.self_query.deeplake.DeepLakeTranslator()\\nTranslate DeepLake internal query language elements to valid filters.\\n\\nretrievers.self_query.elasticsearch.ElasticsearchTranslator()\\nTranslate Elasticsearch internal query language elements to valid filters.\\n\\nretrievers.self_query.milvus.MilvusTranslator()\\nTranslate Milvus internal query language elements to valid filters.\\n\\nretrievers.self_query.mongodb_atlas.MongoDBAtlasTranslator()\\nTranslate Mongo internal query language elements to valid filters.\\n\\nretrievers.self_query.myscale.MyScaleTranslator([...])\\nTranslate MyScale internal query language elements to valid filters.\\n\\nretrievers.self_query.opensearch.OpenSearchTranslator()\\nTranslate OpenSearch internal query domain-specific language elements to valid filters.\\n\\nretrievers.self_query.pinecone.PineconeTranslator()\\nTranslate Pinecone internal query language elements to valid filters.\\n\\nretrievers.self_query.qdrant.QdrantTranslator(...)\\nTranslate Qdrant internal query language elements to valid filters.\\n\\nretrievers.self_query.redis.RedisTranslator(schema)\\nVisitor for translating structured queries to Redis filter expressions.\\n\\nretrievers.self_query.supabase.SupabaseVectorTranslator()\\nTranslate Langchain filters to Supabase PostgREST filters.\\n\\nretrievers.self_query.timescalevector.TimescaleVectorTranslator()\\nTranslate the internal query language elements to valid filters.\\n\\nretrievers.self_query.vectara.VectaraTranslator()\\nTranslate Vectara internal query language elements to valid filters.\\n\\nretrievers.self_query.weaviate.WeaviateTranslator()\\nTranslate Weaviate internal query language elements to valid filters.\\n\\nretrievers.time_weighted_retriever.TimeWeightedVectorStoreRetriever\\nRetriever that combines embedding similarity with recency in retrieving values.\\n\\nretrievers.web_research.LineList\\nList of questions.\\n\\nretrievers.web_research.QuestionListOutputParser\\nOutput parser for a list of numbered questions.\\n\\nretrievers.web_research.SearchQueries\\nSearch queries to research for the user\\'s goal.\\n\\nretrievers.web_research.WebResearchRetriever\\nGoogle Search API retriever.\\n\\nFunctionsÂ¶\\n\\nretrievers.document_compressors.chain_extract.default_get_input(...)\\nReturn the compression chain input.\\n\\nretrievers.document_compressors.chain_filter.default_get_input(...)\\nReturn the compression chain input.\\n\\nretrievers.self_query.deeplake.can_cast_to_float(string)\\nCheck if a string can be cast to a float.\\n\\nretrievers.self_query.milvus.process_value(...)\\nConvert a value to a string and add double quotes if it is a string.\\n\\nretrievers.self_query.vectara.process_value(value)\\nConvert a value to a string and add single quotes if it is a string.\\n\\nlangchain.runnablesÂ¶\\n\\nClassesÂ¶\\n\\nrunnables.hub.HubRunnable\\nAn instance of a runnable stored in the LangChain Hub.\\n\\nrunnables.openai_functions.OpenAIFunction\\nA function description for ChatOpenAI\\n\\nrunnables.openai_functions.OpenAIFunctionsRouter\\nA runnable that routes to the selected function.\\n\\nlangchain.smithÂ¶\\nLangSmith utilities.\\nThis module provides utilities for connecting to LangSmith. For more information on LangSmith, see the LangSmith documentation.\\nEvaluation\\nLangSmith helps you evaluate Chains and other language model application components using a number of LangChain evaluators.\\nAn example of this is shown below, assuming youâ€™ve created a LangSmith dataset called <my_dataset_name>:\\nfrom langsmith import Client\\nfrom langchain_community.chat_models import ChatOpenAI\\nfrom langchain.chains import LLMChain\\nfrom langchain.smith import RunEvalConfig, run_on_dataset\\n\\n# Chains may have memory. Passing in a constructor function lets the\\n# evaluation framework avoid cross-contamination between runs.\\ndef construct_chain():\\n    llm = ChatOpenAI(temperature=0)\\n    chain = LLMChain.from_string(\\n        llm,\\n        \"What\\'s the answer to {your_input_key}\"\\n    )\\n    return chain\\n\\n# Load off-the-shelf evaluators via config or the EvaluatorType (string or enum)\\nevaluation_config = RunEvalConfig(\\n    evaluators=[\\n        \"qa\",  # \"Correctness\" against a reference answer\\n        \"embedding_distance\",\\n        RunEvalConfig.Criteria(\"helpfulness\"),\\n        RunEvalConfig.Criteria({\\n            \"fifth-grader-score\": \"Do you have to be smarter than a fifth grader to answer this question?\"\\n        }),\\n    ]\\n)\\n\\nclient = Client()\\nrun_on_dataset(\\n    client,\\n    \"<my_dataset_name>\",\\n    construct_chain,\\n    evaluation=evaluation_config,\\n)\\n\\nYou can also create custom evaluators by subclassing the\\nStringEvaluator\\nor LangSmithâ€™s RunEvaluator classes.\\nfrom typing import Optional\\nfrom langchain.evaluation import StringEvaluator\\n\\nclass MyStringEvaluator(StringEvaluator):\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        return False\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        return True\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        return \"exact_match\"\\n\\n    def _evaluate_strings(self, prediction, reference=None, input=None, **kwargs) -> dict:\\n        return {\"score\": prediction == reference}\\n\\nevaluation_config = RunEvalConfig(\\n    custom_evaluators = [MyStringEvaluator()],\\n)\\n\\nrun_on_dataset(\\n    client,\\n    \"<my_dataset_name>\",\\n    construct_chain,\\n    evaluation=evaluation_config,\\n)\\n\\nPrimary Functions\\n\\narun_on_dataset: Asynchronous function to evaluate a chain, agent, or other LangChain component over a dataset.\\nrun_on_dataset: Function to evaluate a chain, agent, or other LangChain component over a dataset.\\nRunEvalConfig: Class representing the configuration for running evaluation. You can select evaluators by EvaluatorType or config, or you can pass in custom_evaluators\\n\\nClassesÂ¶\\n\\nsmith.evaluation.config.EvalConfig\\nConfiguration for a given run evaluator.\\n\\nsmith.evaluation.config.RunEvalConfig\\nConfiguration for a run evaluation.\\n\\nsmith.evaluation.config.SingleKeyEvalConfig\\nConfiguration for a run evaluator that only requires a single key.\\n\\nsmith.evaluation.progress.ProgressBarCallback(total)\\nA simple progress bar for the console.\\n\\nsmith.evaluation.runner_utils.EvalError(...)\\nYour architecture raised an error.\\n\\nsmith.evaluation.runner_utils.InputFormatError\\nRaised when the input format is invalid.\\n\\nsmith.evaluation.runner_utils.TestResult\\nA dictionary of the results of a single test run.\\n\\nsmith.evaluation.string_run_evaluator.ChainStringRunMapper\\nExtract items to evaluate from the run object from a chain.\\n\\nsmith.evaluation.string_run_evaluator.LLMStringRunMapper\\nExtract items to evaluate from the run object.\\n\\nsmith.evaluation.string_run_evaluator.StringExampleMapper\\nMap an example, or row in the dataset, to the inputs of an evaluation.\\n\\nsmith.evaluation.string_run_evaluator.StringRunEvaluatorChain\\nEvaluate Run and optional examples.\\n\\nsmith.evaluation.string_run_evaluator.StringRunMapper\\nExtract items to evaluate from the run object.\\n\\nsmith.evaluation.string_run_evaluator.ToolStringRunMapper\\nMap an input to the tool.\\n\\nFunctionsÂ¶\\n\\nsmith.evaluation.name_generation.random_name()\\nGenerate a random name.\\n\\nsmith.evaluation.runner_utils.arun_on_dataset(...)\\nRun the Chain or language model on a dataset and store traces to the specified project name.\\n\\nsmith.evaluation.runner_utils.run_on_dataset(...)\\nRun the Chain or language model on a dataset and store traces to the specified project name.\\n\\nlangchain.storageÂ¶\\nImplementations of key-value stores and storage helpers.\\nModule provides implementations of various key-value stores that conform\\nto a simple key-value interface.\\nThe primary goal of these storages is to support implementation of caching.\\n\\nClassesÂ¶\\n\\nstorage.encoder_backed.EncoderBackedStore(...)\\nWraps a store with key and value encoders/decoders.\\n\\nstorage.file_system.LocalFileStore(root_path)\\nBaseStore interface that works on the local file system.\\n\\nstorage.in_memory.InMemoryBaseStore()\\nIn-memory implementation of the BaseStore using a dictionary.\\n\\nlangchain.text_splitterÂ¶\\nText Splitters are classes for splitting text.\\nClass hierarchy:\\nBaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                             RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: MarkdownHeaderTextSplitter and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\nMain helpers:\\nDocument, Tokenizer, Language, LineType, HeaderType\\n\\nClassesÂ¶\\n\\ntext_splitter.CharacterTextSplitter([...])\\nSplitting text that looks at characters.\\n\\ntext_splitter.ElementType\\nElement type as typed dict.\\n\\ntext_splitter.HTMLHeaderTextSplitter(...[,\\xa0...])\\nSplitting HTML files based on specified headers.\\n\\ntext_splitter.HeaderType\\nHeader type as typed dict.\\n\\ntext_splitter.KonlpyTextSplitter([separator])\\nSplitting text using Konlpy package.\\n\\ntext_splitter.Language(value[,\\xa0names,\\xa0...])\\nEnum of the programming languages.\\n\\ntext_splitter.LatexTextSplitter(**kwargs)\\nAttempts to split the text along Latex-formatted layout elements.\\n\\ntext_splitter.LineType\\nLine type as typed dict.\\n\\ntext_splitter.MarkdownHeaderTextSplitter(...)\\nSplitting markdown files based on specified headers.\\n\\ntext_splitter.MarkdownTextSplitter(**kwargs)\\nAttempts to split the text along Markdown-formatted headings.\\n\\ntext_splitter.NLTKTextSplitter([separator,\\xa0...])\\nSplitting text using NLTK package.\\n\\ntext_splitter.PythonCodeTextSplitter(**kwargs)\\nAttempts to split the text along Python syntax.\\n\\ntext_splitter.RecursiveCharacterTextSplitter([...])\\nSplitting text by recursively look at characters.\\n\\ntext_splitter.SentenceTransformersTokenTextSplitter([...])\\nSplitting text to tokens using sentence model tokenizer.\\n\\ntext_splitter.SpacyTextSplitter([separator,\\xa0...])\\nSplitting text using Spacy package.\\n\\ntext_splitter.TextSplitter(chunk_size,\\xa0...)\\nInterface for splitting text into chunks.\\n\\ntext_splitter.TokenTextSplitter([...])\\nSplitting text to tokens using model tokenizer.\\n\\ntext_splitter.Tokenizer(chunk_overlap,\\xa0...)\\nTokenizer data class.\\n\\nFunctionsÂ¶\\n\\ntext_splitter.split_text_on_tokens(*,\\xa0text,\\xa0...)\\nSplit incoming text and return chunks using tokenizer.\\n\\nlangchain.toolsÂ¶\\nTools are classes that an Agent uses to interact with the world.\\nEach tool has a description. Agent uses the description to choose the right\\ntool for the job.\\nClass hierarchy:\\nToolMetaclass --> BaseTool --> <name>Tool  # Examples: AIPluginTool, BaseGraphQLTool\\n                               <name>      # Examples: BraveSearch, HumanInputRun\\n\\nMain helpers:\\nCallbackManagerForToolRun, AsyncCallbackManagerForToolRun\\n\\nClassesÂ¶\\n\\ntools.retriever.RetrieverInput\\nInput to the retriever.\\n\\nFunctionsÂ¶\\n\\ntools.render.render_text_description(tools)\\nRender the tool name and description in plain text.\\n\\ntools.render.render_text_description_and_args(tools)\\nRender the tool name, description, and args in plain text.\\n\\ntools.retriever.create_retriever_tool(...[,\\xa0...])\\nCreate a tool to do retrieval of documents.\\n\\nlangchain.utilsÂ¶\\nUtility functions for LangChain.\\nThese functions do not depend on any other LangChain module.\\n\\nFunctionsÂ¶\\n\\nutils.interactive_env.is_interactive_env()\\nDetermine if running within IPython or Jupyter.\\n\\n            Â© 2023, LangChain, Inc..\\n          Last updated on Jan 25, 2024.\\n          Show this page source', timestamp=datetime.datetime(2024, 1, 31, 2, 9, 39, 145824), tags='', summary='')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_store.read_data(topic = \"langchain_api_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e8a3df-f188-4c04-a43f-868c339f42ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchani-book-py3.10-ipykernel",
   "language": "python",
   "name": "langchani-book-py3.10-ipykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
